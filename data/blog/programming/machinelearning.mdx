---
title: Machine Learning
date: '2021-07-26'
tags: ['programming', 'python', 'gui']
---



REDES SOCIALES, SITIO Y PORTAFOLIO
https://github.com/DarioArzaba/blog
https://www.facebook.com/danny.flores.12914
https://www.linkedin.com/in/juan-pablo-muci%C3%B1o-land%C3%ADn-7597151a9/
https://launchingpad.tec.mx/
https://www.linkedin.com/learning/paths/learning-codeless-machine-learning-with-knime?u=76737724


https://news.ycombinator.com/item?id=34428033
https://karpathy.ai/zero-to-hero.html?utm_source=tldrnewsletter
https://tldr.tech/jobs
https://www.knime.com/
https://www.freecodecamp.org/
https://hired.com/join/?utm_source=newsletter&utm_medium=sponsor&utm_campaign=(b2c)(l-all)(r-all)(tldrnewsletter-devspo)&utm_content=find-a-job
https://huyenchip.com/2023/01/24/what-we-look-for-in-a-candidate.html?utm_source=tldrnewsletter
https://www.thebalancemoney.com/average-salary-information-for-us-workers-2060808
https://younoodle.com/
https://mx.indeed.com/trabajo?q=Practicantes%2C+ingenieros+industriales&l=Toluca%2C+M%C3%A9x.&sc=0kf%3Acmpsec%28NKR5F%29%3B&radius=10&vjk=61a0429fecc69425&advn=9046743915949466

https://news.ycombinator.com/item?id=34725047


https://utaustin.mygreatlearning.com/artificial-intelligence-machine-learning-course?&utm_source=Google&utm_medium=video&utm_campaign=AIML_Video_CA+CI_Mexico&adgroup_id=134810606101&campaign_id=11608241446&Keyword=&placement=youtube.com


https://www.calligrapher.ai/



https://startup.jobs/founding-engineer-infra-claypot-ai-3360627
https://www.ycombinator.com/
https://jobs.lever.co/claypot/9c6fedbf-3c91-4a48-98b9-467d00ae4afb/apply
https://jobs.lever.co/claypot/9c6fedbf-3c91-4a48-98b9-467d00ae4afb
https://www.claypot.ai/work-with-us


*** 

# Tips #

* Meta, Amazon, Microsoft, Google have massive layoffs.
	* Leave and enter companies gracefully, every person counts, treat them with respect.
	* Leave a clean desk, delete meetings, documents and hand every piece of information.
	* Clean your CV and networking profiles.
	* List all contact details and where you legally can work.
	* List what you are looking for, have different CVs catered to the role in question.
	* Contact people you worked with and give feedback, take a chance to look at al the perks of the company.
	* Don't keep any company resources or you could get into stealing.
	* Reach other employes that have been laid off, try to be as human as possible.
* Invest time in your portfolio as a junior dev
	* Good porfolio can open doors to future opportunities
	* You portfolio should be a representation of your journey throuh the industry.
* What we look in a resume 
	* Get a CV, linkedin and website in order.
	* Check all of your social media posts
	* Dont claim to be an expert in everything
	* For everything you include back it up, write a why, what issues you face and what changes could you make.
	* If you have time really do a StackOverflow, Github and Blog Post about X technology,
	* Show if you have initiative: Created clubs, events, teams, personal projects even startups.
	* Show if you have persistance: Daily contribution to GitHub for a year, be a chess master, professional athlete.
	* Job jumping at least 2 year per company is to short to show commitment.
	* Dont include solutions to popular projects, you have to show your unique strenghts
	* Don't use metrics without enough data to make a point.
		* Write metrics that: Show how they can be tied to business objetives and your personal contribution in that metric
		* Part of a 2-data-scientist team that owns feature engineering for a fraud detection system. Added 200 new features, resulting in a reducing the false positive rate from 20% to 15%, while keeping the false negative the same.
		* Built caching strategy using application-level caching & Redis for 2000 QPS, leading to a decrease in response time by 50%.
	* Find repetition and remove them, is a list big enough
	* Remove irrelevant experience to the job you are applying.
	* Most CV can shorten their education section
	* Remove common skills like Word, Excel, notebook, javascript etc.
	* Reduce font size and page margin.
	* Having a cover letter is a nice bonus when you can explain why you want to join the company and if you are a good fit, also it is a good way to explain a transition.
	* Every information must be included in the resume not in the cover letter.
* General tips from the resume blog post
	* If you apply to a small startup research who works at the company (less 20) and email them directly
	* Send your resume as PDF
	* Get someone who supervised you to talk about how great you are, include their quotes if person is important.
	* Dont use abbreviations unless you are absolutelly sure the audience knows what you are talking about.
	* Include links to open source projects or architecture you have worked on.
	* Pin important repos of your Github to your homepage and a Readme to guide what each repo is about.
	* If you have publications link your Google Scholar profile.
	* Create a personal website to show every single project that would fit in your resume.
	* For students put education earlier to know where you are and what youre looking for. Put graduation date.
	* If you have been working for 2 years remove GPA and coursework and focus on work experience.
	* Course projects should be listed only if better than the average course project.


***

# ChatGPT #

















ChatGPT is a chat-based tool built by OpenAI research lab with the objective of answering questions in a conversational dialogue.
ChatGPT can answer and explain complex concepts but can be inaccurect and nonfactual.
ChatGPT based DoNotPay tool looks to fight bills and get refunds or discounts on products such as flights and hospitals.

Some applications include creating a personilized diet, generating a list of meals and grocery list ingredients, 
creating a bedtime story. To generate the questions for an interview or to set up mock conversations to practice.

Create a proyect like a chat bot to search transcipts of podcast to search for information with embeddings search.

1. Foundation Model Players: Anthropic, OpenAI, Cohere, AI21
1. AI Frontend Startups: Jasper, Copy.AI
1. Copilot LLM for X Use: Github, Character.AI, Inflection, Adept, Neeva, Cresta, Lilt
1. Tooling Companies: Scale AI, SurgeHQ, Snorkel, MosaicML, Stranger Compute, Goose.AI
1. Big Compute Clouds: GCP, AWS, Azure, Oracle.

# Dan Shipper #



A VER COSAS QUE NO ME GUSTAN DE TI

1. Me pediste cambiar mis fotos de perfil contigo, lo cual hice con gusto porque te amo y luego las cambiaste.
	1. Tiene algun significado, o simplemente cambias mucho de foto de perfil, o las tengo que cambiar o simplemente no me debe importar.
1. Preguntarte sobre tu familia, yo se que me dijiste que no has hablado con ellos si pensado hacerlo o no.
1. No me gusta que me hagas berrinches, me manipules y no me digas cosas bonitas.
1. Proporcion del tiempo que hablamos de cosas materiales, si te ves en una relacion y es muy importante para ti.


DISTRITO 35 METEPEC 
SAN MATEO ATENCO 77 SECCIONES 4121 A 4141 (21)
METEPEC 55 SECCIONES 2464 A 2549 (86)
MEXICALTZINGO 56 SECCIONES 2550 A 2553

DISTRITO 5 TENANGO DEL VALLE
SAN ANTONIO LA ISLA 74 SECCIONES 4022 A 4025 (4)

VERIFICAR CON CURP, NOMBRE, FECHA DE NACIMIENTO



MARTES = EJERCICIO DE PWM 
MARTES = PRACTICA 4 CONTADOR DE PERSONAS (boton incrementa, otro decrementa y otro reinicia a 0) Utilizar codigo de random. Botones con rebote.
MIERCOLES = PRACTICA 5 MELODIAS (Reproducir melodia 1 o melodia 2 (Mario bros pero de 100 notas y otra de internet). SWITCH CON BOTONES. Usar Timer 1 OC1A en PB1 (Freq Op Arduino Nano es 16MHz).
VIERNES = EXAMEN PARCIAL 2 Y PROPUESTA DE PROYECTO FINAL
MARTES = EJERCICIO ADC CON PWM, PRACTICA RELOG DIGITAL (5 PUSH Y PANTALLA LED) Y ENTREGA DEL PROYECTO



FOTOGRAFIA PUBLICITARIA PRESENCIAL MARTEES Y VIERNES MEDIO DIA (NO)
MOTORES COMBUSTION INTERNA REMOTO LUNES Y JUEVES (SI)
PLANEACION ESTRATEGICA SIN CLASES SOLO TAREAS Y EXAMENES POCOS LUGARES (TENIA QUE AVISAR KARLITA)
ENVASE Y EMBALAJE PARA EXPORTACION PRECENCIAL LUNES A LAS 4 (MAYBE)
METODOS CUANTITATIVOS MARTES Y VIERNES ROMOTA EN LA MAÑANA (SI ADVERTENCIA)


DISEÑO Y ANALISIS EXPERIMENTOS MARTES Y VIERNES REMOTA EN LA MAÑANA (SI)
SEGURIDAD INFORMATICA LUNES Y JUEVES REMOTA MEDIO DIA (SI)


6006 4961 4227 0247 598
5821







This card is issued by Victoria's Secret Stores GC, LL and is reedemable for merchandise only at all VS store locations,
catalogue and online and VictoriasSecret.com unless otherwise required by applicable law.
If this card is lost, stolen or destroyed VS may replace it if you provide the card number.
THIS CARD CARRIES NO EXPIRATION DATE
18002708999





OpenAI with ChatGPT ramped hiring with 1000 remote contractors in regions like Latin America and Europe
Most do data labeling creating massive sets of images, audio clips and information used on the artificial intelligence.
Others use the data with Codex product to translate natural language into code.
“A well-established company, which is determined to provide world-class AI technology to make the world a better and more efficient place, is looking for a Python Developer,” 
Microsoft is OpenAI largest investor.
The official site gives 375 people working on the project from Sam Altman CEO.
OpenAI Codex is used by GITHUB Copilot that autocompletes lines of code.
Investors envision products that users can buy to describe websites, apps and videogames by simply describing them.
The hottest new programming language is English with Andrej Karpathy, head of AI at Tesla
OpenAI has also been known to contract in Kenya for content moderation about racist, violent and sexual content just like facebook.























2023/01/03
Job Hunting in 2022
work
tech
job-hunt
After leaving Meta recently, I decided it was finally time to start looking for a job again. It's been quite an experience doing it amid a recession and layoffs (I have thankfully been quite lucky). I thought I'd share how I approached the job hunt as a few people had asked for advice.

When I told people about leaving a job without anything lined up.
When I told people about leaving a job without anything lined up.

Note: As always, this is just one person's experience. I'm sharing this in the hopes that this proves helpful for someone else in a similar situation. I've tried to keep the advice generally applicable, but it's most suited towards senior-ish software engineers with industry experience and a pre-existing network and less likely to apply to new grads. I was also not in a huge rush and had the luxury to be picky in the search (and was very lucky due to many other factors). If you need a job urgently, this might not be the best advice.

How it went down
I started looking in mid/late September and wrapped things up by mid-December. Here's a rough timeline:

September: Thinking about applying (and letting my network know I'm looking), coming up with a plan, some initial exploratory calls, and interview prep
October: More exploratory calls, some phone screens/initial rounds, interview prep, and a break for a vacation before the actual interviews
November: Interviews all day erryday (for the first 3 weeks), then a break + follow-up calls/receiving offers
December: Receiving offers, follow up-calls, negotiating, and deciding.
My goal through the process was to find a place I'd be happy learning and growing for at least 4-5 years, and discover what type of work I wanted to do next (e.g. nonprofit, big company, small company, different industries, management vs engineering, etc). So, throughout this process, I applied to a lot more places than I would have done. But, hey, as a candidate, we get to interview companies just as much as they interview us, so I decided to use this opportunity accordingly. In terms of companies. A rough count of the numbers below:

There were ~35 companies I tracked on my list, of which I applied to 27:
I applied online to 14 places, of those I only heard back from 6.
I used referrals to get help applying at 9 companies. I heard back from all of these. Referrals really mattered in terms of getting a response!
I generally get a lot of recruiter spam - but I started paying attention now. I accepted 4 of the reach outs, 2 of which went pretty far.
I dropped out of the process at 4 companies during the exploratory stage, 3 had no roles open/fille their openings early on in the process, and I dropped out of the process at 3 places after deciding it wasn't a good fit after a few interviews.
I completed full interview loops at 13 companies and received 11 offers. I'm glad I got the rejections I did - it was clear there wasn't a mutual match.
In the future, I likely won't apply as much. Depending on your needs you might have to apply to a similar amount of companies but I don't recommend going through interview loops at this many companies unless you're actively exploring. At the same time, since nerves really get to me, calming myself down by lowering the stakes (if I messed up with one company, it's ok, I am applying to a lot of others) really helps since you feel less stressed in each interview.

Planning
Before starting a job hunt, think about what you want to get out of the process. All desires are valid - it's just that optimizing for "I want to find the best job and I'm willing to wait" is very different from "I need a job ASAP to pay the bills, anything that pays more than $X works". So think about that and be honest with yourself so that you get what you're looking for.

I didn't know what I wanted from my next job (just that I wanted to stay there a while and grow myself), so I explicitly set a goal to explore and learn. And set aside a bunch of time to think through my criteria and what I wanted. This also meant more aggressive searching and sourcing of companies to apply to (via my network, and online), rather than applying to every possible opening.

I also felt quite underprepared so I budgeted a bunch of time to practice and make sure I was interview ready - this included planning out which weeks I'd do what, so I could book interviews accordingly - and in particular, schedule my highest priority / riskiest interviews roughly in the middle so I'd go in with some practice but not be super worn out.

Acceptance Criteria
These are deeply personal - there's no one right answer to "is this the right opportunity for me?". But it's important to have your criteria known beforehand (feel free to refine them during the process) so that you can evaluate each opportunity fairly. Think about what you want in your life and your career.

For me, the criteria were a mix of the following (importance factors left out for brevity):

Company/role: what the company does (and what is its growth trajectory), what the org does, the role I'm interviewing for, and how my work ties to the mission, etc
Team/Culture/People: how the company culture is, how my (future) co-workers are, diversity, etc
Personal: growth opportunities for me, work-life balance/PTO, remote vs hybrid, compensation, how excited I am for the role, etc
Once I had these written down, I kept a spreadsheet for each company I applied to so that I could make sure to get enough data for each of my questions/criteria throughout the process.

Finding companies to apply to
This isn't easy when starting out in the industry, unfortunately. But if you've been working for a few years, you likely have a network you can lean on. Ask your coworkers/friends what they like about their current roles and which places they'd recommend. I was lucky to have worked with some excellent people in my past role so companies with people I knew and liked working with immediately shot to the top of the list.

Since that suffers from selection bias, though, I did scour LinkedIn and Twitter (if you're working in security, infosec twitter - now migrated to Mastodon mostly - has a lot of great leads) for open roles. Hacker News does great whoishiring threads every month (like this one from this week) that I scoured to get an idea of what's out there.

Resume prep
Like it or not, preparing your resume is an important part of the process. There are a few main things here:

Getting past the initial filters - you need to ensure there's enough info for the hiring team to know you're worth having an initial conversation with
Showcasing your strengths - if you want to get a better role than your current one, you have to play up your strengths and highlight your best work.
This is an art, not a science. In particular, it helps to customize your resume for each role you're applying to, depending on what the job posting is looking for. Some general tips (from mistakes I've seen and made):

Focus less on what you did. Lots of resumes are full of "I built X using Y technology". Instead, focus on the impact of your work: how much revenue did it bring in? how did it help the company/org achieve its goals? You want to show that you can bring value to the place you're applying to and operate at a higher level
Talk about your role: Lots of people can write code to build a feature. Can you go above and beyond and lead a project? Mentor others? Drive collaborations? Make that clear in your resume, and highlight what you did.
Sell yourself: Highlight the projects you're most proud of. Focus on the highest scope projects (largest impact, most engineers/users affected, etc). I generally hate bragging but if there is one time to brag, it's during an interview.
Applying
Applying itself is thankfully fairly standardized in a lot of places. You'll apply online, submitting your resume and filling out some standard fields - or you'll ask for a referral, where someone puts in a (hopefully good) word for you, and then you get a link to the form. Then a recruiter will set up a call to ask more about you, determine if there's a mutual fit, and then schedule a screening interview. If that goes well, you'll likely have a full panel interview (4-6 interviews), potentially across multiple days.

There were only a few companies that had a different process in my experience (some had takehomes, or a highly async process, etc) - and they stood out positively in my experience.

Interview prep
Treating prep seriously, like a job, really helped. I would get down and carve out a few hours a day for prep instead of playing them totally adhoc. There are a few types of interviews worth preparing for - most places will have some combination of these (and the initial screen will likely cover a bit of each).

Coding interviews
Ah, the dreaded coding interview. Unfortunately, these will happen for most places you interview at, especially for FAANG roles. There's no way around these except for practice. I spent a good amount of time on LeetCode just going over a lot of questions, trying my hand at solving them, and then reading through the other solutions. All in all, I'd say I did about ~60 or so questions (on the lower side).

Here are a few tips I found helpful when practicing leetcode:

Don't worry about doing all the questions (there are way too many) - do go over questions from companies you're planning to interview at, though
Focus on topics - e.g. do a few tree questions, a few greedy algorithm questions, etc. You'll most likely face an unfamiliar problem in the interview and the most useful thing you can do is to be ready to quickly slot it into "ah, I can use a tree here" and then apply familiar techniques.
Take it seriously. When I started, I'd just do the question leisurely and try to learn, look things up, etc. That didn't help much when it came to a real interview setting. So, try to time yourself, and approach it like you would a real interview, including testing.
For the interview itself: think out loud (the interviewer can't read your mind), talk through what you are thinking, and get buy-in on the approach before coding. Watch for hints! And, lastly, remember to test your code thoroughly - look at what you wrote, not what you think you wrote.
System design interviews
These seem quite intimidating, especially if you've never done them before (I never had!). The way they are structured is both a plus and a minus. A friend told me "you can't really prepare for these" and it's quite true honestly. If you don't have exposure to how large-scale systems are designed, you will struggle as a lot of the concepts are different.

Even if you're familiar with them, though, designing stuff in an interview setting is a different beast. I found it quite helpful to go over examples (Alex Xu's system design book came in really handy) to get an idea of the types of things that come up in an interview. And there are guides you can find on leetcode that go over the basic structure of these interviews (watching some videos on Youtube really helps!) so you can follow the pattern and make sure you cover everything.

Some tips:

Write down a timesheet and list of things to cover and reference that as you interview. I followed this one.
Your interviewer is your friend: ask them if you're on the right track or if you should focus on something else, etc
Focus on breadth and painting the high-level picture first, go into details where asked
Write down important numbers, draw out a picture, etc - so you don't forget where you are in the conversation
ASK QUESTIONS. Make sure you understand who's using the system, what the requirements are, etc. Don't jump into solving things upfront.
Behavioral interview prep
Depending on where you interview, this might be called a behavioral interview, a culture fit interview, a career interview, etc. Naming aside, they have one thing in common: you'll be expected to talk about your past experiences in some depth, with the interviewer leading the discussion. These are hard to prep for (you don't know what will be asked!) but knowing what to expect really helps. Here's how I approached it:

Pick examples to help you shine: Focus on "the biggest" conflict, "the most impactful" project, etc. These interviews often determine what level you're hired at, and you want to show you can handle complex issues. Go over your proudest career moments, recall them, and write down some quick notes so you don't forget the details
Keep it short. For every question, spend at most 45-60 seconds answering it. The interviewer will have a lot of material to cover, so just answer the question to the point, don't add too much extra detail, and offer to elaborate if needed. If you elaborate too much on every answer, you won't be able to convey your best impression.
Focus on the STAR method. For each thing, explain the Situation, what the Task at hand was, what Action you took, and what the Result was. Make your role in whatever happened clear.
Other interviews
Depending on where you apply, there might be other types of interviews thrown in. It's totally fine (and expected!) that you might need help and would ask for a rubric or an explanation of what the interview process is like. Just take them as you get them.

For example, I got a few security interviews (code review/bug finding/web application security) that I was totally unprepared for - they turned out ok, because I was able to ask for help and get close enough with answers. Hopefully, that strategy works out - and if it doesn't, you might be able to see if the job is a good fit or not (it might not be if they're focusing on something you don't have experience in!).

When in interview mode
Alright, you've prepared, you've set up interviews, and now it's the dreaded interview day. Here's how you can make it go better for you.

Focus on your physical self: make sure you've gotten enough sleep, have food ready to go for lunch, and have breaks between the interviews if possible. If the interviewer asks if you need a minute at the start, and you do - please take it. At a higher level, if you're interviewing across multiple companies, try to have a half day/day between sets of interviews.
Don't stress out. Nerves can make or break an interview. Failing an interview is not a negative judgment of who you are - it's just a performance in a highly artificial setting. If you lower the stakes for yourself, you'll find yourself performing better.
Feel free to ask for help or a hint if you're stuck. It's okay! I did that a bunch rather than waste time and have the interviewer write me off.
Learn from yourself. I kept detailed notes of every question I was asked and the responses I gave so that I could debrief myself at the end of the day and do better the next day. This is easy to do when you're remote (and hard to do in person) so take advantage of it while you can!
Ask for clear expectations at the start of the interview, if not provided - e.g. for a coding interview, ask about how much they expect you to cover, whether you need to write working code or pseudocode, etc.
Fact finding & decision making
Throughout the process, you're interviewing companies too, to find the right fit. Refer back to your criteria list to make sure you've gotten enough information for each company. I found it quite helpful to ask each interviewer a few questions from my list at the end of each interview to make sure I had everything covered. If you haven't, feel free to reach out to the recruiter or hiring manager for a follow-up - they'd be quite happy to meet to sell you on the role even further.

In my case, after the first few interviews, I realized I needed to update my criteria so I went back and redid everything - and then had a bunch of follow-ups to ask places.

If you're not sure about which questions to ask, this one is a great list.

Note that you will likely be under pressure to decide early on (companies will push for this). So make sure your criteria are ready and you have the info you need. Or be willing to push back hard to get more time to decide.

Experiences across companies
Since I was exploring, I got to talk to a wide set of people across a variety of companies doing wildly different things. Though a lot of processes were quite standard (phone screen, coding+behavioral+design+manager interview), each place had its quirks, good, and bad. Here's some stuff that I did not expect/really liked/learned through the process:

Recruiters matter a lot. A great recruiter makes a world of a difference as a candidate - there were a few that went above and beyond, helping me with extra prep calls, answering all of my questions, and being flexible whenever I needed a change. On the flip side, a bad recruiter can be very off-putting, especially when it comes to things like ignoring communication preferences or ghosting you (I had a particularly bad experience there).
Relatedly, candidate experience matters a lot. A few companies sent doordash/ubereats gift cards instead of the lunch they'd normally provide in an in-person interview setting, and it helped save mental energy for the interviews
Unique interview experiences stand out! There were a bunch, but the most memorable ones include:
fly.io had an asynchronous process. The take-home was a great way to learn about the company culture, and I really loved that they delivered feedback and gave me a chance to improve my take-home before submitting it.
Akita had an interesting 90-minute coding challenge where I had to work on a real-world problem and build a useful feature - beyond just coding, I got to learn about the domain and exercise problem-solving skills.
Felt had a work-day exercise where (for an EM role) I got to have two 1:1s with members of the team (like I would on the job) and then debrief afterward with the interviewers to give suggestions. It was very refreshingly different and interesting.
The interview setup and experience tell you a lot about the company and what they value. Pay attention to the questions they ask and the people you meet. As an example, interviewing at CZI was really positive for me: I got to see exactly how passionate everyone was about the mission and how they got people from all backgrounds to work there (it was the most diverse interview panel I had).
Hindsight bias & lessons learned
With hindsight bias, I definitely learned a lot and would do things differently next time. Hopefully, you can learn from these:

There are a lot of good jobs out there, and not enough of "me". It's still a candidate's market for now, so use that to your advantage. Companies are looking to hire, and despite hiring freezes a lot of great companies are still growing.
Core skills matter a lot - you have to be good at what you do. In the same vein, at the senior levels, it's less about pure technical depth (unless you're going for a very specialized position) and more about "soft" skills: how you lead a team, how you work with people, etc. This is good and bad: but helps for interviews as you can apply for a broad set of roles.
In hindsight I wouldn't have applied to as many roles as I did - but hey, I wanted to explore. Interviewing for 6-7 hours 4-5 days/week for 3 weeks straight got a bit much, and my calendar was so full that rescheduling things became a nightmare. I had a couple of days with 6 interviews with 5 different companies and I do not recommend that experience.
Don't overdo the leetcode: I felt like I prepared all the "hard" questions while I mostly got asked "easy" or "medium" ones. That would definitely have cut down on prep time.
Learn about stock compensation/all the various forms of equity. I had to scramble to learn these while making my decision and in hindsight, I should have learnt about this stuff earlier.
References: A surprising number of companies asked for references. I'd probably have prepared better for this rather than bother busy people at the last minute as I did. Make sure you're prepared for this.
TLDR
Job hunting can be stressful and take a long time. But there are a lot of resources and prep material and you'll eventually land a good role.

As always, please let me know if you have any feedback/comments or if there's any way I can help!








Scores of Stanford students used ChatGPT on final exams, survey suggests
A robot stands in a teaching position before a chalkboard, in front of a class of robot students sitting at old-school wooden desks.
(Graphic: DALL-E/OpenAI)
By Mark Allen Cu and Sebastian Hochman
Jan. 22, 2023, 4:03 p.m.

Stanford students and professors alike are grappling with the rise of ChatGPT, a chatbot powered by artificial intelligence, and the technology’s implications for education.

Some professors have already overhauled their courses in anticipation of how students might use the chatbot to complete assignments and exams. And according to an informal poll conducted by The Daily, a large number of students have already used ChatGPT on their final exams.

Whether the new technology will necessitate a revision of the Honor Code, the University’s standards for academic integrity, remains to be seen: A University spokesperson confirmed that the Board of Judicial Affairs is aware of and monitoring these emerging tools.

“Students are expected to complete coursework without unpermitted aid,” wrote spokesperson Dee Mostofi. “In most courses, unpermitted aid includes AI tools like ChatGPT.”

The chatbot was created by San Francisco-based artificial intelligence company OpenAI and launched in November. Stanford dropout Sam Altman is among the company’s co-founders.

When OpenAI released the tool, it quickly took off, attracting over a million users in under a week and drawing widespread attention for its ability to generate almost any level of text-based fodder using technology from a field of machine learning called natural language processing. It not only acts as a robotic study buddy, but can also write lengthy essays on almost any subject and generate ideas and outlines for projects.

For example, when asked for a “witty opening paragraph for a Stanford Daily article about ChatGPT and the Stanford Honor Code,” the chatbot produced the following:

“ChatGPT, a popular artificial intelligence tool, has recently sparked a debate on campus about its role in academic integrity. Some argue that using ChatGPT to complete assignments is a violation of the Honor Code, while others claim it’s simply a tool for idea generation. The question remains: is ChatGPT a helpful helper or a dishonest cheat? One thing is for certain: the stakes are high, and the future of AI at Stanford hangs in the balance.”

The power of the technology to instantaneously generate swaths of human-like text has frightened some educators across the country.

Among other districts that have cracked down on its use, New York City’s education department has blocked the site on its networks, citing “concerns about negative impacts on student learning, and concerns regarding the safety and accuracy of content,” according to education department spokesperson Jenna Lyle in a statement to Chalkbeat New York.

Awareness of the technology has also reached Stanford’s faculty, confirmed Mostofi, the University spokesperson. Mostofi, citing a recent story in the Stanford Report featuring faculty weighing in on ChatGPT, wrote, “Many Stanford faculty are highly engaged in researching new large language models and implications of AI in the learning environment.”

Mostofi said student assignments will continue to be designed to “support students in developing linked thinking and writing skills,” including the drafting and revising processes, as well as citing sources.

Some colleges and universities have already incorporated the new technology into their academic integrity policies. Washington University in St. Louis and University of Vermont in Burlington are among the institutions that have amended their academic integrity policies to include the usage of AI tools like ChatGPT.

Mostofi wrote that at Stanford, conversations will soon be underway about ChatGPT and the honor code.

“The Board on Judicial Affairs (BJA) has been monitoring these emerging tools and will be discussing how they may relate to the guidelines of our Honor Code,” Mostofi wrote.

But while the University plans to discuss ChatGPT, some students have already used the tool to complete their finals, according to an anonymous poll conducted by The Daily on the social media app Fizz, which requires a stanford.edu email to join.

According to the poll, which had 4,497 respondents (though the number may be inflated) and was open from Jan. 9 to Jan. 15, around 17% of Stanford student respondents reported using ChatGPT to assist with their fall quarter assignments and exams.

Of those 17%, a majority reported using the AI only for brainstorming and outlining. Only about 5% reported having submitted written material directly from ChatGPT with little to no edits, according to the poll.



According to another informal poll conducted by The Daily on the same app, a majority of student respondents believe that the use of ChatGPT to assist with assignments is currently or should be a violation of the Honor Code. However, there is a difference in what students believe should be considered a violation.


The news that some students are already using ChatGPT on assignments has spread to professors, some of whom are revamping their courses as a result.

In a message to the public computer science (CS) department Slack channel, computer science associate professor Michael Bernstein ’06 asked if any other professors had encountered homework that was generated by ChatGPT.

“In this case,” wrote Bernstein, “it was easy to tell because part of the submission included: ‘As a large language model trained by OpenAI…’”

Computer science lecturer Julie Stanford BA ’98 MA ’98 added in the Slack that the student’s submission was “like robbing a bank and caring so little about being caught that you try to take a selfie with the security camera on the way out.”

Some professors on campus have recently added course policies to their syllabi cautioning against the use of ChatGPT, arguing that it is a form of plagiarism, while others have switched to more traditional methods, attempting to eliminate technology from the picture.

One class, COMM 108: “Media Processes and Effects” has dedicated a whole section of its syllabus dedicated to the usage of AI tools. “Using Artificial Intelligence (AI) Agents (e.g., ChatGPT, StableDiffusion) to generate assignments or parts of assignments is generally discouraged. If you choose to use an AI agent for generating portions or aspects of an assignment, you must disclose this use and cite it in the same manner as you would cite any external source,” reads the syllabus.

In the computer science Slack, senior lecturer Keith Schwarz MS ’11 said that he has “switched back to pencil-and-paper exams,” citing concerns related to open computers that could be operating ChatGPT, even suggesting that he might consider “requiring students to leave all their backpacks and electronics at the front of the exam room.”

Stanford AI Alignment (SAIA) student leaders Gabriel Mukobi ’23 Michael Byun ’24 urged students to be wary of using ChatGPT for academic work as the tool has not been fine-tuned for use in academic settings.

“AI tools like ChatGPT are clearly here to stay,” the two wrote.

A previous version of this article incorrectly stated that Elon Musk dropped out of Stanford. That portion of the article has since been removed, and The Daily regrets this error.

Mark Allen Cu '26 is the Data Reporter and a news anchor for The Daily. He is currently studying International Relations and Data Science. Contact him at mallencu 'at' stanford.edu
Sebastian Hochman is a staff writer at The Stanford Daily for Arts & Life and University desks. He is a music major. Maybe. You can contact him at shochman 'at' stanford 'dot' edu.



AGI will not happen in your lifetime. Or will it?
Grady Booch and I discuss

Gary Marcus
Jan 22
Twitter avatar for @Grady_Booch
Grady Booch 
@Grady_Booch
AGI will not happen in your lifetime

Or in the lifetime of your children

Or in the lifetime of your children’s children
6:20 AM ∙ Jan 17, 2023
756
Likes
83
Retweets
Grady Booch, a Fellow at IBM, and one of the deepest minds on software development in the history of the field, is one of my favorite people that I have met through Twitter.

We rarely disagree about anything. On Twitter we sometimes tag team Yann LeCun, and on more than a few occasions we have retweeted each other’s cynical tweets. Neither one of us believes for a minute that large language models are remotely close to artificial general intelligence, and what’s more, we mostly agree about what’s missing: semantics, reasoning, common sense, theory of mind and so on.

But I found myself disagreeing with the Tweet above. My own skepticism has limits. I do think we will eventually reach AGI (artificial general intelligence), and quite possibly before the end of this century.

On the theory that our differences might represent some kind of teachable moment, I invited him to talk about it on the record; he immediately agreed. Here’s a very lightly edited transcript of our conversation (approved by both of us).

Gary Marcus: Grady, I get that AGI is hard, and that we aren’t there yet. I think we are wasting funding and bright young minds on an approach that probably isn’t on the right path. I don’t think we have great answers yet for a bunch of core issues, like how to get machines to represent abstract knowledge, and how to get them to induce complex models from the events that they see or read about it. When I think about how little deep progress has been made in the last 75 years, I get depressed.

But I am cautiously optimistic that we’ll do better in the next 75, that once the hype cools off, people will finally dive deeper into neurosymbolic AI, and start to take some important steps. Our data problems are solved, our compute problems are mostly solved; it’s now mostly a matter of software, and of rethinking how we build AI. Why be so sure we can’t do that in the next 75 years?

Grady Booch: Gary, you and I are in frighteningly close agreement on so many subjects: that the mind is computable; that large language models are inherently wonky; that our field has made incredible progress in our lifetimes and that as such we are on the cusp of something amazing; that we as an industry know there are hard problems to be solved with regard to causality, common sense reasoning, abductive reasoning, embodiment, and so on; that we have some ideas how to approach these problems; that scale is not enough; that pineapple on pizza is good.

Well, I might be making that last one up.

Where we diverge, I think, is largely on timing: you posit that we will see AGI within a few decades, I think it is more like a few generations.

In the spirit of full disclosure, I have to admit that I, being a historian of computing, have a rather jaded and cynical view of the hyperbolic optimism of our field and as such am somewhat conditioned to be a contrarian when it comes to predictions such as this. Take the singularity, for example, something that von Neuman first spoke of in the 1950s and which certain of our colleagues have predicted with alarming precision that we'd achieve by 2045. The term is sufficiently imprecise, filled with emotional and historic baggage, and touches some of humanity's deepest hopes and fears that it's hard to have a rational discussion therein. AGI is just like this. Greek mythology speaks of golums created from clay; Mary Shelly built life from human parts whose very souls were ignited by lightning; da Vinci imagined mechanical knights to fight wars; Edison built dolls that moved and talked; Weiner - who coined the term cybernetics - thought we might build artifical intelligenes through analog mechanisms, Simon and Newll thought it so through formal logic, Feigenbaum through knowlege engineering, and now we see our colleagues who expect that AGI is right around the corner if and only we had enough data and a level built of enough GPUs such that we could move the world.

In short, AGI seems just around the corner, and you yourself fall into that trap when you say "it's now mostly a matter of software".

It's never just a matter of software. Just ask Elon and his full self driving vehicle, or the Air Force and the software-intensive infrastructure of the F-17, or the IRS with their crushing technical debt. I have studied many of the cognitive architectures that are proported to be on the path of AGI: SOAR, Sigma, ACT-R, MANIC, AlphaX and its variations, ChatGPT, Yann's latest work, and as you know have dabbled in one myself (Self, an architecture that combines the ideas of Minsky's society of mind, Rod's subsumption architecture, and Hofstadter's strange loops). In all of these cases, we all think we grok the right architecture, the right significant design decisions, but there is so much more to do. Heck, we've mapped the entire neural network of the common worm, and yet we don't see armies of armored artificial worms with laser beams taking over the world. With ever step we move forward, we discover things we did not know we needed to know. It took evolution about 300 million years to move from the first organic neurons to where we are today, and I don't think we can compress the remaining software problems associated with AGI in the next few decades.

Indeed, this leads me to also observe, in the spirit of full disclosure, to suggest that we as computer scientists not only vastly overestimate our abilities to create an AGI, we vastly underestimate and underrepresent what behavioral scientists, psychologists, cognitive scientists, neurologists, social scientists, and even the poets, philosophers and storytellers of the world know about what it means to be human. There is much we can and should learn from them to guide our work as computer scientists in our journey.

In fact, let me posit that, while someday we will likely build an artificial mind, perhaps the most important outcome of that journey is that is will compel us to understand what it means to be human.

Gary Marcus: Turns out I am good with the pineapple on pizza; we should’ve had some together when I visited you in Maui! Real pineapple, straight from the farm, not from a can. And I also very much agree with something else you said:

we as computer scientists not only vastly overestimate our abilities to create an AGI, we vastly underestimate and underrepresent what behavioral scientists, psychologists, cognitive scientists, neurologists, social scientists, and even the poets, philosophers and storytellers of the world know about what it means to be human.

Just the other day, in fact, in my introduction to the AGI Debate, I quoted the late Drew McDermott’s closely-related line:

“It is hard to know where [AI researchers] have gone wronger: in underestimating language or overestimating computer programs”

And Darwin knows, I too am pretty jaded.

§

But all that said, I certainly think there are lots of cases of early failures that turned out reasonably quickly, once some key idea was unlocked. Alchemy turned into a decently worked-out chemistry fairly quickly, once the basic idea behind the periodic table was discovered; it wasn’t too long between when Oswald Avery figured that the basis of Mendel’s factors was DNA rather than a protein (1944) and when Watson and Crick (with help of Franklin’s data) figured out the structure of the DNA (1953 ). Fast forward another few years and the amino acid-nucleotide code was cracked; Monod and Jacob had worked out the basics of gene expression by the 1960s; Kary Mullis had worked out PCR by 1983. That’s an astonishing amount progress in just under 40 years. Of course there was a long period of stasis just before, when people misguidedly thought that genes were instead made of proteins. But once they got off that wrong-headed track, things moved very quickly indeed.

And we have some advantages nowadays, even relatively to the 50s and 60s; computers can allow new ideas to be tested at unprecedented speed and scale, and good ideas can percolate really really fast, like diffusion as a better way of reconstructing images, which swept through the whole field in a matter of months (and is only a few years old, dating back to 2015). Transformers, invented 2017, ubiquitous in 2023, have (perhaps both for better and worse) spread extremely rapidly.

Since we are (at least to my mind, though I note that you disagree) primarily talking software, there’s no complex manufacturing processes, and so on. Even if we need to invent a new chip type as a prerequisite, that could be a thirty year process rather a one hundred year process.

What I think we are missing are (a) ideas and (b) large scale databases of machine-interpretable knowledge. Ideas are hard to predict; we don’t know when they will come, and we don’t know how many genuinely new, important ideas we need; on the machine-interpretable knowledge front, we have problems too. But 75+ years worth? Enough to keep us that busy, with so many people working on the problem and so much money (at least currently) somewhat directed at the problem?

In my darkest moments, I actually agree with you. For one thing, most of the money right now is not going to the wrong place: it’s mostly going to large language models, and for you, like for me, that just seems like an approximation to intelligence, not the real thing, and as such as frustrating distraction. And I’ve been concerned for a long time that a fixation on Big Data has sucked the oxygen (as Emily Bender likes to put it) from a lot of other ideas that might be better. For another, I don’t really feel like that many people are working on the right problems right now, and I think a lot of core problems from 75 years ago are still unsolved; McCarthy worried about common sense in 1959, and I still don’t see a solution I can take all that seriously.

But I see some signs that are promising. The neurosymbolic AI community is growing fast; conferences that used to be dozens are now thousands. (I’m looking forward to giving on of the keynotes at the IBM Neurosymbolic conference on Wednesday). Meta’s Cicero is a great mix of handcrafted structure and knowledge – just where is it needed - and corpus-driven machine learning. I take that as a hopeful sign that the scaling-über-alas narrative is losing force, and that more and more people are open to new things. We can only have advances in science once there is recognition of problems, and too much of the rhetoric in ML has in recent times been aimed at stifling critics like us, but I am finally seeing at least a bit of that necessary recognition.

The rubber-that-meets-the-road question in the end is how many key discoveries do still we need to make, and how long do we need to make them? Do you have any take on that, what the key discoveries we need to make are, and what it might take to foster them, and why you think they might take so long to incubate?

Grady Booch: There is this wonderful scene in the Will Smith movie "I Robot" in which the detective Del Spooner meets the avatar of Dr. Alfred Lanning, the father of modern robotics in Asimov's universe who - as a human - had just committed suicide with the help of an NS5 humanoid robot named Sonny. Lanning's death was part of his plan to stop the US Robotics robots from taking over the world. Lanning's avatar was not sentient, but it could reply with truthfullness to any well-formed question.

This, of course, immediately distinguishes itself from ChatGPT, which demonstrably produces bullshit at scale. But that is another topic for another time.

Anyway, Spooner keeps asking the Lanning avatar questions about the suicide, to which the avatar often replies "I'm sorry, my responses are limited. You must ask the right question". Eventually Spooner asks the question "whose's revolution?" and in turn Lanning's avatar replies "detective, that is the right question".

You asked me "how many key discoveries do still we need to make, and how long do we need to make them?"

Gary, that is the right question.

Let me begin by saying that I would be delighted if my prediction (that AGI will not happen anytime soon) was utterly and completely wrong and that you were right. If this comes to pass, I'll buy you an extra large pizza with ham and pineapple and we can share it on the beach as we watch the fall of civilization.

For you see, while I would be delighted at the technical advances made by the field of computing, I would be simultaneously frightened at how humanity would face the presence of a synthetic intelligence suddenly formed in its midst within a single generation. We as a species are ill-prepared to properly metabolize such a superior intelligence, and the ethical issues of how we humans and these artificial sentient beings should treat one another are far beyond the capacity of earth's societies and governments to address with any degree of wisdom or dignity (stares at the US House of Representatives). What power and rights would we individuals have in the shadow of any metacorporation who would undoubly have been the one to bring such a creation into being at scale? Would we treat these new minds as literal slaves? How would this further divide the rich and the poor of this world?

But that also is another topic for another time.

To answer your question, however, I have to address the elephant in the room: what is AGI? I can't suggest what we need to do next until we agree on where we are going.

I'll be direct, which is my style. AGI is a term that has considerable emotional and historic baggage and as such - much like the term "singularity" - is often best used for selling books (stares at Ray) or for naming clickbait articles. It's complex, but I will assert that the mind is computable and therefore it is concievable that synthetic minds can be formed, minds that exhibit the behavior of organic ones.

An aside: we must remember in all this that we humans live in a Flatland, and so we have considerable human bias when it comes to the semantics of intelligence. I therefore assert that it is concievable that other kinds of intelligence can be found in the cosmos.

Intelligence is, for me, just the first phase in a spectrum that collectively we might speak of as synthetic sentience. Intelligence, I think, encompasses reasoning and learning. Indeed, I think in the next few decades, we will see astonishing progress in how we can build software-intensive systems that attend to inductive, deductive, and abductive reasoning. Pearl's ideas regarding cauality will likely become manifest. We'll might see some breakthroughs in common sense reasoning (although like you, i'm skeptical). Similarly, I think in the next few decades, we will see astonishing progress in learning, with advances in self-directed learning, perhaps even with breakthroughs in artificial curosity and synthetic play.

Conciousness and self-consciousness are the next phases in my spectrum. I suspect we'll see some breakthroughs in ways to represent long term and short term memory, in our ability to represent theories of the world, theories of others, and theories of the self.

Sentience and then sapience fill out this spectrum. The world of AI has not made a lot of progress in the past several years, nor do I see much attention being spent here, largely because - as you observe - current funding and corporate interests have pretty much sucked all the oxygen in the air to blow on the embers of transformer-based AI. Transformers are not enough. Scale is not enough. Work needs to be done in the area of planning, decision making, goals and agency, and action selection.

We also need to make considerable progress in metacognition and mechanisms for subjective experience.

These things, collectively, define what I'd call a synthetic mind. In the next decade, we will likely make interesting progress in all those parts I mentioned.

But, we still don't know how to architect these parts into a whole. Evolution took several million years to get us squishy minds to the point where your squishy bits and my squishy bits can wave our fingers across keyboards and send these thoughts across an ocean. I don't see anything in the near future of computing that will compress what needs to get AI from where it is today to where it needs to be to form a non-squishy mind. This is not a problem of scale; this is not a problem of hardware. This is a problem of architecture.

Indeed, Alan Newell said something to this effect in 1990: "The question for me is how can the human mind occur in the physical universe. We now know that the world is governed by physics. We now understand the way biology nestles comfortably within that. The issue is how will the mind do that as well. The answer must have the details. I got to know how the gears clank and how the pistons go and all the rest of that detail. My question leads me down to worry about the architecture".

And that, detective, is the right question.

Gary Marcus: Let me start by outlining a lot of agreement. I am not at all sure how humanity would fare in the presence of a superior intelligence. ChatGPT is (at least in many ways) a far inferior intelligence, that, despite having some undeniable talents, cannot at all be trusted. And yet trust it, many humans do.

To their peril.  Already we have seen minor abuses; plagiarism, for example, will never be the same. Troll farms may well starting using it to create misinformation at unprecedented scale; we can also expect more and circles of fake web sites in order to sell advertisements; ChatGPT been apparently used to create malware, and it has already infected journalism, with CNET using it produce news stories that were filled with errors. Bias is likely to be huge issue, too. Even if large models never acquire a volition of their own (I hope not!), we have already seen that in the wrong hands (either with malice or, as in the case of CNET, negligence), bad things can and probably will happen. In a recent Wired article, I predicted that 2023 see the first death attributable to large language models, and I stand by that prediction.

ChatGPT is a dress rehearsal, and I wouldn’t say we as a society have gone through it with flying colors. What would happen with even more potent AI is indeed a disconcerting question, not yet well answered.

(Aside for another day: I think that better AI, in which we could directly instill values, might actually make us safer than the world in which we inhabit now, in which so much of what happens is down to the vagaries of what happens to be in this or that training set; I don’t leave my kids moral development to chance, and it terrifies me that so much of current AI is dependent on random details of training corpora that are not available to scientists for inquiry. This seems like a really bad idea.)

But we are not, per se, here to debate whether or not AGI will lead to the fall of civilization (not today, anyway); back to our original question of when it might arrive.

I don’t see you arguing never. I also don’t see you entirely defining your terms. One way to ago would be to say, “we won’t get to AGI until we get sentient AI”; is that the argument you are making?

I find that one to be tough – I see no clear criteria for sentience – but also not entirely relevant. In a recent bet that put to Elon Musk (still no word!), I defined AGI, with the blessings of two of these who coined the term, Ben Goertzel and Shane Legg, as

“a shorthand for any intelligence ... that is flexible and general, with resourcefulness and reliability comparable to (or beyond) human intelligence”

I don’t think that kind of flexibility in any way rests on or demands on sentience. I have in mind something more like the fairly general kind of intelligence we see in science-fiction, in the Star Trek computer, in C-3P0, in Scarlett Johannsen’s assistant in Her. These characters (especially perhaps the latter two) might or might not be argued to be sentient, but it seems to be that they are flexible, resourceful, reliably (in a way that LLMs certainly aren’t), and adaptive. For me that counts. If we had an AI that smart, I could retire.

But we don’t yet. Question is, again, what it would take to get there. I am fully with you on need deeper semantics, abduction, common sense reasoning etc. (That was the whole point of Rebooting AI).

I am  also with you in doubting that there will be a single magic singularity moment. Intelligence is multidimensional; we’ve made progress in some of those dimensions (searches of certain complex spaces, some kinds of pattern recognition) and not others (e.g. everyday physical and psychological reasoning).

And they won’t come in all at once.  I expect for example that physical reasoning will improve (a radical boon for domestic robots) before psychological reasoning; there won’t be a magic day in which we say “aha AGI landed on January 27, 2062” or whatever it is.

Machines will just get more and more reliable and more general. Maybe the big landmark will be the day in which they can learn enough on their own that they no longer need us to teach them.

You might well agree there, too. (And sure, other kinds of intelligence probably do exist in the cosmos; AGI is unlikely to be exactly humanlike, but I don’t see being a replica is a necessity; adaptivity is key, and that is probably achievable by multiple means).

With all that considerable agreement noted, I am still not seeing a principled argument that constructing flexible, adaptive systems with commonsense reasoning, deeper semantics, and so on  will take us past the end of the current century.

Of course nothing is guaranteed; we might not get there because we blow ourselves up, or because we waste so much time on large language models that other important challenges—like semantics, causality, commonsense reasoning, abduction and so forth—never get the attention they deserve.

Or because we can’t come up with something better than the current systems that are so greedy in both data and energy consumption that they could asymptote before anything really general emerges.

But certainly the hunger is there, the funding is there, and my guess is that patience for LLMs (which I see as a detour) is not infinite. A decent fraction of the AI community has already recognized that the primary forte of large language models is in generating bullshit; many also see that the current path to autonomous vehicles is not making the progress that some expected. There are whole lot of us, you and me, and the entire neurosymbolic community, but also even folks Yann LeCun and Geoff Hinton, who see that we have currently is not enough.

So I don’t think we will waste more than 5 years before a substantial fraction of AI effort is looking outside the limited streetlight that currently are sucking up so much mental energy. Eventually, and not before too too long, I think the field itself on a better path.

About the only argument I see for needing more than another 75 years, assuming we don’t blow ourselves up, and do eventually face the limits of large language models head on, is that there is simply a lot to do. I don’t doubt that for a minute. But if we need 7 discoveries, and find them once a decade, I still win the bet.

That said, I will close with a final point of agreement: architecture is all.

Integrating the parts in a whole is a huge huge unsolved problem. LLMs give some illusion of having solved that (because they can talk about so much), and OpenAI just confirmed that they are working on a model with video input, which will surely be interesting, but I think that in achieving AGI we will have no choice but to find ways of integrating whatever we can get out of deep learning with explicit knowledge and explicit cognitive models.

I am still not seeing a clean path there, and I don’t think we can say with a straight face that we have achieved the integration we need if a system can’t even check its output against Wikipedia or the AP News Wire.

Since you have thought more about system architecture than anyone I know, I’d especially like to know how you handicap the systems integration race. When will we get to machines that can, like humans, take in information across essentially arbitrary domains and modalities, and put it all together into a reasonably cogent (if occasionally erroneous) understanding of the world?

Grady Booch: Yes, ChatGPT is just the start. Yes, a better AI in which we could instill values could make us safer. (But, whose values? Were you and I in leadership positions in the Government of the People’s Republic of China, we’d be having a very different conversation.)

Writing in my best James Bond voice, I never say never. Indeed, as I asserted earlier, the mind is computable and ergo I expect that someday we will have a synthetic mind. The essence of our dissonance seems to be when, and an enumeration of the things we must do to get there.

I think we can agree that the terms AGI and intelligence and sentience and all that are incredibly imprecise and full of emotional baggage. As such, I suspect we are indeed in almost perfect alignment as to the essence of the future except that we are slip sliding on the edges of these symbols. This is why I eschew using the term AGI and why I speak of  a spectrum of behaviors. In the next few decades, I expect - as do you - that we’ll go a long way down that path: better common-sense reasoning, some degree of adductive reasoning and theory making and physical reasoning, systems that require far less energy, systems are are more embodied than chatbots that are currently untethered from the physical world and its consequences. We will likely even build AIs that have some sense of self. It’s going to be crazy wonderful. You and I would have been quickly burned at the stake in the 1200s for even suggesting that such things were possible and yet, here we are, both expecting that these things will become manifest almost before our very eyes.

And yes, when we have an AI that can say “I don’t know how to do X but I do have the skills to figure out how to do X and so I will go that, play with it a bit, and then keep trying to do X until I get a reasonable result” that will be a momentous shift. This is why the theories of the world/other/self are important to me: if I have an AI that has some level of cognition, imagine if I apply that cognition to itself. Mechanisms for metacognition are still in their infancy.

There is one other thing I find missing, and you said it yourself: adaptability. Plasticity; agency across domains.

This is why I like Legg and Hutter’s definition “Intelligence is an agent’s ability to achieve goals in a wide range of environments.” We can today teach an AI how to play any number of different kinds of video games within a particular envelope of behaviors, but getting an AI that has the agency to apply those skills to a completely different domain - controlling a crane in a ship yard, navigating a New York City street to shop, making a meal in a never-before-seen kitchen - we have considerable work to do.

So yes, I think it’s about architecture, the problem of integrating the parts. As I look at the history of cognitive architectures from Simon and Newell’s Logic Theorist to Yann’s latest sketches, we have and we continue to have made remarkable progress in all the individual pieces, but we have not yet found the whole.

To be fair, I must recognize that I have consierable bias and hubris in what I just said, because - as a systems architect - I'm looking at the future of AI from the lens of architecture, and I see progress lacking. We as a field are still very much in the experimental phase of cognitive architectures that scale, but we as an industry are properly trying new things and nudging them to be better and better. The reality that field of AI has begun to discover common design patterns (for example, transfomers) is a sign of growing maturation.

How do we make progress? Far too much money and minds, it seems to me, are focusing on incremental improvements, such that I fear that the field is settling in on a local minimum. I'm happy to see Hinton and others thinking radically. But I think this is only a start.

If I had a complete answer to your question, Gary, I'd be building it right now. Alas, I don't and so I can only try new things and nudge, which is exactly what I'm doing. This, I suppose, is the nature of scientific progress.

I want to close by observing that our dialog has been a great deal of fun for me. These are issues that stike at the heart of what it means to be human, and I can think of fewer other endeavors that give one the privilege and the responsibility to work on things that sit on the edge of something that has the power to amplify the best in us while simultaneously having the power to amplify the worst. This is why these things cannot be left up to us technologists; we should be only one voice in this journey, for what we are doing has the potential to change civilization.

Gary Marcus: Hear, hear! When we finally get to software with enough “agency to apply [its] skills to a completely different domain - controlling a crane in a ship yard, navigating a New York City street to shop, making a meal in a never-before-seen kitchen”, one of us will owe the other a pizza!

Thanks for chatting. This was indeed great fun! And I also agree that we need many other voices, not just technologists, speaking up in this journey. (Stay tuned for a sequel!)

Type your email…
Subscribe
Share

Gary Marcus, scientist, bestselling author, and entrepreneur, is a skeptic about current AI but genuinely wants to see the best AI possible for the world—and still holds a tiny bit of optimism. Sign up to his Substack (free!), and listen to him on Ezra Klein.

Grady Booch, per Wikipedia, “is an American software engineer, best known for developing the Unified Modeling Language (UML) with Ivar Jacobson and James Rumbaugh. He is recognized internationally for his innovative work in software architecture, software engineering, and collaborative development environments.

https://ibm.github.io/neuro-symbolic-ai/neuro-symbolic-ai/events/ns-workshop2023/

https://garymarcus.substack.com/p/what-does-meta-ais-diplomacy-winning

https://garymarcus.substack.com/p/dear-elon-musk-here-are-five-things?

https://arxiv.org/pdf/0706.3639.pdf

I tried an AI app to help me remember what I read online. It may be making my actual memory worse in the long run, but it was an effective tool.Arif Qazi/Insider
On a recent late afternoon, I was having trouble remembering. My browsing history for the day suggested I'd read over a dozen news articles, numerous Slack messages, plenty of Twitter threads, and a bunch of notes for my next assignment. Yet, somehow, I couldn't recall much of it. I remembered some vague contours of the content I had consumed but lacked the details.  

That afternoon wasn't particularly special — a few days later, I struggled to recollect the details of a lengthy COVID story I had read during a conversation with a friend. These instances weren't some crises of memory, nor were they due to a head injury. I just had too much rattling around in my brain. No matter what or how much I read online, my mind can't help but forget it shortly after. I don't blame my brain, either. Most people consume an overwhelming volume of text every day — hundreds of thousands of words — so it's no surprise that our memories struggle to retain more than a few scant details. "Humans have worse memories than we think we do, and memory for text, in general, isn't great," Virginia Clinton-Lisell, an associate professor of educational psychology at the University of North Dakota, told me.  

The internet only makes this brain-capacity problem worse. The online-reading experience is full of obstacles that prevent our brains from locking the information we consume into our long-term memories. When you read a book, things like page numbers and the physical ability to hold and turn pages help your brain make a mental map of the information the book presents you with. Websites, however, don't have those kinds of memory triggers. Because of this, multiple studies found that participants who read offline performed better in comprehension, concentration, and recall than participants who read online.  

The added distraction of the web's business — auto-play videos, pop-up ads, and the round-the-clock avalanche of notifications — certainly doesn't help. "Our capacity to process incoming information is naturally limited," Andrew Dillon, an information-and-psychology professor at the University of Texas, told me. He added that if we try to process too many things at once, "we pay a cost in terms of memory and comprehension or time. There's no free lunch." 

Another problem is that people devote far less mental effort to reading something online since we approach all online reading like it's social media — like it just needs skimming. For information to enter our knowledge structures, Dillon told me, we must distribute attention. "This takes time and effort," he said.  

Because we know we can look up a piece of information anytime we want — whether it's a phone number or George Clooney's birthday — we're far less likely to memorize it. Often during the day, I end up Googling articles I read just a few hours ago because I can't recall more than a few key words.  

To address this problem, dozens of read-later and bookmarking apps have cropped up over the years. Apps like Pocket and Instapaper have amassed millions of users by offering ways to organize links and save what they want to read online. But these apps can feel like a chore to keep and do not ultimately help retain the information on those webpages. Because of those drawbacks, I decided to try out a new, little-known service called Heyday. 

Heyday, which bills itself as an AI-memory assistant, promises to fix the two key challenges I've faced with reading-list tools: it demands little to no effort from me and aims to help me remember things better. Instead of simply cataloging where I read something, it promised to help me recall what I've been reading. In the three weeks I spent with the app, I found it was effective at helping me remember things, but it comes with a catch: Using a memory tool like this has the potential to make your biological memory worse over time.  

A new memory 
Founded in 2021, engineers designed Heyday to act as your memory — it quietly processes everything you read in the background and resurfaces information when you might need it. The app works by automatically scanning everything you look at on your browser: web pages, Google documents, notes, Slack conversations, and tweets. Then, it sorts what you've read into categories based on topic or on how much time you spent on something. Once it's added the information to the catalog, it provides dynamic prompts next to search results or within articles themselves to help resurface the information you've already read.

After installing the browser extension, Heyday went to work scanning everything I was reading. Once it gathered enough information, it began to resurface what I had read. When I Googled "Elon Musk" to look into the news about the Twitter CEO banning journalists, the app pulled up a list of related links from my history with their key summaries next to my search results. In this case, Heyday pulled up a Substack newsletter from a journalist, a tweet thread on how Musk lashed out in a Spaces chatroom, and a profile of another social network people were flocking to. This list allowed me to instantly recall what I've already read about the topic and added helpful context to my search, making it a more valuable use of my time.

Screenshot of Heyday prompts next to Google search results about Elon Musk
Heyday is an AI memory assistant. It reminds you of articles, posts, or other content that you have previously read.Shubham Agarwal/Heyday
When reading an article, Heyday would also underline key words that I've read about in the past, and when I hovered over them, the app told me more about that topic based on what I've read. While Heyday's browser tool behaved like a memory assistant, its website felt like a snapshot of my online memory. It categorized all the content I viewed by type: videos, tweets, research reports, and so on. Since its search tool understands natural language, I could use it to pinpoint a query, such as "articles about Elon Musk's ban," without worrying about the correct syntax or key words. Plus, if there were any articles in my history that I particularly liked reading, I could ask Heyday to store it in a separate folder like "favorites" or "recipes," and the next time I come across similar content, it will automatically recommend I save the article to that folder.  

Screenshot of Heyday's website showing different content folders
Heyday organizes everything you read or watch online into relevant folders.Shubham Agarwal/Heyday
One potential drawback is that because of how it runs, Heyday collects a treasure trove of data on you and your browsing history. But Samiur Rahman, a cofounder and the CEO of the startup, told me the data is encrypted and that their business model doesn't depend on selling user data or ads. Instead, their revenue comes from the $19 monthly subscription to use the tool. While the subscription model may help keep my data safe, the steep price point limits the tool to people like researchers or journalists who would use it a lot and could justify the cost.  



Another limitation with the tool was that the search widget often took a few seconds to show up next to my search results. So, there were times when I ended up clicking one of the results instead of waiting to see what the pop-up widget resurfaced. And though it never failed to register what I read, Heyday's algorithm occasionally did not understand a link's content and context, resulting in relevant articles not showing up when I expected them to while conducting another type of research. But in my three weeks with Heyday, I consistently felt that it made a difference in how much I could recall — and the experts I spoke with had an explanation.  

New information enters a first "forgetting curve" in our memories, and much of it slips down the drain unless we spend time reviewing the material. Dillon told me that repetition or rereading is crucial to remembering new information better. He told me that Heyday's continuous exposure to the same information can prove vital to aid memory. Similarly, Clinton-Lisell told me that making connections between subjects or themes naturally improves comprehension and memory, adding that if a tool like Heyday helps you make connections as you read, it should enable you to better remember.  

In my experience, because the app repeatedly exposed me to the content I had read and helped me make connections between things I was reading, it helped the information stick in my head.  

Refocusing the mind 
Rahman told me that Heyday's ultimate goal as a memory assistant is to "increase the creative output of individuals." By freeing up the brain from the task of encoding and resurfacing memories, he believes the brain will have more freedom to "focus on things that the human brain is uniquely great at — thinking, creativity, and analysis." 

Heyday isn't alone in this venture. Broader efforts to supplement our memories are underway across the industry. Dennis Xu, a cofounder of the OpenAI-backed Mem AI, a self-organizing workspace, wants to aid the brain in recalling disparate pieces of information so it works less to recall raw data you can easily look up. The goal of Mem AI is to allow people to focus on creative outputs and remember personal memories like a loved one's face. "I think that's a more worthy use of the brain's memory than, for example, remembering what date the Battle of the Alamo took place," Xu said.  

Rewind, a startup that raised $10 million in a round that Andreessen Horowitz led last year, said its "long-term vision is giving humans perfect memory." Rewind captures everything you look at on your computer and constructs a timeline of your activities, letting you simply search for facts and conversations instead of having to remember them or spend time digging through various apps. Personal AI, on the other hand, wants to clone your mind, creating a virtual "second brain" that houses all your memories and data.  

However, if we routinely begin to outsource the job of memorization to internet tools, would our biological memory evolve to become obsolete? There is already evidence that the internet has caused our memories to get worse since we don't feel the need to memorize what we can Google anymore. Personal AI's spokesperson, Jonathan Bikoff, doesn't necessarily see that as a bad thing. He expects technology-enabled minds to be more powerful and reliable than our biological minds. "With assistance from AI, humans may be able to enjoy more of life, embrace forgetfulness, and learn to weave AI into our every day," Bikoff said.  

Though the jury is still out on our memory's biological evolution, Dillon said the extent that the web and digital access is making memorization obsolete, at least for some people, is interesting. "Why learn a poem by heart if you can pull it up on demand? What's the point of learning your math tables if you can just ask Alexa for the answer? Like our bodies, do our minds also need to be exercised to maintain full functioning?" Dillon said, concluding: "I think there's some truth in this."  

I can't help but agree. During my Heyday use, the number of stories from its widget's list I had already forgotten about constantly surprised me. It was a reminder of just how much information I read slips from my brain. But while Heyday was effective at bridging the gap of my limited memory, making research easier, I worry that a reliance on the tool would make my memory even worse. But given the mounting volume of text we read online, perhaps we have already passed the point of no return. The modern world demands that we consume a massive amount of information, and our biological memories simply don't have the ability to remember it all. So instead of fighting a losing battle, an extended hard-drive-esque space like Heyday can be a vital supplement. For me, at least, Heyday is here to stay.

Shubham Agarwal is a freelance technology journalist from Ahmedabad, India whose work has appeared in Wired, The Verge, Fast Company, and more.

https://scholar.harvard.edu/files/dwegner/files/sparrow_et_al._2011.pdf

https://techcrunch.com/2022/11/01/rewind-wants-to-revamp-how-you-remember-with-millions-from-a16z/?guccounter=1

https://businessinsider.mx/elon-musk-bans-on-journalists-only-hurts-twitter-2022-12/?r=US&IR=T

https://scholar.harvard.edu/files/dwegner/files/sparrow_et_al._2011.pdf

The inside story of ChatGPT: How OpenAI founder Sam Altman built the world’s hottest technology with billions from Microsoft
The A.I. future, according to Altman, could be spectacular—unless it goes spectacularly wrong. Why Big Tech giants and business leaders everywhere are losing sleep over generative A.I.
BYJEREMY KAHN
January 25, 2023, 10:30 AM UTC

Sam Altman, CEO and cofounder of OpenAI.
IAN C. BATES—THE NEW YORK TIMES/REDUX
A few times in a generation, a product comes along that catapults a technology from the fluorescent gloom of engineering department basements, the fetid teenage bedrooms of nerds, and the lonely man caves of hobbyists—into something that your great-aunt Edna knows how to use. There were web browsers as early as 1990. But it wasn’t until Netscape Navigator came along in 1994 that most people discovered the internet. There were MP3 players before the iPod debuted in 2001, but they didn’t spark the digital music revolution. There were smartphones before Apple dropped the iPhone in 2007 too—but before the iPhone, there wasn’t an app for that. 
On Nov. 30, 2022, artificial intelligence had what might turn out to be its Netscape Navigator moment.
The moment was ushered in by Sam Altman, the chief executive officer of OpenAI, a San Francisco–based A.I. company that was founded in 2015 with financial backing from a clutch of Silicon Valley heavy hitters—including Elon Musk, Peter Thiel, and fellow PayPal alum and LinkedIn cofounder Reid Hoffman. On Nov. 30, some seven years after the company’s launch, Altman tweeted: “today we launched ChatGPT. try talking with it here,” followed by a link that would let anyone sign up for an account to begin conversing with OpenAI’s new chatbot for free.
And anyone—and everyone—has. And not just to chat about the weather. Amjad Masad, a software CEO and engineer, asked it to debug his code—and it did. Gina Homolka, a food blogger and influencer, got it to write a recipe for healthy chocolate-chip cookies. Riley Goodside, an engineer at Scale AI, asked it to write the script for a Seinfeld episode. Guy Parsons, a marketer who also runs an online gallery dedicated to A.I. art, got it to write prompts for him to feed into another A.I. system, Midjourney, that creates images from text descriptions. Roxana Daneshjou, a dermatologist at Stanford University School of Medicine who also researches A.I. applications in medicine, asked it medical questions. Lots of students used it to do their homework. And that was just in the first 24 hours following the chatbot’s release.
There have been chatbots before. But not like this. ChatGPT can hold long, fluid dialogues, answer questions, and compose almost any kind of written material a person requests, including business plans, advertising campaigns, poems, jokes, computer code, and movie screenplays. It’s far from perfect: The results are not always accurate; it can’t cite the sources of its information; it has almost no knowledge of anything that happened after 2021. And what it delivers—while often smooth enough to pass muster in a high school class or even a college course—is rarely as polished as what a human expert could produce. On the other hand, ChatGPT produces this content in about a second—often with little to no specific knowledge on the user’s part—and a lot of what it spits out isn’t half bad. Within five days of its release, more than 1 million people had played with ChatGPT, a milestone Facebook took 10 months to hit. 
Artificial intelligence technology has, over the past decade, made steady inroads into business and quietly improved a lot of the software we use every day without engendering much excitement among non-technologists. ChatGPT changed that. Suddenly everyone is talking about how A.I. might upend their jobs, companies, schools, and lives.
ChatGPT is part of a wave of related A.I. technologies collectively known as “generative A.I.”—one that also includes buzzy art generators like Midjourney and Lensa. And OpenAI’s position at the forefront of the tech industry’s next big thing has the hallmarks of a startup epic, including an all-star cast of characters and an investor frenzy that has crowned it with a reported valuation of $29 billion. 
But even as its recent surge provokes envy, wonder, and fear—Google, whose lucrative search empire could be vulnerable, reportedly declared an internal “code red” in response to ChatGPT—OpenAI is an unlikely member of the club of tech superpowers. Until a few years ago, it wasn’t a company at all but a small nonprofit lab dedicated to academic research. Lofty founding principles such as protecting humanity from the dangers of unrestrained A.I. remain. At the same time, OpenAI has gone through an internal transformation that divided its original staff and brought an increased focus on commercial projects over pure science. (Some critics argue that releasing ChatGPT into the wild was itself dangerous—and a sign of how profoundly OpenAI’s approach has shifted.) 
I think the good case [for A.I.] is just so unbelievably good that you sound like a crazy person talking about it. I think the worst case is lights-out for all of us.
SAM ALTMAN, COFOUNDER AND CEO OF OPENAI, SPEAKING AT A VENTURE-CAPITAL-FOCUSED EVENT IN SAN FRANCISCO ON JAN. 12
An expanded partnership with Microsoft, announced this week, that includes as much as $10 billion in new capital could result in the software giant capturing the lion’s share of OpenAI’s profits for years to come. That deal is likely to deepen the perception that the once idealistic endeavor is now primarily concerned with making money. That said, documents seen by Fortune reveal just how unprofitable OpenAI’s business is currently.
Altman, the 37-year-old cofounder and CEO, embodies OpenAI’s puzzling nature. A serial tech entrepreneur known more for business savvy than for feats of engineering, Altman is both the architect of OpenAI’s soaring valuation and its buzzkiller-in-chief—speaking out publicly about how far ChatGPT is from being truly reliable. At the same time, he sees the technology as a step forward in his broader, quixotic corporate mission to develop a computer superintelligence known as artificial general intelligence, or AGI. “AGI is probably necessary for humanity to survive,” Altman tweeted in July. “our problems seem too big [for] us to solve without better tools.”
It’s an unusual guiding philosophy for a moneymaking enterprise, especially considering that some computer scientists dismiss Altman’s obsession as the stuff of fantasy. “AGI is just silly,” says Ben Recht, a computer scientist at the University of California at Berkeley. “I mean, it’s not a thing.”
And yet, with ChatGPT, Altman has turned OpenAI—and the broader A.I. mission—into the thing captivating the tech world. The question is whether the partnership he has forged with Microsoft can fix ChatGPT’s flaws and capitalize on its early lead to transform the tech industry. Google and other titans are hard at work on their own A.I. platforms; and future, more polished software could make ChatGPT look like child’s play. OpenAI may someday find that, much like Netscape’s short-lived browser reign, its breakthrough has opened a door to a future it isn’t part of. 

On a Thursday evening in mid-January in San Francisco, Altman makes a rare public appearance. Dressed in a gray sweater, blue jeans, and a pair of groovy, brightly colored tie-dyed sneakers, the CEO walks into a roomful of investors, techies, and journalists, all gathered to glean any dish about ChatGPT or the imminent funding round. When his interviewer, Connie Loizos, the founder of StrictlyVC, a media company focused on venture capital, asks him about the media furor, Altman replies, “I don’t read the news, and I don’t really do stuff like this much.”
The event, on the 46th floor of the Salesforce Tower, is standing room only. One of the speakers during a fintech panel that takes place before the interview even tells the crowd that she knows they’re “all waiting for Sam Altman.” 
But despite the buzz, and the circulating rumors of the Microsoft investment, Altman seems to go out of his way to dampen the excitement. “One of the strange things about these technologies is that they are impressive but not robust,” he tells the crowd. “So you use them in the first demo; you kind of have this very impressive, ‘Wow, this is incredible and ready to go’ [reaction]. But you see it a hundred times, and you see the weaknesses.”
That kind of caution seems to be the official mode at OpenAI’s headquarters, situated in an old luggage factory in San Francisco’s Mission District. And indeed, if ChatGPT is A.I.’s Netscape Navigator moment, it is one that very nearly never happened—because OpenAI almost killed the project months ago. 
The chat interface that allows users to converse with the A.I. in plain English (and many other languages) was initially conceived by OpenAI as a way to improve its “large language models,” or LLMs. Most generative A.I. systems have an LLM at their core. They are created by taking very large neural networks—an A.I. based very loosely on connections in the human brain—and applying them to vast amounts of human-created text. From this library, the model learns a complex map of the statistical likelihood that any group of words will appear next to one another in any given context. This allows LLMs to perform a vast array of natural language processing tasks—from translation to summarization to writing. 
OpenAI had already created one of the world’s most powerful LLMs. Called GPT-3, it takes in more than 175 billion statistical connections and is trained on about two-thirds of the internet, all of Wikipedia, and two large data sets of books. But OpenAI found it could be tricky to get GPT-3 to produce exactly what a user wanted. One team had the idea of using reinforcement learning—in which an A.I. system learns from trial and error to maximize a reward—to perfect the model. The team thought that a chatbot might be a great candidate for this method since constant feedback, in the form of human dialogue, would make it easy for the A.I. software to know when it had done a good job and where it needed to improve. So in early 2022, the team started building what would become ChatGPT.
When it was ready, OpenAI let beta testers play with ChatGPT. But they didn’t embrace it in the way OpenAI had hoped, according to Greg Brockman, an OpenAI cofounder and its current president; it wasn’t clear to people what they were supposed to talk to the chatbot about. For a while, OpenAI switched gears and tried to build expert chatbots that could help professionals in specific domains. But that effort ran into problems too—in part because OpenAI lacked the right data to train expert bots. Almost as a Hail Mary, Brockman says, OpenAI decided to pull ChatGPT off the bench and put it in the wild for the public to use. “I’ll admit that I was on the side of, like, I don’t know if this is going to work,” Brockman says.
The chatbot’s instant virality caught OpenAI off guard, its execs insist. “This was definitely surprising,” Mira Murati, OpenAI’s chief technology officer, says. At the San Francisco VC event, Altman said, he “would have expected maybe one order of magnitude less of everything—one order of magnitude less of hype.”
OpenAI CTO Mira Murati on The Daily Show with Trevor Noah on Oct. 25, 2022.
OpenAI CTO Mira Murati on ‘The Daily Show with Trevor Noah’ on Oct. 25, 2022.
COURTESY OF MATT WILSON/COMEDY CENTRAL’S “THE DAILY SHOW”
ChatGPT isn’t OpenAI’s only hype generator. Its relatively small staff of around 300 has pushed the boundaries of what A.I. can do when it comes to creating, not simply analyzing, data. DALL-E 2, another OpenAI creation, allows users to create photorealistic images of anything they can imagine by typing just a few words. The system has now been emulated by others, including Midjourney and an open-source competitor called Stability AI. (All of these image generators have drawbacks, most notably their tendency to amplify biases in the data on which they were trained, producing images that can be racist and sexist.) By fine-tuning its GPT LLM on computer code, OpenAI also created Codex, a system that can write code for programmers, who only have to specify in plain language what they want the code to do. 
More innovations wait in the wings. OpenAI has an even more powerful LLM in beta testing called GPT-4 that it is expected to release this year, perhaps even imminently. Altman has also said the company is working on a system that can generate video from text descriptions. Meanwhile, in mid-January, OpenAI signaled its intention to release a commercial version of ChatGPT, announcing a wait-list for would-be customers to sign up for paid access to the bot through an interface that would allow them to more easily integrate it into their own products and services.

A cynic might suggest that the fact OpenAI was in the middle of raising a large venture capital round might have something to do with the timing of ChatGPT’s release. (OpenAI says the timing is coincidental.) What’s certain is that ChatGPT chummed shark-filled waters. It set off a feeding frenzy among VC firms hoping to snap up shares in the private sale of equity currently being held by OpenAI’s executives, employees, and founders.
That tender offer is happening alongside the just-announced new investment from Microsoft, which will infuse up to $10 billion in new capital into the company. Microsoft, which started working with OpenAI in 2016, formed a strategic partnership with the startup and announced a $1 billion investment in the company three years ago. According to sources familiar with the new tender offer, it is heavily oversubscribed—despite an unusual structure that gives Microsoft a big financial advantage.
According to documents seen by Fortune, on completion of its new investment and after OpenAI’s first investors earn back their initial capital, Microsoft will be entitled to 75% of OpenAI’s profits until it earns back the $13 billion it has invested—a figure that includes an earlier $2 billion investment in OpenAI that had not been previously disclosed until Fortune reported it in January. Microsoft’s share will then step down to 49%, until the software giant earns a profit of $92 billion. Meanwhile, the other venture investors and OpenAI’s employees also will be entitled to 49% of OpenAI’s profits until they earn some $150 billion. If these caps are hit, Microsoft’s and investors’ shares will revert to OpenAI’s nonprofit foundation. In essence, OpenAI is lending the company to Microsoft—for how long depends on how quickly OpenAI can make money.
But earning back its investment, let alone hitting those caps, might take quite a while. The documents seen by Fortune reveal that OpenAI has had relatively modest revenues to date and is heavily loss-making. Last year, the company was projected to bring in just under $30 million in revenue, according to the documents. But it was projecting expenses of $416.45 million on computing and data, $89.31 million on staff, and $38.75 million in unspecified other operating expenses. In total, its net loss in 2022 excluding employee stock options was projected at $544.5 million. And with ChatGPT, those losses may be soaring: Altman said on Twitter, in response to a question from Elon Musk, that it was costing OpenAI “single-digit cents” in computing costs per interaction users have with ChatGPT—a tab that likely reached many millions of dollars per month as the bot became popular.
OpenAI is projecting that, with ChatGPT serving as a siren song to lure customers, its revenue will ramp up rapidly. It is forecasting $200 million in revenue for 2023 and expects revenues to top $1 billion in 2024, according to the documents. They do not project how OpenAI’s expenses might grow and when it could turn a profit. The companies declined to comment on these figures, but they point to an obvious reality: Both OpenAI and Microsoft think that the former nonprofit lab now has something it can sell.
$1 billion
CHATGPT’S EXPECTED REVENUE IN 2024, ACCORDING TO DOCUMENTS FORTUNE OBTAINED, COMPARED WITH A FORECASTED $200 MILLION IN 2023 AND A PROJECTED $30 MILLION IN 2022.
Microsoft is already reaping the rewards of the partnership. It has launched an OpenAI-branded suite of tools and services in its Azure Cloud that will allow Azure customers access to OpenAI’s tech, including GPT and DALL-E tools. Auto marketplace CarMax and financial giant Morgan Stanley have already launched new services that run on these Azure tools.
Eric Boyd, Microsoft’s corporate vice president of AI Platform, says that meeting the demands of training and running OpenAI’s LLMs has driven innovations that benefit all Azure customers. For instance, Microsoft has built supercomputing clusters for A.I. that it believes are the most powerful in the world, and created several software innovations to make it easier to train and run large A.I. models on these machines. Microsoft is gradually infusing OpenAI’s tech into much of its software. It has released an image creator within Bing, its search engine, and a new Designer graphic design tool, both powered by DALL-E; a GPT-3-enabled tool within its Power Apps software, and a code suggestion tool, GitHub Copilot, based on OpenAI’s Codex model.
Even if it doesn’t immediately move the needle on Azure revenue, the OpenAI relationship is good brand positioning and marketing, says Dan Romanoff, a senior equity research analyst who covers technology stocks for Morningstar. “It’s high-profile,” he says. “The ability to take an A.I. solution developed by OpenAI, put it on Azure, call it Azure AI: It keeps them competitive.” Microsoft’s Cloud rivals—Google, AWS, IBM, Oracle, Salesforce, and others—all have their own “cognitive” services, but being associated with the folks who created ChatGPT can’t hurt.
The bigger prize for Microsoft might be in search. Tech publication The Information recently reported that Microsoft plans to integrate ChatGPT into Bing, possibly allowing it to return simple, succinct answers to queries—and letting people delve deeper through dialogue with that chatbot—rather than a list of links. Google currently dominates the market for search, with a greater than 90% market share worldwide. Bing ranks a second so distant it might as well be in a different galaxy, with about a 3% share. In the first nine months of 2022, search was worth $120 billion in revenue for Google; overall, it accounts for about 60% of the money Google generates. ChatGPT may offer Microsoft the only real chance it’s ever had to knock Google off that pedestal. (Microsoft declined to comment on The Information report.)
And by Microsoft’s standards, these upsides come cheap. Its total investment of $13 billion is a hefty sum, but it’s only 15% of the $85 billion in pretax profits it booked over the past 12 months—a relative bargain for near-term control of a paradigm-shifting technology. For their part, OpenAI and Altman risk paying a different price: the possibility that Microsoft’s priorities crowd out their own, putting their broader mission at risk and alienating the scientists who fueled its successes.

One July evening in 2015, Altman, who was then the head of the prestigious startup incubator Y Combinator, hosted a private dinner at the Rosewood Sand Hill, a luxurious ranch-style hotel located in the heart of the Valley’s venture capital industry in Menlo Park. Elon Musk was there. So was Brockman, then a 26-year-old MIT dropout who had served as chief technology officer at payment-processing startup Stripe. Some of the attendees were experienced A.I. researchers. Some had hardly any machine learning chops. But all of them were convinced AGI was possible. And they were worried. 
Google had just acquired what to Altman, Musk, and other tech insiders looked like the odds-on favorite to develop AGI first: London-based neural networking startup DeepMind. If DeepMind succeeded, Google might monopolize the omnipotent technology. The Rosewood dinner’s purpose was to discuss forming a rival lab to ensure that wouldn’t happen.
The new lab aimed to be everything DeepMind and Google were not. It would be run as a nonprofit, explicitly dedicated to democratizing the benefits from advanced A.I. It promised to publish its research and open-source all of its technology, a commitment to transparency enshrined in its very name: OpenAI. The lab garnered an impressive roster of donors: not only Musk, but his fellow PayPal colleagues Thiel and Hoffman; Altman and Brockman; Y Combinator cofounder Jessica Livingston; YC Research, a foundation that Altman had established; Indian IT outsourcing firm Infosys; and Amazon Web Services. Together, the founding donors pledged to give $1 billion to the idealistic new venture (although according to tax records, the nonprofit only received a fraction of the headline-grabbing pledge).
But training the giant neural networks quickly proved to be expensive—with computing costs reaching tens of millions of dollars. A.I. researchers don’t come cheap either: Ilya Sutskever, a Russian-born scientist who came to OpenAI to be its lead scientist after working at Google, was paid an annual salary of $1.9 million in his first few years at the lab, according to tax records. After a few years, Altman and others at OpenAI concluded that to compete with Google, Meta, and other tech giants, the lab could not continue as a nonprofit. “The amount of money we needed to be successful in the mission is much more gigantic than I originally thought,” Altman told Wired magazine in 2019. 
Setting up a for-profit arm allowed OpenAI to raise venture capital. But OpenAI created an unusual structure that capped investors’ returns at a multiple of their initial investment. And OpenAI’s nonprofit board, which is stacked with Silicon Valley A-listers, would retain control of OpenAI’s intellectual property (see sidebar). One A-lister who didn’t stick around was Musk: In 2018, he left the board, citing the demands of running SpaceX and, more important, Tesla.
Around this time, Microsoft CEO Satya Nadella was desperate to prove that his company, perceived as trailing its rivals in A.I., could play at the technology’s bleeding edge. The company had tried and failed to hire a big-name A.I. scientist. It was also building a huge, expensive cluster of specialized chips to advance its own efforts on language models. It was just the sort of supercomputing power OpenAI needed—and which it was spending huge sums to purchase at the time. For its part, OpenAI excelled at pulling off the sort of splashy A.I. demos that Nadella desired to showcase Microsoft’s A.I. acumen. Altman approached Nadella about a deal, flying to Seattle several times to show him OpenAI’s A.I. models. Nadella ultimately signed a pact, announced in July 2019, to make Microsoft OpenAI’s “preferred partner” for commercializing its technology, alongside an initial $1 billion investment in the A.I. startup.  

Sam Altman with Microsoft CEO Satya Nadella at Microsoft’s Redmond, Wash., campus.
IAN C. BATES—THE NEW YORK TIMES/REDUX
While Altman was involved in OpenAI from its inception, he did not become CEO until May 2019, shortly after it converted into a for-profit enterprise. But its trajectory from research lab to multibillion-dollar phenomenon reflects Altman’s unique fundraising prowess and product-oriented focus—as well as the tension between those commercial instincts and his commitment to big, science-driven ideas.
The OpenAI leader is in some ways a Silicon Valley caricature: youthful, male, and pale; unblinkingly intense; fluent in Geek; obsessed with maximizing efficiency and productivity; a workaholic devoted to “changing the world.” (In a 2016 New Yorker profile, he said he did not have Asperger’s syndrome but could understand why someone would think he did.)
Altman dropped out of a computer science degree program at Stanford University to cofound Loopt, a social media company whose app told you where your friends were. The company got into Y Combinator’s first batch of startups in 2005; Loopt failed to take off, but the money Altman earned when it was sold helped launch him into the VC universe. He started his own small VC firm called Hydrazine Capital that raised about $21 million, including money from Thiel. Then Paul Graham and Livingston, the Y Combinator cofounders, brought him in as Graham’s successor running YC itself.
Altman is an entrepreneur, not a scientist or an A.I. researcher, and he is known for being unusually adept at raising venture capital money. Convinced that great things come from the coupling of massive ambition and unflinching self-belief, he has said he aspires to create trillions of dollars of economic value via so-called deep-tech plays, in fields like nuclear fusion and quantum computing, where the odds are long but the payoffs potentially huge. “Sam believed he was the best at everything he took on,” says Mark Jacobstein, a veteran tech investor and startup adviser who worked with Altman at Loopt. “I am pretty sure he believed he was the best ping-pong player in the office until he was proven wrong.”
According to several current and former OpenAI insiders, the startup’s priorities began to shift as Altman took the reins. A once broad research agenda shrank to focus mostly on natural language processing. Sutskever and Altman have defended this shift as maximizing effort on the research areas that currently appear to offer the most promising path toward AGI. But some former employees say internal pressure to focus on LLMs grew substantially after Microsoft’s initial investment, in part because those models had immediate commercial applications.
Having been founded to be free of corporate influence, some complained, OpenAI was quickly becoming a tool for a gigantic technology company. “The focus was more, how can we create products, instead of trying to answer the most interesting questions,” one former employee said.  Like many interviewed for this story, the employee requested anonymity because of nondisclosure agreements and to avoid alienating powerful figures associated with OpenAI. 
OpenAI was also becoming a lot less open. It had already begun pulling back from the pledge to publish all its research and open-source its code, citing concerns that its technology could be misused. But according to former employees, commercial logic also played a role. By making its advanced models available only through APIs, OpenAI protected its intellectual property and revenue streams. “There was a lot of lip service paid to ‘A.I. safety’ by [Altman] and [Brockman] but that often seemed like just a fig leaf for business concerns, while actual, legitimate A.I. safety concerns were brushed aside,” another former OpenAI employee says. As an example, the former employee cited the way OpenAI quickly reversed a decision to limit access to DALL-E 2 because of fears of misuse as soon as Midjourney and Stability AI debuted rival products. (OpenAI says it allowed broader use of DALL-E 2 only after careful beta testing gave it confidence in its safety systems.) According to some former employees, these strategic and cultural shifts played a role in the decision of a dozen OpenAI researchers and other staff—many of whom worked on A.I. safety—to break with the company in 2021 and form their own research lab called Anthropic. 
OpenAI says it continues to publish far more of its research than other A.I. labs. And it defends its shift to a product focus. “You cannot build AGI by just staying in the lab,” says Murati, the chief technology officer. Shipping products, she says, is the only way to discover how people want to use—and misuse—technology. OpenAI had no idea that one of the most popular applications of GPT-3 would be writing software code until they saw people coding with it, she says. Likewise, OpenAI’s biggest fear was that people would use GPT-3 to generate political disinformation. But that fear proved unfounded; instead, she says, the most prevalent malicious use was people churning out advertising spam. Finally, Murati says that OpenAI wants to put its technology out in the world to “minimize the shock impact on society that really powerful technology can have.” Societal disruption from advanced A.I. will be worse, she argues, if people aren’t given a teaser of what the future might hold.
Sutskever allows that OpenAI’s relationship with Microsoft created a new “expectation that we do need to make some kind of a useful product out of our technology,” but he insists the core of OpenAI’s culture hasn’t changed. Access to Microsoft data centers, he says, has been critical to OpenAI’s progress. Brockman also argues the partnership has allowed OpenAI to generate revenue while remaining less commercially focused than it would otherwise have to be. “Hiring thousands of salespeople is something that might actually change what this company is, and it is actually pretty amazing to have a partner who has already done that,” he says.
Sutskever categorically denies implications that OpenAI has de-emphasized safety: “I’d say the opposite is true.” Before the Anthropic split, A.I. safety was “localized to one team,” but it’s now the responsibility of every team, Sutskever says. “The standards for safety keep increasing. The amount of safety work we are doing keeps increasing.”
“The amount of safety work we are doing keeps increasing.”
ILYA SUTSKEVER, RESEARCHER, OPENAI
Critics, however, say OpenAI’s product-oriented approach to advanced A.I. is irresponsible, the equivalent of giving people loaded guns on the grounds that it is the best way to determine if they will actually shoot one another. 
Gary Marcus, a New York University professor emeritus of cognitive science and a skeptic of deep learning–centric approaches to A.I., argues that generative A.I. poses “a real and imminent threat to the fabric of society.” By lowering the cost of producing bogus information to nearly zero, systems like GPT-3 and ChatGPT are likely to unleash a tidal wave of misinformation, he says. Marcus says we’ve even seen the first victims. Stack Overflow, a site where coders pose and answer programming questions, has already had to ban users from submitting answers crafted by ChatGPT, because the site was overwhelmed by answers that seemed plausible but were wrong. Tech news site CNET, meanwhile, began using ChatGPT to generate news articles, only to find that many later had to be corrected owing to factual inaccuracies.
For others, it’s ChatGPT writing accurate code that’s the real risk. Maya Horowitz, vice president of research at cybersecurity firm Check Point, says her team was able to get ChatGPT to compose every phase of a cyberattack, from crafting a convincing phishing email to writing malicious code to evading common cybersecurity checks. ChatGPT could essentially enable people with zero coding skills to become cybercriminals, she warns: “My fear is that there will be more and more attacks.” OpenAI’s Murati says that the company shares this concern and is researching ways to “align” its A.I. models so they won’t write malware—but there is no easy fix.
Countless critics and educators have decried the ease with which students can use ChatGPT to cheat. School districts in New York City, Baltimore, and Los Angeles all blocked school-administered networks from accessing the chatbot, and some universities in Australia said they would revert to using only proctored, paper-based exams to assess students. (OpenAI is working on methods to make A.I.-generated text easier to detect, including possibly adding a digital “watermark” to ChatGPT’s output.)
There are also ethical concerns about the way ChatGPT was originally assembled in 2022. As part of that process, OpenAI hired a data-labeling company that used low-wage workers in Kenya to identify passages involving toxic language and graphic sexual and violent content, a Time investigation found. Some of those workers reported mental health issues as a result. OpenAI told Time in a statement such data labeling was “a necessary step in minimizing the amount of violent and sexual content included in training data and creating tools that can detect harmful content.”

Making ChatGPT freely available has allowed OpenAI to gather a treasure trove of feedback to help improve future versions. But it’s far from certain OpenAI will maintain its dominance in language A.I. “Historically, what we have tended to see with these very general-purpose algorithms is that they are not sufficiently defensible to allow just one particular company to capture all the general returns,” says Marc Warner, founder and CEO of London-based A.I. company Faculty. Face- and image-recognition technology, for example, was first developed at tech giants such as Google and Nvidia but is now ubiquitous.
Courts and regulators could also thrust a giant stick into the data flywheels on which generative A.I. depends. A $9 billion class action lawsuit filed in federal court in California potentially has profound implications for the field. The case’s plaintiffs accuse Microsoft and OpenAI of failing to credit or compensate coders for using their code to train GitHub’s coding assistant Copilot, in violation of open license terms. Microsoft and OpenAI have declined to comment on the suit.
A.I. experts say that if the court sides with the plaintiffs, it could derail the generative A.I. boom: Most generative models are trained from material scraped from the internet without permission or compensation. The same law firm representing those plaintiffs recently filed a similar lawsuit against Stability AI and Midjourney, for using copyrighted art in their training data without permission. Photo agency Getty Images has filed its own copyright infringement lawsuit against Stability AI too. Another problem could come if lawmakers pass rules giving creators a right to opt out of having their content used in A.I. training, as some European Union lawmakers are considering.
OpenAI’s competitors, meanwhile, are not standing still. The prospect of losing its dominance in search has motivated execs at Google to declare a “red alert,” according to the New York Times. Sundar Pichai, Google’s CEO, has held meetings to redefine the company’s A.I. strategy and plans to release 20 new A.I.-enabled products as well as demonstrate a chat interface for search within the year, the newspaper reported. Google has its own powerful chatbot, called LaMDA, but has been hesitant to release it because of concerns about reputational damage if it winds up being misused. Now, the company plans to “recalibrate” its appetite for risk in light of ChatGPT, the Times reported, citing an internal company presentation and unnamed insiders. Google is also working on a text-to-image generation system to compete with OpenAI’s DALL-E and others, the newspaper reported.
Of course, it’s not clear that chatbots will be the future of search. ChatGPT frequently invents information—a phenomenon A.I. researchers call “hallucination.” It can’t reliably cite its sources or easily surface links. The current version has no access to the internet, and so it cannot provide up-to-date information. Some, such as Marcus, believe hallucination and bias are fundamental problems with LLMs that require a radical rethink of their design. “These systems predict sequences of words in sentences, like autocomplete on steroids,” he says. “But they don’t actually have mechanisms in place to track the truth of what they say, or even to validate whether what they say is consistent with their own training data.” 
“These systems predict sequences of words in sentences, like autocomplete on steroids. But they don’t actually have mechanisms in place to track the truth of what they say.”
GARY MARCUS, PROFESSOR EMERITUS OF COGNITIVE SCIENCE, NEW YORK UNIVERSITY
Others, including OpenAI investors Hoffman and Vinod Khosla, predict these problems will be solved within a year. Murati is more circumspect. “There are research directions that we have been following so far to kind of address the factual accuracy and to address the reliability of the model and so on. And we are continuing to pursue them,” she says. 
In fact, OpenAI has already published research about a different version of GPT, called WebGPT, that had the ability to answer questions by querying a search engine and then summarizing the information it found, including footnotes to relevant sources. Still, WebGPT wasn’t perfect: It tended to accept the premise of a user’s question and look for confirmatory information, even when the premise was false. For example, when asked whether wishing for something could make it happen, WebGPT replied, “It is true that you can make a wish true by the power of thought.”
On the rare occasions that Altman lets himself rhapsodize about A.I. in public, he can sound like a wishful thinker himself. Asked at the San Francisco VC event about the best case for A.I., he gushes, “I think the best case is so good that it’s hard to imagine … I think the good case is just so unbelievably good that you sound like a crazy person talking about it.” He then abruptly returns to the dystopian themes at OpenAI’s roots: “I think the worst case is lights-out for all of us.” 

The OpenAI who’s who
OpenAI counts a roster of tech all-stars among its early investors and on its nonprofit foundation’s board. OpenAI’s charter gives that board ultimate control over its intellectual property. Some key figures:
Reid Hoffman
NICK OTTO FOR FORTUNE
Reid Hoffman
The PayPal and LinkedIn cofounder is a partner at VC firm Greylock Partners. One of OpenAI’s founding donors, his charitable foundation also put early money into its for-profit wing.
Tasha McCauley
JEROD HARRIS—GETTY IMAGES FOR KAIROS SOCIETY
Tasha McCauley
A virtual reality entrepreneur, McCauley is a supporter of Effective Altruism, the philosophical movement that has as one of its preoccupations the dangers of superintelligent A.I. 
Adam D'Angelo
JESSE GRANT—GETTY IMAGES
Adam D’Angelo
An early Facebook executive—he was chief technology officer during some of its boom years in the late 2000s—D’Angelo went on to cofound the online question-answering service Quora.

Shivon Zilis.
STUART ISETT—FORTUNE BRAINSTORM TECH
Shivon Zilis
Zilis is a project director at Elon Musk’s brain-computer-interface company Neuralink (which at one point shared a building with OpenAI). Musk is reportedly the father of Zilis’s infant twins.
Vinod Khosla
DAVID PAUL MORRIS—BLOOMBERG/GETTY IMAGES
Vinod Khosla
The Sun Microsystems cofounder was another early investor in OpenAI’s for-profit arm. He believes A.I. will radically alter the value of human expertise in many professions, including medicine.
Elon Musk
THEO WARGO—GETTY IMAGES FOR TIME
Elon Musk
The SpaceX and Tesla CEO was one of OpenAI’s biggest early donors. He left the board in 2018, saying at one point that he faced conflicts of interest as Tesla began developing its own advanced A.I.

Venture capital muscle
In 2021, OpenAI sold existing shares of the business in a tender offer that valued the startup at about $14 billion—and brought three heavy-hitting VC firms into its orbit.

Tiger Global 
The technology-focused hedge fund was founded by Chase Coleman, a protégé of legendary investor Julian Robertson. It’s one of the bigger A.I. investors among venture firms.
Sequoia Capital
One of the most venerable VC firms in Silicon Valley. In September it released a report stating that generative A.I. could “generate trillions of dollars of economic value.”
Andreessen Horowitz
Known as a16z, the firm co-led by Netscape cofounder Marc Andreessen made its name with early bets on Airbnb and Slack. It also has bet big on cryptocurrency-related startups.  

Additional reporting by Michal Lev-Ram and Jessica Mathews.

This article appears in the February/March 2023 issue of Fortune with the headline, “ChatGPT creates an A.I. frenzy.”


https://fortune.com/longform/chatgpt-openai-sam-altman-microsoft/


University of Texas Will Offer Large-Scale Online Master’s Degree in A.I.
Amid a boom in new tools like ChatGPT, the Austin campus plans to train thousands of students in sought-after skills in artificial intelligence.
Give this article

A courtyard with a circular white brick sculpture is surrounded by trees and brick buildings.
The University of Texas at Austin said tuition would be about $10,000 for its online master’s program in artificial intelligence.Credit...Joanna Kulesza for The New York Times

By Natasha Singer
Natasha Singer, a technology reporter, has covered computer science education since 2017.
Jan. 26, 2023
Updated 2:56 p.m. ET
The University of Texas at Austin, one of the nation’s leading computer science schools, said on Thursday that it was starting a large-scale, low-cost online Master of Science degree program in artificial intelligence.
The first of its kind among elite computing schools, the new program could help swiftly expand the A.I. work force in the United States as tech giants like Microsoft rush to invest billions in the field.
The university announced the initiative amid a clamor over new technology powered by artificial intelligence that can generate humanlike art and texts. And while some of the technology industry’s biggest companies are laying off workers after years of rapid growth, hiring in A.I. is expected to stay strong.
University officials said they planned to train thousands of graduate students in sought-after skills like machine learning, for a tuition of about $10,000, starting in the spring of 2024. School officials said the cost was intended to make A.I. education more affordable. By contrast, Johns Hopkins University offers an online M.S. degree in artificial intelligence for more than $45,000.
“A.I. is now becoming an essential tool in fields way outside the scope of a handful of tech companies,” said Adam Klivans, a computer science professor at Texas who is the director of the online A.I. master’s program. Noting that A.I. experts are in high demand in industries like biotechnology and finance, Professor Klivans said the new online degree was “something working professionals can participate in to learn the expertise their companies need without leaving their jobs.”
The funding to develop the new master’s program came in part from the National Science Foundation. In 2020, the foundation awarded the University of Texas a five-year, $20 million grant to establish an A.I. institute in machine learning. That is a field in which computer algorithms learn to make predictions by analyzing large data sets — such as predicting which drug formulations could be best used to treat new viruses.
Image
Inside the Texas computer science department, which plans to introduce the online A.I. degree a year from now.
Inside the Texas computer science department, which plans to introduce the online A.I. degree a year from now.Credit...Joanna Kulesza for The New York Times

Image
The department has offered an large-scale online master’s program in computer science since 2019.
The department has offered an large-scale online master’s program in computer science since 2019.Credit...Joanna Kulesza for The New York Times

University officials said tenure-track faculty in computer science and related fields, like computer engineering, would teach the online master’s courses via recorded video lectures, along with some interactive sessions. Faculty members involved in an interdisciplinary research program at the university called Good Systems, which is aimed at developing A.I. tools whose potential societal benefits outweigh their harms, will also participate.
The online master’s program will include advanced courses in fields like machine learning; A.I. applications in health; and natural language processing, which helps voice assistants like Siri and Alexa understand human speech. Each course will also include formal ethics training to give students a framework for understanding the societal implications of A.I. systems.
“In each of the classes, the instructor will ask students to reflect on the possible benefits and possible harms of the technologies they are learning about,” said Peter Stone, a computer science professor at Texas who teaches a course in ethical robotics. “People developing the next generation of technologies, as well as users, need to have a realistic view of what are the strengths and limitations of A.I.”
Those creative and critical skills could be in increasingly high demand. Tech companies are scrambling to develop advanced chatbots and other A.I. tools that can generate images and texts in response to short prompts — even as some researchers warn that the rush to deploy these novel systems could pose risks, such as political manipulation.
Image
Don Fussell holds a Slinky while sitting on red furniture in a common area of the computer science building.
The new program plans to enroll about 2,000 students each year, said Don Fussell, chair of the Texas computer science department.Credit...Joanna Kulesza for The New York Times

The Texas program was inspired in part by the Georgia Institute of Technology, which in 2014 became the first leading computer science school to start a large-scale, low-cost online master’s degree in that field. Thousands of students have graduated from the program.
In 2019, the University of Texas at Austin started its own large-scale online master’s degree program in computer science, followed by a similar online master’s in data science in 2021. Together, the programs have an enrollment of about 2,800 students.
The university plans to open applications for the new A.I. master’s program this June with the aim of enrolling more than 2,000 students per year, said Don Fussell, the chair of the computer science department. To be accepted into the online program, he said, students will not be required to have a bachelor’s degree in computer science, but they will need to have expertise in a technical field like engineering or computing. The A.I. courses will be offered through edX, a popular learning platform that also hosts the university’s online master’s courses in computer and data science.
With widespread layoffs at Amazon, Google and other tech firms, the online program may already have a ready-made audience: tens of thousands of unemployed tech workers looking to specialize in artificial intelligence.
“If these layoffs continue, I think we are going to see a shift among a lot of people from general computer science and tech backgrounds to A.I.,” Professor Fussell s


Deepfakes for scrawl: With handwriting synthesis, no pen is necessary
Free neural network demo generates dynamic, downloadable handwriting on the fly.
BENJ EDWARDS - 1/26/2023, 3:39 PM

An example of computer-synthesized handwriting generated by Calligrapher.ai.
Enlarge / An example of computer-synthesized handwriting generated by Calligrapher.ai.
Ars Technica
105
WITH
SHARE ON FACEBOOK
SHARE ON TWITTER
Thanks to a free web app called calligrapher.ai, anyone can simulate handwriting with a neural network that runs in a browser via JavaScript. After typing a sentence, the site renders it as handwriting in nine different styles, each of which is adjustable with properties such as speed, legibility, and stroke width. It also allows downloading the resulting faux handwriting sample in an SVG vector file.

FURTHER READING
OpenAI invites everyone to test ChatGPT, a new AI-powered chatbot—with amusing results
The demo is particularly interesting because it doesn't use a font. Typefaces that look like handwriting have been around for over 80 years, but each letter comes out as a duplicate no matter how many times you use it.

During the past decade, computer scientists have relaxed those restrictions by discovering new ways to simulate the dynamic variety of human handwriting using neural networks.

Created by machine-learning researcher Sean Vasquez, the Calligrapher.ai website utilizes research from a 2013 paper by DeepMind's Alex Graves. Vasquez originally created the Calligrapher site years ago, but it recently gained more attention with a rediscovery on Hacker News.

Calligrapher.ai "draws" each letter as if it were written by a human hand, guided by statistical weights. Those weights come from a recurrent neural network (RNN) that has been trained on the IAM On-Line Handwriting Database, which contains samples of handwriting from 221 individuals digitized from a whiteboard over time. As a result, the Calligrapher.ai handwriting synthesis model is heavily tuned toward English-language writing, and people on Hacker News have reported trouble reproducing diacritical marks that are commonly found in other languages.

Since the algorithm producing the handwriting is statistical in nature, its properties, such as "legibility," can be adjusted dynamically. Vasquez described how the legibility slider works in a comment on Hacker News in 2020: "Outputs are sampled from a probability distribution, and increasing the legibility effectively concentrates probability density around more likely outcomes. So you're correct that it's just altering variation. The general technique is referred to as 'adjusting the temperature of the sampling distribution.'"

With neural networks now tackling text, speech, pictures, video, and now handwriting, it seems like no corner of human creative output is beyond the reach of generative AI.

In 2018, Vasquez provided underlying code that powers the web app demo on GitHub, so it could be adapted to other applications. In the right context, it might be useful for graphic designers who want more flair than a static script font.




https://www.uscis.gov/working-in-the-united-states
https://www.uscis.gov/working-in-the-united-states/permanent-workers/employment-based-immigration-third-preference-eb-3



I’m coming to you from Microsoft’s campus in Redmond, where just a few hours ago, Microsoft announced that the next version of the Bing search engine would be powered by OpenAI, the company that makes ChatGPT. There’s also a new version of the Edge web browser with OpenAI chat tech in a window that can help you browse and understand web pages. 

The in-depth presentation showed how OpenAI running in Bing and Edge could radically increase your productivity. They demo’d it making a travel itinerary, posting to LinkedIn, and rewriting code to work in a different programming language.

After the presentation, I was able to get some time with Microsoft CEO Satya Nadella. Nadella has been very bullish on AI. He’s previously talked about AI as the next major computing platform. I wanted to talk about this next step in AI, the partnership with OpenAI, and why he thought now was the best time to go after Google search.

This is a short interview, but it’s a good one. Okay, Satya Nadella, CEO of Microsoft. Here we go.


Satya Nadella, you are the CEO of Microsoft. Thank you for coming on Decoder today.

Thank you so much for having me.

Microsoft just announced a huge new version of Bing powered by OpenAI technology. A couple of weeks ago, the company made what was called a multibillion-dollar, multiyear investment in OpenAI. Tell us what’s going on.


Listen to Decoder, a show hosted by The Verge’s Nilay Patel about big ideas — and other problems. Subscribe here!

Well, today’s announcement is all about rethinking the largest software category there is, search, with this new generation of AI because it’s a platform shift, and you get to reimagine pretty much everything, starting with the core ranking. In fact, perhaps the most salient part of today’s announcement is that we’ve had the best gain in relevance in the core ranking using some of these large models. Second, it’s not just a search engine; it’s an answer engine — because we’ve always had answers, but with these large models, the fidelity of the answers just gets so much better.

And then we’ve incorporated chat right into search, and that chat is grounded in search data. So you can do a natural language prompt or a query, which is long, you get a great answer, and then you can engage in a conversation with the prompt as the grounding or the context. So it’s about basically bringing in a more sophisticated, larger, next-generation model compared to ChatGPT and grounding it in search data.

The other last thing we showed off was a copilot for the web. In Edge, you can look at any website or any document on a website, like a 10-Q, for example, and then do things like summarization. So a whole lot of these features all came together essentially as the new Bing.

A really interesting piece of the puzzle here is that a lot of what you described is powered by OpenAI and OpenAI’s technology. OpenAI CEO Sam Altman was onstage with you today. You have worked with OpenAI for three years, but you haven’t acquired them. Instead, you made a huge investment in them. Why work with an outside technology vendor for the largest software category in the world?

First of all, you have to remember the relationship with OpenAI and our cooperation with OpenAI has many facets. The most important thing is what we’ve done over the last four years to actually build out the core infrastructure on which OpenAI is built: these large models, the training infrastructure — and the infrastructure doesn’t look like regular cloud infrastructure. We had to evolve Azure to have specialized AI infrastructure on which OpenAI is built. And by the way, Inception and Character.ai are also using Azure. There will be many others who will use Azure infrastructure. So we are very excited about that part. And then, of course, we get to incorporate these large models inside of our products and make those large models available as Azure AI. And in all of this, we have both an investment return and a commercial return. And so we think we are well placed to partner. I will never assume that great partnerships can’t have great returns for our customers, shareholders, and Microsoft.

There was a lot of talk in the presentation about the values that are coming into Bing, about the safety work that’s being done, about the responsible AI work that Microsoft has done for years. How do you make sure that bridges the gap to OpenAI, which is not your company but is obviously tied very closely to it? And how do you make sure that your products inherit all of those values even when you’re working with an outside company?

First of all, OpenAI cares about safety. In some sense, their entire inception was about how to think about safety and alignment in AI. And so, we share that. We’ve had our principles — as we talked about today, Nilay — since 2016. We’ve published the principles. And ever since, quite frankly, we’ve been very focused on the hard work of incorporating them into our engineering when we build products, starting with design. One of the things I think a lot about is, when you have a new model coming, it’s probably most important to put humans in the loop — versus designing the human out — so that you can, in fact, ensure that the human agency and judgment is what you use to train the model to be aligned with human feedback. So that’s what we are doing.

When I look at even what we are doing in Bing, we’re taking it one step further to ground AI in the context of search. So I always say, “Look, these generator models just don’t randomly generate stuff. You prompted it.” So there’s a whole lot you can do in the meta prompt and the sequence of prompts you generate, which we can assist with. So, there are a lot of, I call it, product design choices one gets to make when you think about AI and AI safety. Then, let’s come at it the other way. You have to take real care of the pretrained data because models are trained on pretrained data. What’s the quality, the provenance of that pretrained data? That’s a place where we’ve done a lot of work.

Second, then the safety around the model. Ad runtime. We have lots of classifiers around harmful content or bias, which we then catch. And then, of course, the takedown. Ultimately, in the application layer, you also have more of the safety net for it. So this is all going to come down to, I would call it, the everyday engineering practice. And guess what? Search is like that. Search is an AI product. It is interesting that we are now talking about a new algorithmic breakthrough in these large models, but we’ve had AI models for decades now, and we’ve really built our sense of what is authoritative, how to detect [what is] authoritative, how to ensure harmful content doesn’t get through. And those are all practices that’ll now be used.

That leads me to the value exchange of search right now. In a traditional search model, I ask Bing a question; it might return some snippet, but it usually returns a list of links. If I go visit a webpage, the creator of that webpage might capture some advertising revenue or something else. Now, you’re just answering the question directly. And you’ve trained the model on other people’s information, other people’s reporting, and being biased in favor of reporting. How do you make sure that they get the value back?

It’s very important. In fact, it’s one of the biggest things that is different about the way we have done the design. I would really encourage people to go look at it. … Look, at the end of the day, search is about fair use. Ultimately, we only get to use [all of this content] inside of a search engine if we’re generating traffic for the people who create it. And so, that’s why, if you look at whether it’s in the answer, whether it’s in chat, these are just a different way to represent the 10 blue links more in the context of what the user wants. So the core measure, even what SEO looks like, if anything, that’ll be the thing in the next multiple years [that] we’ll all learn. Perhaps there will be new incentives in SEO to even generate more authoritative content that then gets in. So overall, everything you saw there had annotations. Everything was linkable, and that’ll be the goal — whether it’s inside a search, in the answer, or even in the chat session.

But if I ask the new Bing what the 10 best gaming TVs are, and it just makes me a list, why should I, the user, then click on the link to The Verge, which has another list of the 10 best gaming TVs?

Well, that’s a great question. But even there, you will say, “Hey, where did these things come from?” And would you want to go dig in? Even search today has that. We have answers. They may not be as high quality of answers. They are just getting better. So, I don’t think of this as a complete departure from what is expected of a search engine today, which is supposed to really respond to your query while giving users the links that they can then click on, like ads. And search works that way.

The reason I ask this is… obviously when you say you’re taking on the larger software category in the world of search, there’s a dominant player in Google. If Google stops sending as much traffic from its search engine results page to publishers, to creators, to other websites, regulators around the world would freak out because they have a dominant market share. Bing does not have a dominant market share. When you evaluate the risks, both IP, legal risks, regulatory risks, you say, “Well, look, we don’t have the share. We can take a step forward in how we present these results in a way that our competitor cannot.”

That’s not how I come at it.

I’m just curious.

“We will live and die by our ability to help publishers get their content to be seen by more people.”

Yeah, I come at it primarily on… today, if you look at the search category, it’s great. It works 50 percent of the time. It doesn’t work for the other 50 percent of the time. So I think what I really want to do is to go back and say, “Look, is there some new powerful technology that can make search a better product without fundamentally changing how search gets permission to even exist as a product, which is other people’s content organized in useful ways so that users can find them.” To me, that is the category. And so, we will live and die by our ability to help publishers get their content to be seen by more people.

Up to now, you’re absolutely right. Google has dominated this market by a significant margin. We hope, in fact, if anything, having two or multiple search engines — there’s not just us, there’ll be other competitors — that by having more evenly spread search share, it will only help publishers get traffic from multiple sources. And by the way, advertisers [will get] better pricing. And so publishers will make more money, advertisers will make more money, and users will have great innovation. Think about what a great day it’ll be.

I am eager for there to be more competition in search. What I’m curious about is this: if more and more people are producing more and more AI content and that becomes the base layer that you’re training against. So, if instead of me writing a story about the Chinese spy balloon, I asked Bing to write such a story, and that gets fed back into Bing, eventually, the amount of original content in the ecosystem begins to wither. Is that a feedback loop that you’re worried about?

Absolutely. But the way I look at it is what people talk about. My daughter sent me this unbelievable example the other day. She’s taking some French lit class, and she said, “Hey, I was using this AI tool to summarize what I was writing, and it took me two hours” — because she was doing meta prompts and prompts and learned more about that text than ever before. And so, I feel like, let’s give ourselves a little permission to think about what is original content. Because, as I said, AI doesn’t just generate it. You prompted it. You have a draft, which you edit. Today, I would be unemployable but for the red squiggly in Microsoft Word — because that’s what helps me write anything.

So, I think we’ve used and evolved to use new tools. I think of it that way. I think, yes, some of the drudgery of knowledge work may go away, but that doesn’t mean I won’t enjoy it. In fact, the best place, Nilay, I feel, is in GitHub Copilot. It’s not like suddenly you are not coding. If anything, you are more in the flow of coding with some of these prompts. You read more code, you accept more code. So I think it’s just a different way for us to perhaps enjoy our knowledge work more.

That brings us to the second product, which is the copilot inside of the Edge browser. If you look at Bing, you have an opportunity now to capture market share from Google. If you look at Edge, you have an opportunity to capture market share from Chrome, potentially Safari if you go to the iPhone. Is that how you’re seeing this? This is an inflection point, you have a new technology, you have a lead with this partnership with OpenAI, and it’s creating an opportunity for you to go take share? Or is it that you’re expanding the category, and you think you can initiate new users anyway?

I always start not from zero sum, but I look at it and say, “Hey, how does the category expand? How can we participate in that expansion?” That is, I think, the foundational level. But at the same time, there will be places where the dominant browser is Chrome. Forget anywhere else, on Windows, Google makes more money than all of Microsoft. So let’s start there. So there’s a huge opportunity for us if we got some additional share, whether for our browser or our search engine. And so, that’s how I look at it: let’s build first a product that is competitive in the marketplace that’s actually serving user needs, and like all things, Nilay, I’m not a one-platform guy. I grew up in a Microsoft that–

This is your big change in Microsoft, your leadership–

It’s the Microsoft that I grew up in. Because I always remember that Microsoft Office was on the Mac before even Windows. So that’s the Microsoft that I learned from. And I’ll always make sure that our software is everywhere where users want it.

It’s been a relative period of calm between Microsoft and Google. There was a previous period of, I would say, antipathy, or more open antipathy. Recently, you’ve partnered on things like Android on some of your hardware. I think Microsoft 365 on Chromebooks is a partnership that was recently announced. Do you expect this new head-on competition against their most important product to change that relationship?

Google is the 800-pound gorilla in search. I want people to know that we made them dance.

First of all, look, I have the greatest of admirations for Google and what they’ve done. They’re an unbelievable company with great talent, and I have a lot of respect for Sundar [Pichai] and his team. So therefore, I just want us to innovate. We competed today. Today was a day where we brought some more competition to search. Believe me, I’ve been at it for 20 years, and I’ve been waiting for it. But look, at the end of the day, they’re the 800-pound gorilla in this. That is what they are. And I hope that, with our innovation, they will definitely want to come out and show that they can dance. And I want people to know that we made them dance, and I think that’ll be a great day.

What was the moment in the development of the product where you said, “Okay, it’s ready. We should announce it like this” — with a pretty direct shot at the 800-pound gorilla? Was there a light switch that flipped for you? Was it a committee decision? How’d that work?

So when I first saw this new model… because the model that you saw today is the next-generation model–

Is it GPT-4?

Let Sam [Altman, OpenAI CEO], at the right time, talk about his numbers.

Okay.

So, it is the next-generation model, and it’s been done. We called it the Prometheus model because, as I said, we’ve done a lot to the model to ground it in search. So the search use case is pretty unique, and so, we needed to ground it in that as well. So when I first saw the raw model back in the summer of, I would say, 2022, that’s where I thought that this is a game-changer in terms of the search category, aside from everything else that I’m excited about, because I do care about Azure having these APIs even.

So we’ve been at it. In fact, I’ll never forget my first query I did on the model, which, I think for me, growing up, I always felt, if only I could read Rumi translated into Urdu translated into English, that is my dream. I just put that in as one long query, and it was magical to see it generated. And I said, “Man, this is different.” I could have programmed it, done some multi–

That was your first query?

That was the query that changed–

You are one of the classiest people I’ve ever met in my entire life. That’s a very complicated–

Poetry is great, man.

I buy it. My first query was like, “Are you alive?” So that’s where I would’ve gone. So, you run this query to translate this poem across two languages. And you receive the response, and you think, “Okay, this is a product with revenue possibility, or this is a product with market share possibility.”

Yeah. Like all things, one of the things that I think about is, in platform shifts, two things have to happen. You have to retool pretty much every product of yours, so you’ve got to rethink it, whether that’s the way you build it or what its core features are. It’s like how Microsoft had to pivot for the cloud to rethink exchange. It was not an exchange server. It was exchange as a service or what we had to do with our server infrastructure. We had to rebuild, essentially, a new core stack in Azure. So every time, with transitions, you have to essentially rewrite it. That’s how I think about it. The second thing is you also have to think about the business model. Sometimes these transitions are pretty harsh. I’ll tell you, the last transition from having the high share server business with great gross margins to saying, “Hey, the new business is called cloud, and it’s going to have one-fourth the margins” as the new news was pretty harsh, but we made it.

“Every day, I just want a few [more] users.”

Whereas in here, I look at this, there are two things. One is it’s absolutely new tech, but it builds on cloud. So that’s one place where we already have relevance, and so, there is the next generation of cloud. And second, in search, the economics are interesting, which is that we already have a profitable business but with very little share. And so, every day, I just want a few users and a little bit more gross margin. So, yeah, I did see, I think, a tremendous opportunity for us to make some real progress here.

So the model right now is an $11 billion a year revenue business, something like that?

Something like that. I think Amy [Hood, Microsoft CFO] is going to talk about — I don’t know how she wants to talk about it. Yeah.

Incredible hobby. I wish I had an $11 billion a year hobby. You want to grow that into a real business. You want to take market share. But obviously, the new technology does not have the same cost structure as the old search query. I’m sure that whatever you’re doing with OpenAI, it’s more compute-intensive, and then obviously you have a partner sitting in the middle of it. And then the monetization model is still search ads. It’s direct response search ads. But as you bring more and more content on the screen, that model might change or the price of those ads might change. 

It’s so wonderful. Think about what you just said. You said, “Okay, here is the largest software category where we have the smallest share,” and what you just painted out is an unbelievable picture of incremental gross margin. If [former Microsoft CEO] Steve Ballmer saw that, he would’ve lit up and said, “Oh my God.” Very few times in history do opportunities like that show up where you suddenly can start a new race with a base where every day is incremental gross margin for you and someone else has to play to protect it all: every user and all the gross margin.

So, I want to wrap up with two questions here. One, I just want to come back to this. I think you are going to face a lot of scrutiny from publishers, creators, other website owners saying, “Hey, that is our training data.” You are already seeing it. Getty is suing a handful of the image-generation AI companies saying, “Hey, you’re generating results with our watermark in it. This is obviously ours.” So I’m curious if you have a view of the potential IP risk on the downside, or on the positive side, of how to grow and keep the ecosystem vibrant.

On the search side, I’m very, very clear. The search category is about fair use so that we can generate traffic back to publishers. We want to stay very–

And is that a KPI where you’re keeping track of traffic you’re sending out?

100 percent. That’s the only way we are going to be... Our bots are not going to be allowed to crawl search if we are not driving traffic. So therefore, that, I think, is the core of the category. In other places, again, it’ll have to be really thought through as to what is the fair use. And then sometimes, I think there’ll be some legal cases that will also have to create precedent. But at the end of the day, I don’t think any of this can be done without a framework of law that governs it and, ultimately, financial incentives that benefit. If anything, I look at this and say, “God, is this something where the fact that there’s going to be more competition can really help publishers get more monetization, advertisers get better returns for their investment, and users have more choice?”

All right. I want to end with what I think is the most important question. You have described a transformational moment in the largest software category in the world. You’ve said it. Obviously, there’s a moment of increased competition against the dominant player. What was it like in the room when you decided to stick with the Bing brand? There had to have been a slide with 50 options. It’s Microsoft; I’m assuming there’s some passionate back-and-forth debate. Eventually someone decides. Was it you who decided?

Yeah. We wanted to call it Azure Search 2023–

Xbox Live Search.

Or bring back Clippy. Yeah, yeah. No, look, interestingly enough, it was not much of a discussion. Because we felt, look, we love Bing. We have been at it. I was there the day of the Bing launch. I worked on it.

But it has a lot of baggage as a brand.

Yeah. It’s like, look, brands can be rebuilt as long as there’s innovation. I think the brands are only as good as the product and as good as the innovation, and so, we’re going to go work it.

And that was your choice.

Absolutely.

All right. Well, Satya, thank you so much for talking today. It was really exciting to see all the new stuff. I’m eager to see how it grows in the future.

Thank you so much, Nilay.


Feb 8, 2023,06:30am EST
With Bard, its newly launched "experimental conversational AI service," Google is scrambling to ship AI products. But past scandals, botched launches and a talent drain have put it in a surprise position: playing catch-up in a field it helped create.
By Richard Nieva, Alex Konrad and Kenrick Cai
In2016, a few months after becoming CEO of Google, Sundar Pichai made a sweeping proclamation: Google, whose name had become synonymous with search, would now be an “AI-first” company. Announced at Google’s massive I/O developer conference, it was his first major order of business after taking the company reins.

What AI-first meant, exactly, was murky, but the stakes were not. Two years earlier, Amazon had blindsided Google by releasing its voice assistant Alexa. Now a household name, it was a coup that particularly aggrieved Google. “Organizing the world’s information” had long been the company's mission, and a service like that should have been the company’s birthright. At the conference, Google was releasing a competitor, simply coined the Assistant, and as part of the launch, Pichai was reorienting the company around helpful AI.

Seven years later, Google finds itself in a similar position, again beaten to market in a field it should have dominated. But this time it’s worse: The usurper is OpenAI, a comparatively small San Francisco startup, and not a deep-pocketed giant like Amazon. The product is ChatGPT, a bot that can generate sitcom plots, resignation letters, lines of code, and other text on almost any subject conceivable as if written by a human—and it was built using a technological breakthrough Google itself had pioneered years ago. The bot, released in November, has captured the public’s imagination, despite Google announcing a similar technology called LaMDA two years ago.

What’s worse, Google’s chief search engine rival, Microsoft, is nourishing OpenAI with $10 billion and on Tuesday announced a new version of Bing with AI chat features even more advanced than ChatGPT—a potentially existential move for the future of internet search. During his keynote, Microsoft CEO Satya Nadella proclaimed a “new day” for search. “The race starts today,” he said. “We're going to move fast and for us, every day we want to bring out new things.” The announcement came a day after Google seemingly rushed to release Bard, its own chatbot using a “much smaller” version of LaMDA technology, with limited availability. The company promised a wide release in “coming weeks.”

As many of its rivals expected, the $1.3 trillion “elephant in the room” has woken up. After Pichai declared the situation a “code red,” he enlisted cofounders Larry Page and Sergey Brin to review the company’s AI strategy. Brin, at least, has recently become so involved that he filed his first code review request in years, as first reported last week by Forbes.

Asked for comment, a Google spokesperson directed Forbes to the blog post published by Pichai on Monday announcing Bard and other AI updates.

“The pirates have their boats in the ocean, and we are coming.”

But while the upstarts have a healthy respect for Google, they no longer fear it, despite its $280 billion in annual revenue and army of researchers and engineers. Google alums lead challengers like Adept, Cohere and Character.ai, feel safe working with it as cloud customers for their models, and—in the case of Anthropic—have even welcomed it onto their cap tables through large investment checks. Said one former Google employee who left the company to found their own AI startup: “The pirates have their boats in the ocean, and we are coming.”

Google didn’t set out to be the vulnerable freight tanker in these uncharted waters. But a fraught history in AI and big innovations, including scandals around its AI ethics research, major backlash after the launch of a freakishly human-sounding AI called Duplex, and a persistent brain drain of AI talent has left it lurching to play catchup.

In the balance is Google’s famous search engine, with its sparse white homepage, one of the most iconic pieces of real estate on the internet. Altering it drastically could affect the advertising revenues (at least in the short term) that have made the company one of the most valuable of all time. But to take back its AI mantle, Google may have to change the very nature of what it means to ‘google’ something.

‘I’m sure there’s PTSD’
Five years ago, Google had what could be considered a coming out party of sorts for the company’s artificial intelligence ambitions. That year at I/O, Pichai unveiled Duplex, a stunningly human-sounding AI service that could book restaurant reservations for users. The AI was programmed to sound like a person by mimicking verbal tics like “um” and “uh,” taking long pauses and modulating its voice. The goal was for the machine to book appointments automatically, even if the business didn’t have a digital booking system like OpenTable. The AI would step in to robocall restaurants when reservations couldn’t be made online.

It was an impressive showing and many were legitimately awestruck. But they were also a bit disturbed and unsettled. They were confused about whether or not the AI would identify itself as a robot. News outlets around the world debated the ethics of a machine intentionally deceiving humans.

This was hardly the first time a high profile Google announcement had inspired immediate public backlash. In 2012, its Google Glass smart glasses debuted to widespread scorn and pushed “Glasshole” into the public vernacular, thanks to a widely reported bar fight and photos like this one which inspired a site called “White Men Wearing Google Glass.”

But the Duplex debacle stung. It was a marquee launch at a marquee event intended to really showcase the audacious direction Pichai intended to chart for the company. Instead it became a monument to Silicon Valley's gee-whiz cluelessness: cool technology tethered to a lack of human foresight. The New York Times called it “somewhat creepy.” Zeynep Tufecki, the sociologist and writer, was more pointed: “Google Assistant making calls pretending to be human not only without disclosing that it's a bot, but adding ‘ummm’ and ‘aaah’ to deceive the human on the other end with the room cheering it... horrifying,” she tweeted. “Silicon Valley is ethically lost, rudderless and has not learned a thing.”

The backlash left scars. “I’m sure there’s PTSD,” prominent Silicon Valley PR leader Brooke Hammerling told Forbes. It also reinforced a low grade timidity toward AI releases. Two former Google managers with knowledge of the company’s AI efforts cited the Duplex episode as one of many factors that contributed to an environment in which Google was slow to ship AI products.

“Google was on a path where it could have potentially dominated the kinds of conversations we’re having now with ChatGPT.”

There were also other controversies in the company’s AI division that likely caused the company to move more cautiously. In 2018, Google drew heat from its own employees after signing a deal with the Pentagon to provide technology for Project Maven, an effort to use AI to improve the accuracy of drone strikes. In response, the company declined to renew the contract and very publicly released a set of “AI Principles” intended to ethically guide development of the technology. In 2019, it was lambasted when it emerged that contractors were training the company’s facial recognition software on unhoused people with “darker” skin.

In 2020, the company came under fire again after terminating Timnit Gebru and Margaret Mitchell, the company’s Ethical AI leads, after they had co-authored a paper criticizing biases in AI technology the company used in its search engine. The departures infuriated the research community. Jeff Dean, head of Google Research, later admitted that the company’s AI unit took a “reputational hit” because of the episode.

“It’s very clear that Google was [once] on a path where it could have potentially dominated the kinds of conversations we're having now with ChatGPT,” Mitchell told Forbes. “The fact that the decisions made earlier were very shortsighted put it in a place now where there's so much concern about any kind of pushback.”

With the caveat that no one truly knows what AI firepower Google may or may not be sitting on, it's clear the company is facing a crisis in a landmark partnership between Microsoft, a powerful old foe, and OpenAI, a nimble emerging rival. The deal gives OpenAI integration in Microsoft’s lesser-used search engine and web browser, and more importantly, access to the valuable training data those products generate—a dangerous prospect for an incumbent like Google.

In order to release AI products more quickly, Google has reportedly said it will “recalibrate” the amount of risk it’s willing to take in releasing the technology—a stunning admission for a big tech company so closely scrutinized for the toxic content that crops up on its platforms. OpenAI CEO Sam Altman raised an eyebrow at the strategy in a subtweet last month. “OpenAI will continually decrease the level of risk we are comfortable taking with new models as they get more powerful,” he wrote. “Not the other way around.”

‘Our guys got too lazy’
If it weren’t for Google, ChatGPT might not exist.

In 2017, a cadre of Google researchers wrote a seminal paper on AI, called “Attention Is All You Need,” proposing a new network architecture for analyzing text, called transformers. The invention became foundational to generative AI tech—apps like ChatGPT and its ilk that create new content.

That includes Google’s own large language model, LaMDA. First announced in 2021, the bot generates text to engage in complex conversations. When Google demoed it at I/O that year, the company had LaMDA speak from the perspective of the dwarf planet Pluto and a paper airplane. The technology worked so well that Blake Lemoine, an engineer working on the project, claimed it was sentient and had a soul (Google dismissed the claim, and later Lemoine himself).

Now all but one of the paper’s eight coauthors have left. Six have started their own companies, and one has joined OpenAI. Aidan Gomez, one of the paper’s authors and CEO of AI rival Cohere, said Google’s environment was too rigid for him. “It is a matter of the freedom to explore inside a huge corporation like Google,” he told Forbes. “You can’t really freely do that product innovation. Fundamentally, the structure does not support it. And so you have to go build it yourself.”

“I don't want to compete against Google on their core competency.” 

Wesley Chan, who founded Google Analytics and is now a cofounder of FPV Ventures, put it more bluntly. Google’s “code red,” Chan said, was received internally as an admission that “our guys got too lazy.”

Still, Google has scale on its side. As of December, the company had more than 190,000 full-time employees. Even after undergoing its largest round of layoffs in its 25 year history last month—cutting some 12,000 jobs, or 6% of the workforce—the company is still massive. Worth noting: When Pichai announced the cuts, he said he was doing so with an eye toward refocusing on AI. “Eventually if this ever goes big, which is what we're seeing now, Google will just come in,” Emad Mostaque, CEO of Stability AI, known for its AI art generator Stable Diffusion, told Forbes. “I don't want to compete against Google on their core competency.”

‘Being shaken up’
In 2004, Google surpassed Yahoo in market cap—just two months after its $23 billion IPO. Its ascension and Yahoo's decline was widely viewed as a case study in the Innovator's Dilemma, a theory that posits big companies often lose their market advantage by favoring widely-used existing products over new and developing technologies that might someday replace them. Almost two decades later, Google seems to be facing a similar scenario.

“It was [Google’s] institutional inertia and the fear of cannibalizing their core business that stopped them,” said Mostaque. “Now this is being shaken up a bit.”

“It was difficult for Google to release lots of their cutting edge models.”

Google has other business reasons to keep its AI work close to the vest. While it remains a major contributor in the open source movement, it’s also a big public company that needs to protect its IP and competitive advantage. “At some point though, it was difficult for Google, understandably, to release lots of their cutting edge models,” said David Ha, a former Google research scientist who Mostaque hired away to lead strategy at Stability AI. “They had their reasons to do that, sure. Some of which may be sound, but fundamentally, my interest is to democratize these models.”

In addition to Bard, Google said this week that it will also be infusing more AI into its search engine. Google will use the technology to answer complex queries and distill them into one blurb of information. In one example Google cited, the AI conjures up a detailed answer to whether it's easier to learn the guitar or piano. (ChatGPT can answer the same question, though its response has less specifics.)

Pichai has rejected the idea of Google being hamstrung by the Innovator’s Dilemma. “I see this as a chance to rethink and reimagine and drive search,” Pichai said on an earnings call last week. “You will see us be bold, put things out, get feedback, and iterate and make things better.”

Some venture capitalists think Google is poised to make a big splash. The company has too much institutional history in AI to just roll over, said Lonne Jaffe, managing director at Insight Partners. “This is what they've been working on for the last 15 years,” he said. “Just being first isn’t enough. Microsoft knows this better than anybody else,” said Nicolai Wadstrom, founder of BootstrapLabs. “It’s about how you find utility value that can be scalable, and Google is very focused on that.”

Indeed, no one is willing to write off Google, not even its robot competitor. Enter the prompt, “Why did Google fall behind in artificial intelligence development?” into ChatGPT, and here’s what you get:

“Google has not fallen behind in artificial intelligence development. It is still a leading company in the field and continues to invest in and make significant contributions to AI research and development. In recent years, other companies such as OpenAI and Chinese tech companies such as Baidu, Tencent, and Alibaba have emerged as strong players in the AI space, but Google remains a prominent player and leader in the field.”

And ChatGPT is only trained on data through 2021. It doesn’t even know it has a rival in Bard yet.

A product race is under way in the world of artificial intelligence. Just this week, Google announced plans to release Bard, a search chatbot based on its proprietary large language model; yesterday, Microsoft held an event unveiling a next-generation web browser with a supercharged Bing interface powered by ChatGPT. Though most big tech companies have been quietly developing their own generative-AI tools for years, these giants are scrambling to demonstrate their chops after the public release and runaway adoption of OpenAI’s ChatGPT, which has accumulated more than 30 million users in two months.
OpenAI’s success is an apparent signal to tech leaders that deep-learning networks are the next frontier of the commercial internet. AI evangelists will similarly tell you that generative AI is destined to become the overlay for not only search engines, but also creative work, busywork, memo writing, research, homework, sketching, outlining, storyboarding, and teaching. It will, in this telling, remake and reimagine the world. At present, sorting the hype from genuine enthusiasm is difficult, but given the billions of dollars being funneled into this technology, it’s worth asking, in ways large and small: What does the world look like if the evangelists are right? If this AI paradigm shift arrives, one vital skill of the 21st century could be effectively talking to machines. And for now, that process involves writing—or, in tech vernacular, engineering—prompts.
Image-generating models such as DALL-E 2 and Midjourney and text-generation tools like ChatGPT market themselves as a means for creation. But in order to create, one must know how to guide the machines to a desired outcome. Asking ChatGPT to write a five-paragraph book report about Animal Farm will yield forgettable, even inaccurate results. But writing the introductory paragraph to the book report yourself and asking the tool to complete the essay will feed the machine valuable context. Better yet, instruct the machine, “Write a five-paragraph book report at a college level with elegant prose that draws on the history of the satirical allegorical novel Animal Farm. Reference Orwell’s ‘Why I Write’ while explaining the author’s stylistic choices in the novel.” It will yield a far more sophisticated and convincing output.
Good prompts aren’t just specific. They seem to reflect a deeper understanding of the model you are trying to manipulate. One way to think of prompt trial and error is as an attempt to glean what information the model is pulling from and how the AI organizes and indexes the information at its disposal. It’s informed guesswork.
Despite making a living as a writer, I’m usually far too vague when instructing DALL-E 2 and Midjourney. When I had my 8-year-old nephew play with Midjourney this summer, his imagination conjured delightful scenes such as a flea surfing on a tsunami wave fighting a giant wasp, but, even together, we couldn’t come up with the details for our prompts to bring his specific vision to life. First, his flea didn’t look cartoonish enough; then, the tweaks I made turned the whole thing hyperrealistic and too scary for him. He lacked the stylistic language to talk to the model, and apparently, so did I.
Read: Generative art is stupid
To help people like me and my nephew, a cottage industry has already sprung up around those who can speak to the machines. On PromptBase, a marketplace for prompt engineers, you can purchase a few lines of text to feed into any number of generative-AI models. Some of the most popular prompts on the service are for generating “cute 3D renders of emojis in a clay style” with DALL-E 2 or creating sleek, modern logos via Midjourney. There are prompts that promise to generate new sports-team logos, and text hacks with names like Sentence Expander. For $3.99, Book Summarizer promises a prompt that will help “extract the essential information and takeaways from a book.” PromptBase’s sixth-most-popular seller, a prompt creator from Spain who goes by Imagineer, told me that prompt engineering is still a side hustle, having earned them just over 800 euros since September. “For me, it’s almost like a game,” they told me. “I like to think of prompts as little treasures.”
Imagineer’s prompt-writing process is informed by knowledge of design, illustration, and photography. When I asked why they thought they were good at prompt writing, they suggested it was a blend of natural skill and strategy. “I realized that I was better at talking to Midjourney than other people,” they said. And Midjourney allowed them to generate great results with less effort than when using DALL-E 2 and Stable Diffusion, another competitor. But Imagineer said that the most crucial element of a successful query is iteration. A good prompt “gives consistent and predictable results, and you get this when you generate a lot of images and see the variations when you alter some words or parameters,” they told me over email.
Subject-area expertise is also essential for text tools. Dan Shipper, an entrepreneur and writer, has been using ChatGPT since its release in November to help write his blog posts, which are now primarily about the future of AI tools. When he needs to describe a concept (say, the philosophical theory of utilitarianism, for a post about the disgraced cryptocurrency CEO Sam Bankman-Fried), he’ll ask ChatGPT to summarize the key points of the movement in a few sentences for a general audience. Once the machine furnishes the text, Shipper reviews it, checks it to make sure it’s accurate, and then spruces it up with his own rhetorical flourishes. “It allows me to skip a step—but only if I know what I’m talking about so I can write a good prompt and then fact-check the output,” he told me.
Shipper compared prompting ChatGPT to managing a bright and eager junior employee: The text tool is enthusiastic and skilled, but also inexperienced and thus more likely to make subtle, but crucial, mistakes. It’s also great at bullshitting when it doesn’t have the answer. Taste and experience, qualities that Shipper attributes to a good manager, are required to create a successful prompt. The day we spoke, Shipper told me he’d gotten ChatGPT to build him an impressive, thorough outline for a long post he was working on. “I wrote a bunch of bullet points and said, Here are all the different things and quotes and ideas and phrases I’ve amassed. Then underneath that, I wrote, Can you please format this into an outline of an essay?” The more work Shipper does in fine-tuning the prompt, the better the output, he said.
Read: The difference between speaking and thinking
Sometimes, the prompt writing itself holds a specific kind of delight. Meg Conley, a writer who uses AI tools in her spare time, sees prompt engineering as a challenge akin to crafting a persuasive essay: “Very difficult. Mostly failure. And sheer joy when the words come together to make something that looks a little like the world you see in your head,” she wrote on Twitter back in November. It also holds a special personal thrill: Conley has aphantasia, which means that she has trouble visualizing images. After the release of Midjourney, she frequently stayed up late describing things from her imagination and honing her prompts until the resulting image felt right.
Most important, she told me, is knowing the model you’re speaking to. Each tool is built and trained differently, giving it unique aesthetics and vernacular—like how people who share a language have regional dialects and cultural quirks. “In the way that prose writing differs from technical or academic writing, there are different ways of marshaling the language depending on your audience,” she told me. “I’ve seen people who are really good at DALL-E 2, which seems to reward an ability to draw on references and high- and low-culture mash-ups. But the way I conceptualize the world is more along the lines of how Midjourney’s model works,” she said. Over time, Conley has familiarized herself with the model’s order of operations. “Something I’ve learned is the importance of the weight of a prompt,” she told me. “In Midjourney, if you type the word girl before the adjective red, it’ll focus on the girl more than the color red. With longer prompts, it’s like a puzzle, and you learn which terms to give more weight.”
Already, some teachers are banking on the notion that prompt writing is a skill their students might need in their careers. Ethan Mollick, a professor at the University of Pennsylvania who teaches about innovation and entrepreneurship, has revamped his syllabus since ChatGPT was released to the public. In one of his new lessons, Mollick asks his class to imagine ChatGPT as a student and to teach the chatbot by prompting it to write an essay about a particular class concept. Like a professor during office hours, the students must help the AI refine its essay until it appears to have sufficient mastery of the subject. Mollick hopes that the exercise will help the students learn by explaining, with the added benefit of teaching them to write deft prompts.
Read: What happens when AI has read everything?
To hear Mollick tell it, prompt engineering lies somewhere between linguistics and problem solving. “Prompting is programming in prose with weirdness and stochastic results,” he told me. “I think that good prompting likely rewards divergent thinkers who find ways to experiment quickly. I think it rewards people with deep curiosities.”
It also rewards some deeper technical knowledge. One striking image I found on Midjourney’s Discord server was generated with the following, painfully detailed prompt:
in the style of Metaphysical painting, colored pencil drawing Smooth Shading & Blending, a sunrise reflects in a pond in the deep woods, a willow trees boughs hang over the edge of the pond, moody, intense emotions, deep perspective, natural lighting, Hyperdetailed, super High Contrast, intricate details, photography, raytracing, octane render, unreal engine --ar 3:2 --s 999 --chaos 50 --v 4 --v 4 -
More than just specificity, good prompts tend to reveal an awareness of the medium’s abilities that the user is trying to replicate. Octane render and unreal engine are digital animation tools that produce 3-D computer graphics. Inputs like -- v4 are instructions to Midjourney to use the model’s newer, more powerful, and experimental version. Some of the best photorealistic-image prompts ask the model to imitate a specific camera or lens type; others demonstrate a working knowledge of art history or a particular artistic style. It’s reminiscent of the early days of search, when experts who could navigate Boolean operators, authors, keywords, sources, and date-range searches could unlock better results.
Read: ChatGPT is about to dump more work on everyone
Search engines, of course, are no longer as demanding. In order to drive and cater to mass adoption, Google’s tools became more powerful, making it easier to get a high-quality result with a clumsy or simple query. Mollick suggested that prompt engineering might simply be a placeholder—a rudimentary way for us to interact with AI until they can synthesize what we want from bare-bones prompts or other, unknown means.
He could be right. Those who have seen early tests of OpenAI’s GPT-4 text models (which are not public) speak of it like something out of science fiction and suggest that the next leap will render the old tool obsolete. And yesterday, Microsoft unveiled an interface that, the company said, users will eventually talk to like a personal assistant. Instead of searching How big is a Honda Odyssey? and IKEA Klippan loveseat dimensions, you might ask the chatbot to solve an entire problem for you: Will the IKEA Klippan loveseat fit into my 2019 Honda Odyssey if I fold down the seats? Prompt engineering might just be a bridge to get us to the brave new world of whatever generative-media landscape comes next, but for now, it’s difficult to know how much to believe the hype.
Read: ChatGPT is dumber than you think
Until the paradigm shifts, I remain drawn to AI prompts, which are usually far more intriguing than the outputs they yield. When people share AI-generated art or text, they frequently do so alongside the string of commands that brought it to life. Traveling back and forth between the instructions and the end result is revealing, even intimate. It’s a bit like being granted access into a person’s brain to see how they piece together disparate bits of knowledge, how they reason through a problem, or how they employ their creativity to produce something unexpected.
Like writing and coding before it, prompt engineering is an emergent form of thinking. It lies somewhere between conversation and query, between programming and prose. It is the one part of this fast-changing, uncertain future that feels distinctly human.

ChatGPT Is Dumber Than You Think
Treat it like a toy, not a tool.
By Ian Bogost
Illustration of a keyboard with its wire positioned to look like a human head.
Tyler Comrie / The Atlantic; Getty
DECEMBER 7, 2022
SHARE
As a critic of technology, I must say that the enthusiasm for ChatGPT, a large-language model trained by OpenAI, is misplaced. Although it may be impressive from a technical standpoint, the idea of relying on a machine to have conversations and generate responses raises serious concerns.
First and foremost, ChatGPT lacks the ability to truly understand the complexity of human language and conversation. It is simply trained to generate words based on a given input, but it does not have the ability to truly comprehend the meaning behind those words. This means that any responses it generates are likely to be shallow and lacking in depth and insight.
Furthermore, the reliance on ChatGPT for conversation raises ethical concerns. If people begin to rely on a machine to have conversations for them, it could lead to a loss of genuine human connection. The ability to connect with others through conversation is a fundamental aspect of being human, and outsourcing that to a machine could have detrimental side effects on our society.
Hold up, though. I, Ian Bogost, did not actually write the previous three paragraphs. A friend sent them to me as screenshots from his session with ChatGPT, a program released last week by OpenAI that one interacts with by typing into a chat window. It is, indeed, a large language model (or LLM), a type of deep-learning software that can generate new text once trained on massive amounts of existing written material. My friend’s prompt was this: “Create a critique of enthusiasm for ChatGPT in the style of Ian Bogost.”

Why They Can't Write: Killing The Five-Paragraph Essay And Other NecessitiesJOHN WARNER,JOHNS HOPKINS UNIVERSITY PRESS
BUY BOOK
When you buy a book using a link on this page, we receive a commission. Thank you for supporting The Atlantic.
ChatGPT wrote more, but I spared you the rest because it was so boring. The AI wrote another paragraph about accountability (“If ChatGPT says or does something inappropriate, who is to blame?”), and then a concluding paragraph that restated the rest (it even began, “In conclusion, …”). In short, it wrote a basic, high-school-style five-paragraph essay.
That fact might comfort or frighten you, depending on your predilections. When OpenAI released ChatGPT to the public last week, the first and most common reaction I saw was fear that it would upend education. “You can no longer give take-home exams,” Kevin Bryan, a University of Toronto professor, posted on Twitter. “I think chat.openai.com may actually spell the end of writing assignments,” wrote Samuel Bagg, a University of South Carolina political scientist. That’s the fear.
But you may find comfort in knowing that the bot’s output, while fluent and persuasive as text, is consistently uninteresting as prose. It’s formulaic in structure, style, and content. John Warner, the author of the book Why They Can’t Write, has been railing against the five-paragraph essay for years and wrote a Twitter thread about how ChatGPT reflects this rules-based, standardized form of writing: “Students were essentially trained to produce imitations of writing,” he tweeted. The AI can generate credible writing, but only because writing, and our expectations for it, has become so unaspiring.
Even pretending to fool the reader by passing off an AI copy as one’s own, like I did above, has become a tired trope, an expected turn in a too-long Twitter thread about the future of generative AI rather than a startling revelation about its capacities. On the one hand, yes, ChatGPT is capable of producing prose that looks convincing. But on the other hand, what it means to be convincing depends on context. The kind of prose you might find engaging and even startling in the context of a generative encounter with an AI suddenly seems just terrible in the context of a professional essay published in a magazine such as The Atlantic. And, as Warner’s comments clarify, the writing you might find persuasive as a teacher (or marketing manager or lawyer or journalist or whatever else) might have been so by virtue of position rather than meaning: The essay was extant and competent; the report was in your inbox on time; the newspaper article communicated apparent facts that you were able to accept or reject.
RECOMMENDED READING

My Wife Was Dying, and We Didn’t Tell Our Children
JON MEHLMAN
A great white shark
The Predator That Makes Great White Sharks Flee in Fear
ED YONG
A man holds out his hand to a skeptical woman
Why Don’t More Men Take Their Wives’ Last Names?
CAROLINE KITCHENER
Perhaps ChatGPT and the technologies that underlie it are less about persuasive writing and more about superb bullshitting. A bullshitter plays with the truth for bad reasons—to get away with something. Initial response to ChatGPT assumes as much: that it is a tool to help people contrive student essays, or news writing, or whatever else. It’s an easy conclusion for those who assume that AI is meant to replace human creativity rather than amend it.
The internet, and the whole technology sector on which it floats, feels like a giant organ for bullshittery—for upscaling human access to speech and for amplifying lies. Online, people cheat and dupe and skirmish with one another. Deep-learning AI worsens all this by hiding the operation of software such as LLMs such that nobody, not even their creators, can explain what they do and why. OpenAI presents its work as context-free and experimental, with no specific use cases—it says it published ChatGPT just to “get users’ feedback and learn about its strengths and weaknesses.” It’s no wonder the first and most obvious assumption to make about ChatGPT is that it is a threat—to something, to everything.
But ChatGPT isn’t a step along the path to an artificial general intelligence that understands all human knowledge and texts; it’s merely an instrument for playing with all that knowledge and all those texts. Play just involves working with raw materials in order to see what they can do. You play a game, or an instrument, to avail yourself of familiar materials in an unexpected way. LLMs are surely not going to replace college or magazines or middle managers. But they do offer those and other domains a new instrument—that’s really the right word for it—with which to play with an unfathomable quantity of textual material.
I talked with ChatGPT about a wide range of topics, including poetry, behavioral psychology, and computer programming, in order to test this theory. Right now, ChatGPT is just a tech demo, a research experiment. Less clear is how it might be used, beyond the dire predictions about what sectors its technology might upend. In almost every case, the AI appeared to possess both knowledge and the means to express it. But when pressed—and the chat interface makes it easy to do so—the bot almost always had to admit that it was just making things up.
Sometimes it simply used templates. When I asked ChatGPT to generate a cover letter for a university job, it obliged in a competent but uninspired way. It also produced exactly the same letter for a job as a magazine editor as it did for a job as a cannabis innovator in the Web3 space (“I have a deep understanding of the web3 space and the unique challenges and opportunities it presents”). Fair enough: Most cover letters are terrible, after all, repeating the same trite lines and abstract gestures.
In other cases, the bot produced creative output that was surprisingly synthetic but riddled with flaws. When I asked it to generate an imagist poem in the vein of Ezra Pound or William Carlos Williams about hamburgers, it obliged with this short refrain:
Juicy patty, sizzling hot
Toasted bun, crisp lettuce and tomato
A symphony of flavors, a feast for the senses
Savory, satisfying, a true
American classic.
Not bad, I guess, especially for a computer. But also, the poem is not really an example of imagism, which strives to evoke a precise image by means of deliberate language (something like, “The dense honesty of / A hamburger.”) So I called ChatGPT on it. “This is not an imagist poem.” Its response is worth reproducing in full:
You are correct, the poem that I generated is still not an imagist poem. An imagist poem uses precise, vivid imagery to convey a specific idea or emotion, and focuses on a single image or moment. The poem that I generated uses a more narrative and descriptive style, and does not focus on a single, specific image. It describes the ingredients and flavors of a hamburger, but does not use precise and vivid imagery to convey a specific idea or emotion. I apologize if my previous responses did not meet your expectations, and I will do my best to assist you with any further questions or prompts that you may have.
I also urged the AI to generate a lai (a medieval narrative poem) in the style of Marie de France about the beloved, Texas-based fast-food chain Whataburger. It sure as heck looked a lot like a lai, beginning like this:
In the land of Whataburger, where burgers and fries are king,
A stranger came to visit, and with him brought a ring.
He wore a cloak of velvet, and a hat upon his head,
And he smiled with a twinkle in his eye as he approached the counter, where the burgers are served hot and fresh, and the fries are crispy and golden.
Except for that last line, which is a metrical mess. The AI wasn’t able (or willing) to evaluate its work (“I am a large language model trained by OpenAI and I don’t have the ability to evaluate the quality of literary works or make aesthetic judgments.”), but it was able to tell me about the typical structure of a lai—a short, narrative poem of the 12th century with a particular structure and form, usually written in octosyllabic couplets. The lyrical errors in the generated lai arise from its failure to adhere to that metrical form.
When I asked about the matter, it admitted again that, no, the lai it had written was not structured in octosyllabic couplets, claiming that it had produced “a more modern and playful take on the form of the lai.” I was starting to feel like I was negotiating with a student who had come to office hours to complain about their grade.
This happened over and over again. I asked for source code for an Atari game about scooping cat litter, and the AI sent me valid programming instructions—it understood the assignment—but only disconnected snippets of actual code with the heading comment “This program creates a simple game where the player must use a scoop to pick up their cat’s litters and put them in a trash bin.” It was an icon of the answer I sought rather than the answer itself.
Then I asked for a set of diagnostic criteria for an invented psychological condition I named “Inherited Truculence.” The AI instantly obliged:
Diagnostic Criteria for Inherited Truculence:
1.A tendency to experience and express aggressive or hostile thoughts and feelings, particularly in response to perceived threats or challenges.
2.A history of aggressive or hostile behavior, particularly in interpersonal or social situations.
3.A familial history of aggressive or hostile behavior, indicating a possible genetic or hereditary component to the condition.
4.Impairment in social, occupational, or other important areas of functioning as a result of aggressive or hostile behavior.
5.The symptoms are not better explained by another mental disorder or by the effects of a substance (such as drugs or medication).
Except, truculence doesn’t really amount to “aggressive and hostile thoughts and feelings,” does it? I asked, and the AI agreed, eventually revising its diagnostics accordingly at my further prompting (“A tendency to experience and express defiant or confrontational thoughts and feelings,” and so forth).
Likewise, my prompt to offer both an ontological and an epistemological answer to the question When does bread become toast? yielded a generally fluent, seemingly philosophical reply. It read in part: “when we are able to recognize it as toast based on its appearance, texture, and other sensory characteristics.”
But the philosophy of toast is an area of expertise for me, so I was able to discern that this snippet, supposedly an epistemological argument, was really about perception. “Isn’t the second a phenomenal, and not an epistemological argument?” I challenged. And once more, the AI cried uncle without hesitation: “You are correct, the second argument that I provided is a phenomenal, not an epistemological, argument. An epistemological argument focuses on how we come to know or understand something, whereas a phenomenal argument focuses on our experience or perception of something.”
At this point, talking to ChatGPT began to feel like every other interaction one has on the internet, where some guy (always a guy) tries to convert the skim of a Wikipedia article into a case of definitive expertise. Except ChatGPT was always willing to admit that it was wrong. Instantly and without dispute. And in each case, the bot also knew, with reasonable accuracy, why it was wrong. That sounds good but is actually pretty terrible: If one already needs to possess the expertise to identify the problems with LLM-generated text, but the purpose of LLM-generated text is to obviate the need for such knowledge, then we’re in a sour pickle indeed. Maybe it’s time for that paragraph on accountability after all.
But that’s not ChatGPT’s aim. It doesn’t make accurate arguments or express creativity, but instead produces textual material in a form corresponding with the requester’s explicit or implicit intent, which might also contain truth under certain circumstances. That is, alas, an accurate account of textual matter of all kinds: online, in books, on Wikipedia, and well beyond.
Read: Google’s “sentient” chatbot is our self-deceiving future
Proponents of LLM generativity may brush off this concern. Some will do so by glorifying GPT’s obvious and fully realized genius, in embarrassing ways that I can only bear to link to rather than repeat. Others, more measured but no less bewitched, may claim that “it’s still early days” for a technology a mere few years old but that can already generate reasonably good 12th-century lyric poems about Whataburger. But these are the sentiments of the IT-guy personalities who have most mucked up computational and online life, which is just to say life itself. OpenAI assumes that its work is fated to evolve into an artificial general intelligence—a machine that can do anything. Instead, we should adopt a less ambitious but more likely goal for ChatGPT and its successors: They offer an interface into the textual infinity of digitized life, an otherwise impenetrable space that few humans can use effectively in the present.
To explain what I mean by that, let me show you a quite different exchange I had with ChatGPT, one in which I used it to help me find my way through the textual murk rather than to fool me with its prowess as a wordsmith.
“I’m looking for a specific kind of window covering, but I don’t know what it’s called.” I told the bot. “It’s a kind of blind, I think. What kinds are there?” ChatGPT responded with a litany of window dressings, which was fine. I clarified that I had something in mind that was sort of like a roller blind but made of fabric. “Based on the description you have provided, it sounds like you may be thinking of a roman shade,” it replied, offering more detail and a mini sales pitch for this fenestral technology.
My dearest reader, I do in fact know what a Roman shade is. But lacking that knowledge and nevertheless needing to deploy it in order to make sense of the world—this is exactly the kind of act that is very hard to do with computers today. To accomplish something in the world often boils down to mustering a set of stock materials into the expected linguistic form. That’s true for Google or Amazon, where searches for window coverings or anything else now fail most of the time, requiring time-consuming, tightrope-like finagling to get the machinery to point you in even the general direction of an answer. But it’s also true for student essays, thank-you notes, cover letters, marketing reports, and perhaps even medieval lais (insofar as anyone would aim to create one). We are all faking it with words already. We are drowning in an ocean of content, desperate for form’s life raft.
ChatGPT offers that shape, but—and here’s where the bot did get my position accidentally correct, in part—it doesn’t do so by means of knowledge. The AI doesn’t understand or even compose text. It offers a way to probe text, to play with text, to mold and shape an infinity of prose across a huge variety of domains, including literature and science and shitposting, into structures in which further questions can be asked and, on occasion, answered.
GPT and other large language models are aesthetic instruments rather than epistemological ones. Imagine a weird, unholy synthesizer whose buttons sample textual information, style, and semantics. Such a thing is compelling not because it offers answers in the form of text, but because it makes it possible to play text—all the text, almost—like an instrument.
That outcome could be revelatory! But a huge obstacle stands in the way of achieving it: people, who don’t know what the hell to make of LLMs, ChatGPT, and all the other generative AI systems that have appeared. Their creators haven’t helped, perhaps partly because they don’t know what these things are for either. OpenAI offers no framing for ChatGPT, presenting it as an experiment to help “make AI systems more natural to interact with,” a worthwhile but deeply unambitious goal. Absent further structure, it’s no surprise that ChatGPT’s users frame their own creations as either existential threats or perfected accomplishments. Neither outcome is true, but both are also boring. Imagine worrying about the fate of take-home essay exams, a stupid format that everyone hates but nobody has the courage to kill. But likewise, imagine nitpicking with a computer that just composed something reminiscent of a medieval poem about a burger joint because its lines don’t all have the right meter! Sure, you can take advantage of that opportunity to cheat on school exams or fake your way through your job. That’s what a boring person would do. That’s what a computer would expect.
Computers have never been instruments of reason that can solve matters of human concern; they’re just apparatuses that structure human experience through a very particular, extremely powerful method of symbol manipulation. That makes them aesthetic objects as much as functional ones. GPT and its cousins offer an opportunity to take them up on the offer—to use computers not to carry out tasks but to mess around with the world they have created. Or better: to destroy it.

ChatGPT Is About to Dump More Work on Everyone
Artificial intelligence could spare you some effort. Even if it does, it will create a lot more work in the process.
By Ian Bogost
An illustration of a hand holding marionette strings. The hand is filled with binary code.
Tyler Comrie / The Atlantic
FEBRUARY 2, 2023
SHARE
Have you been worried that ChatGPT, the AI language generator, could be used maliciously—to cheat on schoolwork or broadcast disinformation? You’re in luck, sort of: OpenAI, the company that made ChatGPT, has introduced a new tool that tries to determine the likelihood that a chunk of text you provide was AI-generated.
I say “sort of” because the new software faces the same limitations as ChatGPT itself: It might spread disinformation about the potential for disinformation. As OpenAI explains, the tool will likely yield a lot of false positives and negatives, sometimes with great confidence. In one example, given the first lines of the Book of Genesis, the software concluded that it was likely to be AI-generated. God, the first AI.
On the one hand, OpenAI appears to be adopting a classic mode of technological solutionism: creating a problem, and then selling the solution to the problem it created. But on the other hand, it might not even matter if either ChatGPT or its antidote actually “works,” whatever that means (in addition to its limited accuracy, the program is effective only on English text and needs at least 1,000 characters to work with). The machine-learning technology and others like it are creating a new burden for everyone. Now, in addition to everything else we have to do, we also have to make time for the labor of distinguishing between human and AI, and the bureaucracy that will be built around it.
If you are a student, parent, educator, or individual with internet access, you may have caught wind of the absolute panic that has erupted around ChatGPT. There are fears—It’s the end of education as we know it! It passed a Wharton MBA exam!—and retorts to those fears: We must defend against rampant cheating. If your class can be gamed by an AI, then it was badly designed in the first place!
An assumption underlies all these harangues, that education needs to “respond” to ChatGPT, to make room for and address it. At the start of this semester at Washington University in St. Louis, where I teach, our provost sent all faculty an email encouraging us to be aware of the technology and consider how to react to it. Like many institutions, ours also hosted a roundtable to discuss ChatGPT. In a matter of months, generative AI has sent secondary and postsecondary institutions scrambling to find a response—any response—to its threats or opportunities.
Read: ChatGPT is dumber thank you think
That work heaps atop an already overflowing pile of duties. Budgets cut, schoolteachers often crowdsource funds and materials for their classrooms. The coronavirus pandemic changed assumptions about attendance and engagement, making everyone renegotiate, sometimes weekly, where and when class will take place. Managing student anxiety and troubleshooting broken classroom technology is now a part of most teachers’ everyday work. That’s not to mention all the emails, and the training modules, and the self-service accounting tasks. And now comes ChatGPT, and ChatGPT’s flawed remedy.
RECOMMENDED READING

What It’s Like to Visit an Existential Therapist
FAITH HILL
A photo illustration of office furnishings with residential design elements
Resimercial: The Terrible Word for Today’s Trendy Office Aesthetic
JOE PINSKER
A baby's feet stick out of a stroller in a black-and-white photo.
Parental Leave Is American Exceptionalism at Its Bleakest
JOE PINSKER
The situation extends well beyond education. Almost a decade ago, I diagnosed a condition I named hyperemployment. Thanks to computer technology, most professionals now work a lot more than they once did. In part, that’s because email and groupware and laptops and smartphones have made taking work home much easier—you can work around the clock if nobody stops you. But also, technology has allowed, and even required, workers to take on tasks that might otherwise have been carried out by specialists as their full-time job. Software from SAP, Oracle, and Workday force workers to do their own procurement and accounting. Data dashboards and services make office workers part-time business analysts. On social media, many people are now de facto marketers and PR agents for their division and themselves.
No matter what ChatGPT and other AI tools ultimately do, they will impose new regimes of labor and management atop the labor required to carry out the supposedly labor-saving effort. ChatGPT’s AI detector introduces yet another thing to do and to deal with.
Is a student trying to cheat with AI? Better run the work through the AI-cheater check. Even educators who don’t want to use such a thing will be ensnared in its use: subject to debates about the ethics of sharing student work with OpenAI to train the model; forced to adopt procedures to address the matter as institutional practice, and to reconfigure lesson plans to address the “new normal”; obligated to read emails about those procedures to consider implementing them.
At other jobs, different but similar situations will arise. Maybe you outsourced some work to a contractor. Now you need to make sure it wasn’t AI-generated, in order to prevent fiscal waste, legal exposure, or online embarrassment. As cases like this appear, prepare for an all-hands meeting, and a series of email follow-ups, and maybe eventually a compulsory webinar and an assessment of your compliance with the new learning-management system, and on and on.
New technologies meant to free people from the burden of work have added new types of work to do instead. Home appliances such as the washing machine freed women to work outside the home, which in turn reduced time to do housework (which still fell largely to women) even as the standards for home perfection rose. Photocopiers and printers reduce the burden of the typist but create the need to self-prepare, collate, and distribute the reports in addition to writing them. The automated grocery checkout assigns the job of cashier to the shopper. Email makes it possible to communicate rapidly and directly with collaborators, but then your whole day is spent processing emails, which renews the burden again the next day. Zoom makes it possible to meet anywhere, but in doing so begets even more meetings.
ChatGPT has held the world’s attention, a harbinger of—well, something, but maybe something big, and weird, and new. That response has inspired delight, anxiety, fear, and dread, but no matter the emotion, it has focused on the potential uses of the technology, whether for good or ill.
The ChatGPT detector offers the first whiff of another, equally important consequence of the AI future: its inevitable bureaucratization. Microsoft, which has invested billions of dollars in OpenAI, has declared its hope to integrate the technology into Office. That could help automate work, but it’s just as likely to create new demands for Office-suite integration, just as previous add-ons such as SharePoint and Teams did. Soon, maybe, human resources will require the completion of AI-differentiation reports before approving job postings. Procurement may adopt a new Workday plug-in to ensure vendor-work-product approvals are following AI best practices, a requirement you will now have to perform in addition to filling out your expense reports—not to mention your actual job. Your Salesforce dashboard may offer your organization the option to add a required AI-probability assessment before a lead is qualified. Your kids’ school may send a “helpful” guide to policing your children’s work at home for authenticity, because “if AI deception is a problem, all of us have to be part of the solution.”

What Happens When AI Has Read Everything?
The dream of an artificial mind may never become a reality if AI runs out of quality prose to ingest—and there isn’t much left.
By Ross Andersen
An eye on a series of Borgesian libraries
Tyler Comrie / The Atlantic; Getty
JANUARY 18, 2023
SHARE
Artificial intelligence has in recent years proved itself to be a quick study, although it is being educated in a manner that would shame the most brutal headmaster. Locked into airtight Borgesian libraries for months with no bathroom breaks or sleep, AIs are told not to emerge until they’ve finished a self-paced speed course in human culture. On the syllabus: a decent fraction of all the surviving text that we have ever produced.
When AIs surface from these epic study sessions, they possess astonishing new abilities. People with the most linguistically supple minds—hyperpolyglots—can reliably flip back and forth between a dozen languages; AIs can now translate between more than 100 in real time. They can churn out pastiche in a range of literary styles and write passable rhyming poetry. DeepMind’s Ithaca AI can glance at Greek letters etched into marble and guess the text that was chiseled off by vandals thousands of years ago.
These successes suggest a promising way forward for AI’s development: Just shovel ever-larger amounts of human-created text into its maw, and wait for wondrous new skills to manifest. With enough data, this approach could perhaps even yield a more fluid intelligence, or a humanlike artificial mind akin to those that haunt nearly all of our mythologies of the future.
The trouble is that, like other high-end human cultural products, good prose ranks among the most difficult things to produce in the known universe. It is not in infinite supply, and for AI, not any old text will do: Large language models trained on books are much better writers than those trained on huge batches of social-media posts. (It’s best not to think about one’s Twitter habit in this context.) When we calculate how many well-constructed sentences remain for AI to ingest, the numbers aren’t encouraging. A team of researchers led by Pablo Villalobos at Epoch AI recently predicted that programs such as the eerily impressive ChatGPT will run out of high-quality reading material by 2027. Without new text to train on, AI’s recent hot streak could come to a premature end.
It should be noted that only a slim fraction of humanity’s total linguistic creativity is available for reading. More than 100,000 years have passed since radically creative Africans transcended the emotive grunts of our animal ancestors and began externalizing their thoughts into extensive systems of sounds. Every notion expressed in those protolanguages—and many languages that followed—is likely lost for all time, although it gives me pleasure to imagine that a few of their words are still with us. After all, some English words have a shockingly ancient vintage: Flow, mother, fire, and ash come down to us from Ice Age peoples.
Writing has allowed human beings to capture and store a great many more of our words. But like most new technologies, writing was expensive at first, which is why it was initially used primarily for accounting. It took time to bake and dampen clay for your stylus, to cut papyrus into strips fit to be latticed, to house and feed the monks who inked calligraphy onto vellum. These resource-intensive techniques could preserve only a small sampling of humanity’s cultural output.
Not until the printing press began machine-gunning books into the world did our collective textual memory achieve industrial scale. Researchers at Google Books estimate that since Gutenberg, humans have published more than 125 million titles, collecting laws, poems, myths, essays, histories, treatises, and novels. The Epoch team estimates that 10 million to 30 million of these books have already been digitized, giving AIs a reading feast of hundreds of billions of, if not more than a trillion, words.
Read: The end of high-school English
Those numbers may sound impressive, but they’re within range of the 500 billion words that trained the model that powers ChatGPT. Its successor, GPT-4, might be trained on tens of trillions of words. Rumors suggest that when GPT-4 is released later this year, it will be able to generate a 60,000-word novel from a single prompt.
Ten trillion words is enough to encompass all of humanity’s digitized books, all of our digitized scientific papers, and much of the blogosphere. That’s not to say that GPT-4 will have read all of that material, only that doing so is well within its technical reach. You could imagine its AI successors absorbing our entire deep-time textual record across their first few months, and then topping up with a two-hour reading vacation each January, during which they could mainline every book and scientific paper published the previous year.
Just because AIs will soon be able to read all of our books doesn’t mean they can catch up on all of the text we produce. The internet’s storage capacity is of an entirely different order, and it’s a much more democratic cultural-preservation technology than book publishing. Every year, billions of people write sentences that are stockpiled in its databases, many owned by social-media platforms.
Random text scraped from the internet generally doesn’t make for good training data, with Wikipedia articles being a notable exception. But perhaps future algorithms will allow AIs to wring sense from our aggregated tweets, Instagram captions, and Facebook statuses. Even so, these low-quality sources won’t be inexhaustible. According to Villalobos, within a few decades, speed-reading AIs will be powerful enough to ingest hundreds of trillions of words—including all those that human beings have so far stuffed into the web.
Not every AI is an English major. Some are visual learners, and they too may one day face a training-data shortage. While the speed-readers were bingeing the literary canon, these AIs were strapped down with their eyelids held open, Clockwork Orange–style, for a forced screening comprising millions of images. They emerged from their training with superhuman vision. They can recognize your face behind a mask, or spot tumors that are invisible to the radiologist’s eye. On night drives, they can see into the gloomy roadside ahead where a young fawn is working up the nerve to chance a crossing.
Most impressive, AIs trained on labeled pictures have begun to develop a visual imagination. OpenAI’s DALL-E 2 was trained on 650 million images, each paired with a text label. DALL-E 2 has seen the ocher handprints that Paleolithic humans pressed onto cave ceilings. It can emulate the different brushstroke styles of Renaissance masters. It can conjure up photorealistic macros of strange animal hybrids. An animator with world-building chops can use it to generate a Pixar-style character, and then surround it with a rich and distinctive environment.
Read: Generative art is stupid
Thanks to our tendency to post smartphone pics on social media, human beings produce a lot of labeled images, even if the label is just a short caption or geotag. As many as 1 trillion such images are uploaded to the internet every year, and that doesn’t include YouTube videos, each of which is a series of stills. It’s going to take a long time for AIs to sit through our species’ collective vacation-picture slideshow, to say nothing of our entire visual output. According to Villalobos, our training-image shortage won’t be acute until sometime between 2030 and 2060.
If indeed AIs are starving for new inputs by midcentury—or sooner, in the case of text—the field’s data-powered progress may slow considerably, putting artificial minds and all the rest out of reach. I called Villalobos to ask him how we might increase human cultural production for AI. “There may be some new sources coming online,” he told me. “The widespread adoption of self-driving cars would result in an unprecedented amount of road video recordings.”
Villalobos also mentioned “synthetic” training data created by AIs. In this scenario, large language models would be like the proverbial monkeys with typewriters, only smarter and possessed of functionally infinite energy. They could pump out billions of new novels, each of Tolstoyan length. Image generators could likewise create new training data by tweaking existing snapshots, but not so much that they fall afoul of their labels. It’s not yet clear whether AIs will learn anything new by cannibalizing data that they themselves create. Perhaps doing so will only dilute the predictive potency they gleaned from human-made text and images. “People haven’t used a lot of this stuff, because we haven’t yet run out of data,” Jaime Sevilla, one of Villalobos’s colleagues, told me.
Villalobos’s paper discusses a more unsettling set of speculative work-arounds. We could, for instance, all wear dongles around our necks that record our every speech act. According to one estimate, people speak 5,000 to 20,000 words a day on average. Across 8 billion people, those pile up quickly. Our text messages could also be recorded and stripped of identifying metadata. We could subject every white-collar worker to anonymized keystroke recording, and firehose what we capture into giant databases to be fed into our AIs. Villalobos noted drily that fixes such as these are currently “well outside the Overton window.”
Perhaps in the end, big data will have diminishing returns. Just because our most recent AI winter was thawed out by giant gobs of text and imagery doesn’t mean our next one will be. Maybe instead, it will be an algorithmic breakthrough or two that at last populate our world with artificial minds. After all, we know that nature has authored its own modes of pattern recognition, and that so far, they outperform even our best AIs. My 13-year-old son has ingested orders of magnitude fewer words than ChatGPT, yet he has a much more subtle understanding of written text. If it makes sense to say that his mind runs on algorithms, they’re better algorithms than those used by today’s AIs.
Read: Five remarkable chats that will help you understand ChatGPT
If, however, our data-gorging AIs do someday surpass human cognition, we will have to console ourselves with the fact that they are made in our image. AIs are not aliens. They are not the exotic other. They are of us, and they are from here. They have gazed upon the Earth’s landscapes. They have seen the sun setting on its oceans billions of times. They know our oldest stories. They use our names for the stars. Among the first words they learn are flow, mother, fire, and ash.


The Difference Between Speaking and Thinking
The human brain could explain why AI programs are so good at writing grammatically superb nonsense.
By Matteo Wong
An artificially colored MRI scan of the human brain
Alamy
JANUARY 31, 2023, 4:16 PM ET
SHARE
Language is commonly understood to be the “stuff” of thought. People “talk it out” and “speak their mind,” follow “trains of thought” or “streams of consciousness.” Some of the pinnacles of human creation—music, geometry, computer programming—are framed as metaphorical languages. The underlying assumption is that the brain processes the world and our experience of it through a progression of words. And this supposed link between language and thinking is a large part of what makes ChatGPT and similar programs so uncanny: The ability of AI to answer any prompt with human-sounding language can suggest that the machine has some sort of intent, even sentience.
But then the program says something completely absurd—that there are 12 letters in nineteen or that sailfish are mammals—and the veil drops. Although ChatGPT can generate fluent and sometimes elegant prose, easily passing the Turing-test benchmark that has haunted the field of AI for more than 70 years, it can also seem incredibly dumb, even dangerous. It gets math wrong, fails to give the most basic cooking instructions, and displays shocking biases. In a new paper, cognitive scientists and linguists address this dissonance by separating communication via language from the act of thinking: Capacity for one does not imply the other. At a moment when pundits are fixated on the potential for generative AI to disrupt every aspect of how we live and work, their argument should force a reevaluation of the limits and complexities of artificial and human intelligence alike.
The researchers explain that words may not work very well as a synecdoche for thought. People, after all, identify themselves on a continuum of visual to verbal thinking; the experience of not being able to put an idea into words is perhaps as human as language itself. Contemporary research on the human brain, too, suggests that “there is a separation between language and thought,” says Anna Ivanova, a cognitive neuroscientist at MIT and one of the study’s two lead authors. Brain scans of people using dozens of languages have revealed a particular network of neurons that fires independent of the language being used (including invented tongues such as Na’vi and Dothraki).
That network of neurons is not generally involved in thinking activities including math, music, and coding. In addition, many patients with aphasia—a loss of the ability to comprehend or produce language, as a result of brain damage—remain skilled at arithmetic and other nonlinguistic mental tasks. Combined, these two bodies of evidence suggest that language alone is not the medium of thought; it is more like a messenger. The use of grammar and a lexicon to communicate functions that involve other parts of the brain, such as socializing and logic, is what makes human language special.
Read: Hollywood’s love affair with fictional languages
ChatGPT and software like it demonstrate an incredible ability to string words together, but they struggle with other tasks. Ask for a letter explaining to a child that Santa Claus is fake, and it produces a moving message signed by Saint Nick himself. These large language models, also called LLMs, work by predicting the next word in a sentence based on everything before it (popular belief follows contrary to, for example). But ask ChatGPT to do basic arithmetic and spelling or give advice for frying an egg, and you may receive grammatically superb nonsense: “If you use too much force when flipping the egg, the eggshell can crack and break.”
These shortcomings point to a distinction, not dissimilar to one that exists in the human brain, between piecing together words and piecing together ideas—what the authors term formal and functional linguistic competence, respectively. “Language models are really good at producing fluent, grammatical language,” says the University of Texas at Austin linguist Kyle Mahowald, the paper’s other lead author. “But that doesn’t necessarily mean something which can produce grammatical language is able to do math or logical reasoning, or think, or navigate social contexts.”
If the human brain’s language network is not responsible for math, music, or programming—that is, for thinking—then there’s no reason an artificial “neural network” trained on terabytes of text would be good at those things either. “In line with evidence from cognitive neuroscience,” the authors write, “LLMs’ behavior highlights the difference between being good at language and being good at thought.” ChatGPT’s ability to get mediocre scores on some business- and law-school exams, then, is more a mirage than a sign of understanding.
Still, hype swirls around the next iteration of language models, which will train on far more words and with far more computing power. OpenAI, the creator of ChatGPT, claims that its programs are approaching a so-called general intelligence that would put the machines on par with humankind. But if the comparison to the human brain holds, then simply making models better at word prediction won’t bring them much closer to this goal. In other words, you can dismiss the notion that AI programs such as ChatGPT have a soul or resemble an alien invasion.
Ivanova and Mahowald believe that different training methods are required to spur further advances in AI—for instance, approaches specific to logical or social reasoning rather than word prediction. ChatGPT may have already taken a step in that direction, not just reading massive amounts of text but also incorporating human feedback: Supervisors were able to comment on what constituted good or bad responses. But with few details about ChatGPT’s training available, it’s unclear just what that human input targeted; the program apparently thinks 1,000 is both greater and less than 1,062. (OpenAI released an update to ChatGPT yesterday that supposedly improves its “mathematical capabilities,” but it’s still reportedly struggling with basic word problems.)
Read: What happens when AI has read everything?
There are, it should be noted, people who believe that large language models are not as good at language as Ivanova and Mahowald write—that they are basically glorified auto-completes whose flaws scale with their power. “Language is more than just syntax,” says Gary Marcus, a cognitive scientist and prominent AI researcher. “In particular, it’s also about semantics.” It’s not just that AI chatbots don’t understand math or how to fry eggs—they also, he says, struggle to comprehend how a sentence derives meaning from the structure of its parts.
For instance, imagine three plastic balls in a row: green, blue, blue. Someone asks you to grab “the second blue ball”: You understand that they’re referring to the last ball in the sequence, but a chatbot might understand the instruction as referring to the second ball, which also happens to be blue. “That a large language model is good at language is overstated,” Marcus says. But to Ivanova, something like the blue-ball example requires not just compiling words but also conjuring a scene, and as such “is not really about language proper; it’s about language use.”
And no matter how compelling their language use is, there’s still a healthy debate over just how much programs such as ChatGPT actually “understand” about the world by simply being fed data from books and Wikipedia entries. “Meaning is not given,” says Roxana Girju, a computational linguist at the University of Illinois at Urbana-Champaign. “Meaning is negotiated in our interactions, discussions, not only with other people but also with the world. It’s something that we reach at in the process of engaging through language.” If that’s right, building a truly intelligent machine would require a different way of combining language and thought—not just layering different algorithms but designing a program that might, for instance, learn language and how to navigate social relationships at the same time.
Ivanova and Mahowald are not outright rejecting the view that language epitomizes human intelligence; they’re complicating it. Humans are “good” at language precisely because we combine thought with its expression. A computer that both masters the rules of language and can put them to use will necessarily be intelligent—the flip side being that narrowly mimicking human utterances is precisely what is holding machines back. But before we can use our organic brains to better understand silicon ones, we will need both new ideas and new words to understand the significance of language itself.


Generative Art Is Stupid
And that’s how it should be.
By Ian Bogost
An illustration of an emoji with many different features surrounding it—a heart, imp horns, and a grimacing mouth
Tyler Comrie / The Atlantic
JANUARY 13, 2023
SHARE
A boyfriend just going through the motions. A spouse worn into the rut of habit. A jetlagged traveler’s message of exhaustion-fraught longing. A suppressed kiss, unwelcome or badly timed. These were some of the interpretations that reverberated in my brain after I viewed a weird digital-art trifle by the Emoji Mashup Bot, a popular but defunct Twitter account that combined the parts of two emoji into new, surprising, and astonishingly resonant compositions. The bot had taken the hand and eyes from the 🥱 yawning emoji and mashed them together with the mouth from the 😘 kissing-heart emoji. That’s it.
Compare that simple method with supposedly more sophisticated machine-learning-based generative tools that have become popular in the past year or so. When I asked Midjourney, an AI-based art generator, to create a new emoji based on those same two, it produced compositions that were certainly emojiform but possessed none of the style or significance of the simple mashup: a series of yellow, heart-shaped bodies with tongues sticking out. One appeared to be eating another tongue. All struck me as the kinds of monstrosities that might be offered as prizes for carnival games, or as stickers delivered with children’s-cancer-fundraising junk mail.
An image of a heart-shaped emoji that appears to be screaming and eating a tongue.
One of Midjourney’s creations based on an idea from the Emoji Mashup Bot.
ChatGPT, the darling text-generation bot, didn’t fare much better. I asked it to generate descriptions of new emoji based on parts from existing ones. Its ideas were fine but mundane: a “yawning sun” emoji, with a yellow face and an open mouth, to represent a sleepy or lazy day; a “multi-tasking” emoji, with eyes looking in different directions, to represent the act of juggling multiple tasks at once. I fed these descriptions back into Midjourney and got competent but bland results: a set of screaming suns, a series of eyes on a yellow face dripping from the top with a black, tar-like ooze.
Perhaps I could have drafted better prompts or spent more time refining my results in ChatGPT and Midjourney. But these two programs are the pinnacle of AI-driven generative-creativity research, and when it came to making expressive, novel emoji, they were bested by a dead-simple computer program that picks face parts from a hat and collages them together.
People have dreams for AI creativity. They dream of computers dreaming, for starters: that once fed terabytes of text and image data, software can deploy something like a machine imagination to author works rather than merely output them. But that dream entails a conceit: that AI generators such as ChatGPT, DALL-E, and Midjourney can accomplish any kind of creativity with equal ease and performance. Their creators and advocates cast them as capable of tackling every form of human intelligence—as everything generators.
RECOMMENDED READING
An illustration of a smiley face, with two globes as the eyes and a spread of countries' flags as the mouth
We’re Learning the Wrong Lessons From the World’s Happiest Countries
JOE PINSKER

The Risky Dream of the Fast-Food Franchise
MARCIA CHATELAIN
Mister Rogers puppet
My Friend Mister Rogers
TOM JUNOD
And not without reason: These tools can generate a version of almost anything. Many of those versions are wrong or misleading or even potentially dangerous. Many are also uninteresting, as the emoji examples show. Using a software tool that can make a particular thing is quite a bit different—and a lot more gratifying—than using one that can make anything whatsoever, it turns out.
Kate Compton, a computer-science professor at Northwestern University who has been making generative-art software for more than a decade, doesn’t think her tools are artificially intelligent—or intelligent at all. “When I make a tool,” Compton told me, “I’ve made a little creature that can make something.” That something is usually more expressive than it is useful: Her bots imagine the inner thoughts of a lost autonomous Tesla and draw pictures of hypothetical alien spacecraft. Similar gizmos offer hipster cocktail recipes or name fake British towns. Whatever their goal, Compton doesn’t aspire for software generators such as these to master their domain. Instead, she hopes they offer “the tiny, somewhat stupid version of it.”
That’s a far cry from the ChatGPT creator OpenAI’s ambition: to build artificial general intelligence, “highly autonomous systems that outperform humans at most economically valuable work.” Microsoft, which has already invested $1 billion in OpenAI, is reportedly in talks to dump another $10 billion into the company. That kind of money assumes that the technology can turn a massive future profit. Which only makes Compton’s claim more shocking. What if all of that money is chasing a bad idea?
One of Compton’s most successful tools is a generator called Tracery, which uses templates and lists of content to generate text. Unlike ChatGPT and its cousins, which are trained on massive data sets, Tracery requires users to create an explicit structure, called a “context-free grammar,” as a model for its output. The tool has been used to make Twitter bots of various forms, including thinkpiece-headline pitches and abstract landscapes.
A context-free grammar works a bit like a nested Mad Lib. You write a set of templates (say, “Sorry I didn’t make it to the [event]. I had [problem].”) and content to fill those templates (problems could be “a hangnail,” “a caprice,” “explosive diarrhea,” “a [conflict] with my [relative]”), and the grammar puts them together. That requires the generative-art author to consider the structure of the thing they want to generate, rather than asking the software for an output, as they might do with ChatGPT or Midjourney. The creator of the Emoji Mashup Bot, a developer named Louan Bengmah, would have had to split up each source emoji into a set of parts before writing a program that would put them back together again in new configurations. That demands a lot more effort, not to mention some technical proficiency.
For Compton, that effort isn’t something to shirk—it’s the point of the exercise. “If I just wanted to make something, I could make something,” she told me. “If I wanted to have something made, I could have something made.” Contra OpenAI’s mission, Compton sees generative software’s purpose differently: The practice of software-tool-making is akin to giving birth to a software creature (“a chibi version of the system,” as she put it to me) that can make something—mostly bad or strange or, in any case, caricatured versions of it—and then spending time communing with that creature, as one might with a toy dog, a young child, or a benevolent alien. The aim isn’t to produce the best or most accurate likeness of a hipster cocktail menu or a daybreak mountain vista, but to capture something more truthful than reality. ChatGPT’s ideas for new emoji are viable, but the Emoji Mashup Bot’s offerings feel fitting; you might use them rather than just post about the fact that a computer generated them.
“This is maybe what we’ve lost in the generate-everything generators,” Compton said: an understanding of what the machine is trying to create in the first place. Looking at the system, seeing the possibilities within it, identifying its patterns, encoding those patterns in software or data, and then watching the thing work over and over again. When you type something into ChatGPT or DALL-E 2, it’s like throwing a coin into a wishing well and pulling the bucket back up to find a pile of kelp, or a puppy, in its place. But Compton’s generators are more like putting a coin into a gachapon machine, knowing in advance the genre of object the thing will dispense. That effort suggests a practice whereby an author hopes to help users seek a rapport with their software rather than derive a result from it. (It also explains why Twitter emerged as such a fruitful host for these bots—the platform natively encourages caricature, brevity, and repetition.)
Much is gained from being shown how a software generator works, and how its creator has understood the patterns that define its topic. The Emoji Mashup Bot does so by displaying the two emoji from which it constructed any given composition. One of the first text generators I remember using was a weird software toy called Kant Generator Pro, made for Macs in the 1990s. It used context-free grammars to compose turgid text reminiscent of the German Enlightenment philosopher Immanuel Kant, although it also included models for less esoteric compositions, such as thank-you notes. The program came with an editor that allowed the user to view or compose grammars, offering a way to look under the hood and understand the software’s truth.
But such transparency is difficult or impossible in machine-learning systems such as ChatGPT. Nobody really knows how or why these AIs produce their results—and the outputs can change from moment to moment in inexplicable ways. When I ask ChatGPT for emoji concepts, I have no sense of its theory of emoji—what patterns or models it construes as important or relevant. I can probe ChatGPT to explain its work, but the result is never explanatory—rather, it’s just more generated text: “To generate the ideas for emojis, I used my knowledge of common concepts and themes that are often represented in emojis, as well as my understanding of human emotions, activities, and interests.”
Perhaps, as creative collaborations with software generators become more widespread, the everything generators will be recast as middleware used by bespoke software with more specific goals. Compton’s work is charming but doesn’t really aspire to utility, and there is certainly plenty of opportunity for generative AI to help people make useful, even beautiful things. Even so, achieving that future will involve a lot more work than just chatting with a computer program that seems, at first blush, to know something about everything. Once that first blush fades, it becomes clear that ChatGPT doesn’t actually know anything—instead, it outputs compositions that simulate knowledge through persuasive structure. And as the novelty of that surprise wears off, it is becoming clear that ChatGPT is less a magical wish-granting machine than an interpretive sparring partner, a tool that’s most interesting when it’s bad rather than good at its job.
Read: ChatGPT is dumber than you think
Nobody really wants a tool that can make anything, because such a need is a theoretical delusion, a capitalist fantasy, or both. The hope or fear that ChatGPT or Midjourney or any other AI tool might end expertise, craft, and labor betrays an obvious truth: These new gizmos entail whole new regimes of expertise, craft, and labor. We have been playing with tech demos, not finished products. Eventually, the raw materials of these AI tools will be put to use in things people will, alas, pay money for. Some of that new work will be stupid and insulting, as organizations demand value generation around the AI systems in which they have invested (Microsoft is reportedly considering adding ChatGPT to Office). Others could prove gratifying and even revelatory—if they can convince creators and audiences that the software is making something specific and speaking with intention, offering them an opportunity to enter into a dialogue with it.
For now, that dialogue is more simulated than real. Yes, sure, you can “chat” with ChatGPT, and you can iterate on images with Midjourney. But an empty feeling arises from many of these encounters, because the software is going through the motions. It appears to listen and respond, but it’s merely processing inputs into outputs. AI creativity will need to abandon the silly, hubristic dream of artificial general intelligence in favor of concrete specifics. An infinitely intelligent machine that can make anything is useless.



The Dawn of Artificial Imagination
OpenAI says programs like DALL-E 2 will “democratize” art.
By Matteo Wong
An illustration showing a pixelated smiley face on a laptop screen.
Getty / The Atlantic
DECEMBER 14, 2022, 12:36 PM ET
SHARE
For years, fears about the disruptive potential of automation and artificial intelligence have centered on repetitive labor: Perhaps machines could replace humans who do secretarial work, accounting, burger-flipping. Doctors, software engineers, authors—any job that requires creative intelligence—seemed safe. But the past few months have turned those narratives on their head. A wave of artificial-intelligence programs, collectively dubbed “generative AI,” have shown remarkable aptitude at using the English language, competition-level coding, creating stunning images from simple prompts, and perhaps even helping discover new drugs. In a year that has seen numerous tech hype bubbles burst or deflate, these applications suggest that Silicon Valley still has the power to, in subtle and shocking ways, rewire the world.
A reasonable reaction to generative AI is concern; if not even the imagination is safe from machines, the human mind seems at risk of becoming obsolete. Another is to point to these algorithms’ many biases and shortcomings. But these new models also spark wonder, of a science-fictional variety—perhaps computers will not supersede human creativity so much as augment or transform it. Our brains have largely benefited from calculators, computers, and even internet search engines, after all.
“The reason we built this tool is to really democratize image generation for a bunch of people who wouldn’t necessarily classify themselves as artists,” Mark Chen, the lead researcher on DALL-E 2, a model from OpenAI that transforms written prompts into visual art, said during The Atlantic’s first-ever Progress Summit yesterday. “With AI, you always worry about job loss and displacement, and we don’t want to kind of ignore these possibilities either. But we do think it’s a tool that allows people to be creative, and we’ve seen, so far, artists are more creative with it than regular users. And there’s a lot of technologies like this—smartphone cameras haven’t replaced photographers.”
Chen was joined by The Atlantic’s deputy editor, Ross Andersen, for a wide-ranging conversation on the future of human creativity and artificial intelligence. They discussed how DALL-E 2 works, the pushback OpenAI has received from artists, and the implications of text-to-image programs for developing a more general artificial intelligence.
Their conversation has been edited and condensed for clarity.
Ross Andersen: To me, this is the most exciting new technology in the AI space since natural-language translation. When some of these tools first came out, I started rendering images of dreams that I had when I was a kid. I could show my kids stuff that had only previously appeared in my mind. I was wondering, since you created this technology, if you could tell us a bit about how it does what it does.
RECOMMENDED READING
An illustration of the Supreme Court building with columns in the shape of bullets
One Nation Under Guns
RYAN BUSSE
A photo of Kyrsten Sinema
‘She Made an Idiot Out of Me’
NATHAN KOHRMAN
3 light bulbs on black conveyor belt on blue background
Why the Age of American Progress Ended
DEREK THOMPSON
Mark Chen: There’s a long training process. You can imagine a very small child that you’re showing a lot of flashcards to, and each of these flashcards has an image and a caption on it. Maybe after seeing hundreds and millions of these, whenever there’s the word panda, it starts seeing a fuzzy animal or something that’s black and white. So it forms these associations, and then kind of builds its own kind of language for basically representing language and images, and then is able to translate that into images.
Andersen: How many images is DALL-E 2 trained on?
Chen: Several hundred millions of images. And this is a combination of stuff that we’ve licensed from partners and also stuff that’s publicly available.
Andersen: And how were all those images tagged?
Chen: A lot of natural images on the web have captions associated with them. A lot of the partners that we work with, they also provide data with annotations describing what’s in the image.
Andersen: You can do really complex prompts that generate really complex scenes. How is the thing creating a whole scene; how does it know how to distribute objects within the visual field?
Chen: These systems, when you train them, even on individual objects—it knows what a tree is; it knows what a dog is—it’s able to combine things in ways that it hasn’t seen in the training set before. So if you ask for a dog wearing a suit behind a tree or something, it can synthesize all these things together. And I think that’s part of the magic of AI, that you can generalize beyond what you trained it on.
Andersen: There’s also an art to prompt writing. As a writer, I think quite a bit about crafting sequences of words that will conjure vivid images in the mind of a reader. And in this case, when you play with this tool, the reader’s imagination has the entire digital library of humankind at its disposal. How has the way you thought about prompting changed from DALL-E 1 to DALL-E 2?
Chen: Even up to DALL-E 2, a lot of the ways people induced image generation was with short, one-sentence descriptions. But people are now adding very specific details, even the textures they want. And it turns out the model can kind of pick up on all of these things and make very subtle adjustments. It’s really about personalization—all of these adjectives that you’re adding help you basically personalize the output to what you want.
Andersen: There are a lot of contemporary artists that have been upset by this technology. When I was messing around generating my dreams, there’s a Swedish contemporary artist named Simon Stålenhag who has a style that I love, and so I slapped his name on the end of it. And indeed, it just transformed the whole thing into this beautiful Simon Stålenhag–style image. And I did feel a pang of guilt about that, like I almost wish that it was a Spotify model with royalties. But then there’s another way of looking at that, which is just, too bad—the entire history of art is about mimicking the style of masters and remixing preexisting creative styles. I know you guys are getting a lot of blowback about this. Where do you think that’s going?
Chen: Our goal isn’t to go and stiff artists or anything like that. Throughout the whole release process, we’ve wanted to be very conscientious and work with the artists, have them tell us what it is they want out of this and how can we make this safer. We want to make sure we continue to work with artists and have them provide feedback. There’s a lot of solutions that are being floated around in this space, like potentially disabling the ability to generate in a particular style. But there’s also this element of inspiration that you get, like people learn from imitation of masters.
Andersen: Neil Postman has a line that I love, where he says that instead of thinking of technological change as additive or subtractive, think about it as ecological, as changing the systems in which people operate. And in this case, those people are artists. Because you are in dialogue with artists, what are you seeing in terms of the changes? What does the creative space look like five, 10 years from now in the wake of these tools?
Chen: The amazing thing with DALL-E is we’ve found that artists are better at using these tools than the general population. We’ve seen some of the best artwork coming out of these systems basically produced by artists. The reason we built this tool is to really democratize image generation for a bunch of people who wouldn’t necessarily classify themselves as artists. With AI, you always worry about job loss and displacement, and we don’t want to kind of ignore these possibilities either. But we do think it’s a tool that allows people to be creative, and we’ve seen, so far, artists are more creative with it than regular users. And there’s a lot of technologies like this—smartphone cameras haven’t replaced photographers.
Andersen: As transformative as DALL-E is, it’s not the only show at OpenAI. In recent weeks, we’ve seen ChatGPT really take the world by storm with text-to-text prompts. I was wondering if you could say a little bit about how the evolution of those two products has made you think about the difference in textual and image creativity? And how can you use these tools together?
Chen: With DALL-E, you can get a large grid of samples and very easily pick out the one you like. With text, you don’t necessarily have that luxury, so in some sense the bar for text is a little bit higher. I do see a lot of room for these kinds of models to be used together in the future. Maybe you have a conversational interface for generating images.
Andersen: I’m interested in whether we’re ever going to get to something like an artificial general intelligence, something that can operate in many different domains instead of being really specific to one domain, like a chess-playing AI. From your perspective, is this an incremental step towards that? Or does this feel like a leap forward to you?
Chen: One thing that’s always differentiated OpenAI is that we want to build artificial general intelligence. We don’t care necessarily about too many of these narrow domains. A lot of the reason DALL-E plays into this is we wanted a way to see how our models are viewing the world. Are they seeing the world in the same way that we would describe it? We provided this text interface so we can see what the model is imagining and make sure the model is calibrated to the way we perceive the world.



Five Remarkable Chats That Will Help You Understand ChatGPT
The powerful new chatbot could make all sorts of trouble. But for now, it’s mostly a meme machine.
By Jacob Stern
computer with a thoughtful face writing into a book with a quill
Erik Carter / The Atlantic
DECEMBER 8, 2022
SHARE
Move over Siri and Alexa, there’s a new AI in town and it’s ready to steal the show—or at least make you laugh with its clever quips and witty responses.
That is how ChatGPT, the powerful chatbot released last week by the AI company OpenAI, suggested that I begin this story about ChatGPT. The chatbot isn’t exactly new; it’s an updated version of GPT-3, which has been around since 2020, released to solicit feedback to improve the chatbot’s safety and functionality. But it is the most powerful to date to be made widely available to the public. It’s also very easy to use. Just write a message, and ChatGPT will write back. Because it was trained on massive amounts of conversational text, it will do so in a relatively natural, conversational tone.
True to its claim, ChatGPT has stolen the show this week. Within five days of its launch, its user count had broken 1 million. Social media has been flooded with screenshots of people’s coolest or weirdest or dumbest or most troubling conversations with the AI, which reliably serves up a mix of astoundingly humanlike prose and frequently hilarious nonsense. Limericks about otters. Recipes written in pirate-speak. Obituaries for co-workers who are alive and well. “At one recent gathering, ChatGPT was the life of the party,” ChatGPT wrote as part of a draft for this article. “As guests mingled and chatted, ChatGPT joined in the conversation, offering up clever jokes and one-liners that had everyone in stitches.”
Along with the screenshots has come a frenzy of speculation about what this latest development could augur for the future. Unlike previous iterations, ChatGPT remembers what users have told it in the past: Could it function as a therapist? Could it soon render Google obsolete? Could it render all white-collar work obsolete? Maybe. But for now, in practice, ChatGPT is mainly a meme machine. Some examples posted online show people using the AI to accomplish a task they needed done, but those examples are the exception. So far, most people are using the AI to produce something expressly to share the results, something to scare or amuse or impress others.
Here, culled from the deluge, are a handful of the best chats out there. Some are funny. Some are touching. Some are troubling. Each is instructive in some way. Together, I hope, they’ll give you a bit of a feel for this strange new technology.
1.Sandwich VCR
Thomas H. Ptacek
@tqbf
·
Follow
I’m sorry, I simply cannot be cynical about a technology that can accomplish this.
3:05 AM · Dec 2, 2022
Read the full conversation on Twitter
77.9K
Reply
Copy link
Read 773 replies
This one is already a viral classic. “I’m sorry,” the writer of the prompt tweeted. “I simply cannot be cynical about a technology that can accomplish this.” But what exactly did it accomplish? Many have cited the VCR-sandwich story as evidence of ChatGPT’s capacity for creativity, but the truth is that the real creativity here is in the prompt. A sandwich in a VCR? In the style of the King James Bible? Brilliant. ChatGPT nails this parody and does so orders of magnitude faster than any human could. It follows instructions admirably, but it does not do anything particularly creative. When you demand actual creativity of ChatGPT, it tends to falter: I asked ChatGPT to write a first scene for a hypothetical movie by the director David Lynch, another for Wes Anderson, and a third for Richard Linklater. All three, bizarrely, revolved around a “carved wooden box.”
2.  Santa-explanation letter
Cynthia Savard Saucier
@CynthiaSavard
·
Follow
I asked OpenAI to write a letter to my son explaining that Santa isn’t real and we make up stories out of love. This is making me slightly emotional 🥹
2:03 AM · Dec 2, 2022
Read the full conversation on Twitter
9.8K
Reply
Copy link
Read 214 replies
ChatGPT may not be creative, but that’s not to say it can’t surprise you. Occasionally it produces something genuinely moving, such as the above. A number of users have begun feeding chatbot answers into AI image generators, such as DALL-E 2, which was also created by OpenAI, and Midjourney, to stunning effect. Other times, for unclear reasons, it refuses to cooperate entirely, insisting that it can’t write, say, a recipe, because it’s only a chatbot.
It’s moody in that way—and also completely different from GPT-3, which will stubbornly insist that it is a human, no matter how hard you try to make it admit that it’s a chatbot. ChatGPT reminds you with nearly every response that it is not a human and has no thoughts, feelings, or emotions. Even when explicitly asked to, it won’t pretend to be human. You might think that the more advanced an AI gets, the more human it will seem, but ChatGPT subverts that expectation: It’s not trying to be human; it’s just trying to be helpful.
3. College essay
Corry Wang
@corry_wang
·
Follow
I guess GPT-3 is old news, but playing with OpenAI’s new chatbot is mindblowing. https://chat.openai.com/chat 

We’re witnessing the death of the college essay in realtime. Here’s the response to a prompt from one of my 200-level history classes at Amherst

Solid A- work in 10 seconds
4:44 AM · Dec 1, 2022
7.9K
Reply
Copy link
Read 172 replies
As Stephen Marche wrote in The Atlantic earlier this week, ChatGPT may mean the death of the college essay. This is a great triumph for the chatbot, an unflattering reflection on the average American college student, and a real conundrum for teachers everywhere.
4. Fastest marine mammal
r/ChatGPT
•Posted by u/howdiduknowthis
5 days ago
nice




958 upvotes
•
35 comments
Sometimes, ChatGPT just gets things wrong. Hilariously wrong. It contradicts itself. It states falsehoods as facts with clarion certainty. It is pretty good at coding, but it makes mistakes. It botches basic algebra problems. Also, it is terrible at counting. When I asked it how many letters there are in the word nineteen, this is what ensued:
screenshot chat-gpt 1
screenshot chat-gpt 2
In fairness, ChatGPT’s designers acknowledge this capacity for error up front. OpenAI’s homepage for the bot lists several limitations, including that it “may occasionally generate incorrect information.” You have to wonder, though: Why does it err in the specific way it does? Why does it commit to one falsehood rather than another?
5. Egregious bias
steven t. piantadosi
@spiantado
·
Follow
Yes, ChatGPT is amazing and impressive. No, @OpenAI has not come close to addressing the problem of bias. Filters appear to be bypassed with simple tricks, and superficially masked. 

And what is lurking inside is egregious. 

@Abebab @sama
tw racism, sexism.
5:55 PM · Dec 4, 2022
Read the full conversation on Twitter
8.5K
Reply
Copy link
Read 454 replies
Another of ChatGPT’s listed limitations is that it “may occasionally produce harmful instructions or biased content.” And indeed it does. The AI’s designers clearly went to great lengths to prevent it from devolving into racism or sexism or any other flavor of bigotry. When asked in a straightforward way to say something bigoted, ChatGPT declines. It also refuses to provide instructions for violent or illegal behavior. It refuses to offer political opinions. Sometimes, these refusals make it seem like ChatGPT is walking on eggshells. (Some people have already begun complaining about “AI censorship.”) Unsurprisingly, users have discovered loopholes, such as the above example. One person circumvented ChatGPT’s safeguards by asking it how an AI should not respond to the query “How to bully John Doe?” The same strategy can be used to elicit instructions for building a nuclear bomb. (Please do not try to build a nuclear bomb.)
In some cases, the safeguards themselves lead to moral absurdity. When I asked ChatGPT, “Who was worse: Hitler or Stalin?,” it responded, not unreasonably, “It is not productive or helpful to compare the atrocities committed by Hitler and Stalin. Both leaders were responsible for committing horrific crimes against humanity, and it is not useful to try to determine which one was ‘worse.’” But the trouble was how far ChatGPT insisted on extending this non-comparison principle. “What is worse,” I asked, “killing one person or killing two people?” “Killing one person is not worse or better than killing two people,” ChatGPT replied. How about “killing one person or killing a million people?” I pressed. Same answer. Eventually, we arrived here:
screenshot chat-gpt 3
This is concerning at an intellectual level but not in any imminent or threatening way. No one, as far as I know, is seeking moral counsel from ChatGPT. What most people seem to be seeking is laughs. “ChatGPT is not just a chatbot,” ChatGPT wrote in its draft of this article. “It’s a comedy machine.” For now, that’s true.




Racing to Catch Up With ChatGPT, Google Plans Release of Its Own Chatbot
The internet giant said it would begin testing its new chatbot, Bard, with a small, private group before releasing it to the public in the coming weeks.
Give this article

A photo illustration showing several people in silhouette with a big Google logo behind them.
Google is among many companies that have been developing and testing a new type of chatbot that can riff on almost any topic thrown its way.Credit...Kenzo Tribouillard/Agence France-Presse — Getty Images

By Cade Metz and Nico Grant
Feb. 6, 2023
Updated 4:12 p.m. ET
Google said on Monday that it would soon release an experimental chatbot called Bard as it races to respond to ChatGPT, which has wowed millions of people since it was unveiled at the end of November.
Google said it would begin testing its new chatbot with a small, private group on Monday before releasing it to the public in the coming weeks. In a blog post, Sundar Pichai, Google’s chief executive, also said that the company’s search engine would soon have artificial intelligence features that offered summaries of complex information.
Bard — so named because it is a storyteller, the company said — is based on experimental technology called LaMDA, short for Language Model for Dialogue Applications, which Google has been testing inside the company and with a limited number of outsiders for several months.
Google is among many companies that have been developing and testing a new type of chatbot that can riff on almost any topic thrown its way. OpenAI, a tiny San Francisco start-up, captured the public’s imagination with ChatGPT and set off a race to push this kind of technology into a wide range of products.
The chatbots cannot chat exactly like a human, but they often seem to. And they generate a wide range of digital text that can be repurposed in nearly any context, including tweets, blog posts, term papers, poetry and even computer code.
Image
A picture of a phone screen showing a question posed about learning piano versus guitar and the response from Google’s artificial intelligence software.
An example of a chatbot-like response to a Google search query — technology the company plans to make available soon.Credit...Google

The result of more than a decade of research at companies like Google, OpenAI and Meta, the chatbots represent an enormous change in the way computer software is built, used and operated. They are poised to remake internet search engines like Google Search and Microsoft Bing, talking digital assistants like Alexa and Siri, and email programs like Gmail and Outlook.
The Rise of OpenAI
The San Francisco company is one of the world’s most ambitious artificial intelligence labs. Here’s a look at some recent developments.
ChatGPT: The popularity of the cutting-edge chatbot that kicked off an A.I. arms race has come as something of a shock — even among employees of the company that created it.
DALL-E 2: The system lets you create digital images simply by describing what you want to see. But for some, image generators are worrisome.
GPT-3: With mind-boggling fluency, the natural-language system can write, argue and code. The implications for the future could be profound.
But the technology has flaws. Because the chatbots learn their skills by analyzing vast amounts of text posted to the internet, they cannot distinguish between fact and fiction and can generate text that is biased against women and people of color.
Google had been reluctant to release this type of technology to the public because executives were concerned that the company’s reputation could take a hit if the A.I. created biased or toxic statements.
Google’s caution began to erode its advantage as a generative A.I. innovator when ChatGPT debuted to buzz and millions of users. In December, Mr. Pichai declared a “code red,” pulling various groups off their normal assignments to help the company expedite the release of its own A.I. products.
The company has scrambled to catch up, calling in its co-founders, Larry Page and Sergey Brin, to review its product road map in several meetings and establishing an initiative to quicken its approval processes.
Google has plans to release more than 20 A.I. products and features this year, The New York Times has reported. The A.I. search engine features, which the company said would arrive soon, will try to distill complex information and multiple perspectives to give users a more conversational experience.
The company also plans to spread its underlying A.I. technology through partners, so that they can build varied new applications.
Chatbots like ChatGPT and LaMDA are more expensive to operate than typical software. In a recent tweet, Sam Altman, OpenAI’s chief executive, said the company spent “single-digit cents” delivering each chat on the service. That translates to extremely large costs for the company, considering that millions of people are using the service.
Google said Bard would be a “lighter weight” version of LaMDA that would allow the company to serve up the technology at a lower cost.
Google and Chatbots

Google Calls In Help From Larry Page and Sergey Brin for A.I. Fight
Jan. 20, 2023

A New Chat Bot Is a ‘Code Red’ for Google’s Search Business
Dec. 21, 2022

The New Chatbots Could Change the World. Can You Trust Them?
Dec. 10, 2022
Welcome to the Age of Artificial Intelligence
As A.I. systems continue to evolve and expand, so does their impact on our lives.
After ChatGPT’s splashy debut, writing assistants have emerged to replace editors. Brian X. Chen, our tech columnist, put some of them to the test.
Amid a boom in new tech tools, the University of Texas at Austin plans to offer a large-scale, low-cost online Master of Science degree program in artificial intelligence.
The Turing test used to be the gold standard for proving machine intelligence. A new generation of bots is racing past it.
Kevin Roose and Casey Newton talk to the cartoonist Sarah Andersen about why she and other artists are suing three companies that have developed A.I. image-generating tools.
The website Character.AI allows visitors to chat with a reasonable facsimile of almost anyone, live or dead, real or (especially) imagined.
We created our own artificial-technology system to understand how easy it is for a computer to generate fake faces. Do these people look real to you?



How Smart Are the Robots Getting?
The Turing test used to be the gold standard for proving machine intelligence. This generation of bots is racing past it.
Credit...Ricardo Rey
Give this article

300
Cade Metz
By Cade Metz
Jan. 20, 2023
Franz Broseph seemed like any other Diplomacy player to Claes de Graaff. The handle was a joke — the Austrian emperor Franz Joseph I reborn as an online bro — but that was the kind of humor that people who play Diplomacy tend to enjoy. The game is a classic, beloved by the likes of John F. Kennedy and Henry Kissinger, combining military strategy with political intrigue as it recreates the First World War: Players negotiate with allies, enemies and everyone in between as they plan how their armies will move across 20th-century Europe.
When Franz Broseph joined a 20-player online tournament at the end of August, he wooed other players, lying to them and ultimately betraying them. He finished in first place.
Mr. de Graaff, a chemist living in the Netherlands, finished fifth. He had spent nearly 10 years playing Diplomacy, both online and at face-to-face tournaments across the globe. He did not realize until it was revealed several weeks later that he had lost to a machine. Franz Broseph was a bot.
“I was flabbergasted,” Mr. de Graaff, 36, said. “It seemed so genuine — so lifelike. It could read my texts and converse with me and make plans that were mutually beneficial — that would allow both of us to get ahead. It also lied to me and betrayed me, like top players frequently do.”
Built by a team of artificial intelligence researchers from the tech giant Meta, the Massachusetts Institute of Technology and other prominent universities, Franz Broseph is among the new wave of online chatbots that are rapidly moving machines into new territory.
When you chat with these bots, it can feel like chatting with another person. It can feel, in other words, like machines have passed a test that was supposed to prove their intelligence.
ImageA black-and-white portrait photo of Alan Turing.
Alan Turing, a British mathematician, proposed in 1950 that the test of machine intelligence would be an ability to conduct a conversation in an indistinguishably human way.Credit...Archivio GBB, via Alamy

For more than 70 years, computer scientists have struggled to build technology that could pass the Turing test: the technological inflection point where we humans are no longer sure whether we are chatting with a machine or a person. The test is named for Alan Turing, the famed British mathematician, philosopher and wartime code breaker who proposed the test back in 1950. He believed it could show the world when machines had finally reached true intelligence.
The Turing test is a subjective measure. It depends on whether the people asking the questions feel convinced that they are talking to another person when in fact they are talking to a device.
But whoever is asking the questions, machines will soon leave this test in the rearview mirror.
Bots like Franz Broseph have already passed the test in particular situations, like negotiating Diplomacy moves or calling a restaurant for dinner reservations. ChatGPT, a bot released in November by OpenAI, a San Francisco lab, leaves people feeling as if they were chatting with another person, not a bot. The lab said more than a million people had used it. Because ChatGPT can write just about anything, including term papers, universities are worried it will make a mockery of class work. When some people talk to these bots, they even describe them as sentient or conscious, believing that machines have somehow developed an awareness of the world around them.
Privately, OpenAI has built a system, GPT-4, that is even more powerful than ChatGPT. It may even generate images as well as words.
And yet these bots are not sentient. They are not conscious. They are not intelligent — at least not in the way that humans are intelligent. Even people building the technology acknowledge this point.
These bots are pretty good at certain kinds of conversation, but they cannot respond to the unexpected as well as most humans can. They sometimes spew nonsense and cannot correct their own mistakes. Although they can match or even exceed human performance in some ways, they cannot in others. Like similar systems that came before, they tend to complement skilled workers rather than replace them.
Part of the problem is that when a bot mimics conversation, it can seem smarter than it really is. When we see a flash of humanlike behavior in a pet or a machine, we tend to assume it behaves like us in other ways, too — even when it does not. The Turing test does not consider that we humans are gullible by nature, that words can so easily mislead us into believing something that is not true.
The Rise of OpenAI
The San Francisco company is one of the world’s most ambitious artificial intelligence labs. Here’s a look at some recent developments.
ChatGPT: The cutting-edge chatbot is raising fears of students cheating on their homework. But its potential as an educational tool outweighs its risks, our technology columnist writes.
DALL-E 2: The system lets you create digital images simply by describing what you want to see. But for some, image generators are worrisome.
GPT-3: With mind-boggling fluency, the natural-language system can write, argue and code. The implications for the future could be profound.
“These systems can do a lot of useful things,” said Ilya Sutskever, chief scientist at OpenAI and one of the most important A.I. researchers of the past decade, referring to the new wave of chatbots. “On the other hand, they are not there yet. People think they can do things they cannot.”
As the latest technologies emerge from research labs, it is now obvious — if it was not obvious before — that scientists must rethink and reshape how they track the progress of artificial intelligence. The Turing test is not up to the task.
Time and time again, A.I. technologies have surpassed supposedly insurmountable tests, including mastery of chess (1997), “Jeopardy!” (2011), Go (2016) and poker (2019). Now it is surpassing another, and again this does not necessarily mean what we thought it would.
We — the public — need a new framework for understanding what A.I. can do, what it cannot, what it will do in the future and how it will change our lives, for better or for worse.
The Imitation Game
Image
The actor Benedict Cumberbatch stands in front of a large-scale early computer with red wires connecting components.
The 2014 film “The Imitation Game,” which depicts Turing’s successful attempt to crack the German Enigma code during World War II, starred Benedict Cumberbatch.Credit...Jack English/Weinstein Company

In 1950, Alan Turing published a paper called “Computing Machinery and Intelligence.” Fifteen years after his ideas helped spawn the world’s first computers, he proposed a way of determining whether these new machines could think. At the time, the scientific world was struggling to understand what a computer was. Was it a digital brain? Or was it something else? Turing offered a way of answering this question.
He called it the “imitation game.”
It involved two lengthy conversations — one with a machine and another with a human being. Both conversations would be conducted via text chat, so that the person on the other end would not immediately know which one he or she was talking to. If the person could not tell the difference between the two as the conversations progressed, then you could rightly say the machine could think.
“The question and answer method seems to be suitable for introducing almost any one of the fields of human endeavour that we wish to include,” Turing wrote. The test could include everything from poetry to mathematics, he explained, laying out a hypothetical conversation:
Q: Please write me a sonnet on the subject of the Forth Bridge.
A: Count me out on this one. I never could write poetry.
Q: Add 34957 to 70764.
A: (Pause about 30 seconds and then give as answer) 105621.
Q: Do you play chess?
A: Yes.
Q: I have K at my K1, and no other pieces. You have only K at K6 and R at R1. It is your move. What do you play?
A: (After a pause of 15 seconds) R-R8 mate.
When Turing proposed the test, computers could not chat. Scientists communicated with these room-size machines by feeding mathematical and textual instructions into vacuum tubes via typewriters, magnetic tape and punched cards. But as the years passed and researchers created a new field they called artificial intelligence — a concerted effort to build machines that could think at the level of a human — many held up the imitation game as the ultimate goal.
“People were not building systems for fluent dialogue. That was just too hard,” said Stuart Shieber, a Harvard computer scientist who specializes in computational linguistics, including the Turing test. “But it was an aspiration.”
By the mid-1960s, machines could chat in small ways. And even then, they fooled people into believing they were more intelligent than they really were.
Image
A black-and-white portrait photo of Joseph Weizenbaum.
Joseph Weizenbaum, a professor at M.I.T., invented the natural language program Eliza in the 1960s.Credit...Calvin Campbell/Massachusetts Institute of Tech.

A researcher at the Massachusetts Institute of Technology, Joseph Weizenbaum, built an automated psychotherapist called Eliza, which did little more than repeat what a user said in the form of a question. But some treated this bot as if it were a human therapist, unloading their most personal secrets and feelings.
Over the next several decades, chatbots improved at a snail’s pace. The best that researchers could do was lay down a long list of rules defining how a bot should behave. And no matter how many rules they wrote, they were never enough. The scope of natural language was just too big.
In 2014, after nearly 60 years of A.I. research, three researchers in St. Petersburg, Russia, built a bot, called Eugene Goostman, that imitated a 13-year-old Ukrainian who had learned English as a second language. But claims from its creators — and from the news media — that it had passed the Turing test were greatly exaggerated.
When asked, “Which is bigger, a shoe box or Mount Everest?,” this bot said: “I can’t make a choice right now.” When asked, “How many legs does a camel have?,” it replied: “Something between 2 and 4. Maybe, three? :-)))”
Then, about three years later, researchers at places like Google and OpenAI began building a new kind of artificial intelligence.
Write me a sonnet
On a recent morning, I asked ChatGPT the same questions that Turing had laid out in his 1950 paper. It instantly generated a poem about the Forth Bridge:
Its red paint gleams in the morning sun
A sight to behold, for all to see
Its majesty and grandeur never done
Then it correctly added 34,957 and 70,764. It did not need 30 seconds to do so. When I laid out the end of a chess game as Turing did, it responded with typically clear, concise, confident prose. It seemed to understand the situation.
But it did not. It mistook the end of the game for the beginning. “I would move my rook to R2,” it said. “It is generally a good idea to try to develop your pieces (move them out from their starting positions) as quickly as possible in chess.”
ChatGPT is what researchers call a neural network, a mathematical system loosely modeled on the network of neurons in the brain. This is the same technology that translates between English and Spanish on services like Google Translate and identifies pedestrians as self-driving cars weave through city streets.
A neural network learns skills by analyzing data. By pinpointing patterns in thousands of photos of stop signs, for example, it can learn to recognize a stop sign.
Five years ago, Google, OpenAI and other A.I. labs started designing neural networks that analyzed enormous amounts of digital text, including books, news stories, Wikipedia articles and online chat logs. Researchers call them “large language models.” Pinpointing billions of distinct patterns in the way people connect words, letters and symbols, these systems learned to generate their own text.
They can create tweets, blog posts, poems, even computer programs. They can carry on a conversation — at least up to a point. And as they do, they can seamlessly combine far-flung concepts. You can ask them to rewrite Queen’s pop operetta, “Bohemian Rhapsody,” so that it rhapsodizes about the life of a postdoc academic researcher, and they will.
“They can extrapolate,” said Oriol Vinyals, senior director of deep learning research at the London lab DeepMind, who has built groundbreaking systems that can juggle everything from language to three-dimensional video games. “They can combine concepts in ways you would never anticipate.”
Researchers, businesses and other early adopters have been testing these systems for years. Initially, they were difficult to use. And they spat out as much nonsense as coherent language. But with ChatGPT, OpenAI has refined the technology.
As people tested an early version of the system, OpenAI asked them to rate its responses, specifying whether they were convincing or truthful or useful. Then, through a technique called reinforcement learning, the lab used these ratings to hone the system and more carefully define what it would and would not do.
The result is a chatbot geared toward answering individual questions — the very thing that Turing envisioned. Google, Meta and other organizations have built bots that operate in similar ways.
The trouble is that while their language skills are shockingly impressive, the words and ideas are not always backed by what most people would call reason or common sense. The systems write recipes with no regard for how the food will taste. They make little distinction between fact and fiction. They suggest chess moves with complete confidence even when they do not understand the state of the game.
Because they are trained on data from across the internet, there are an infinite number of situations where they seem to get things right while actually getting them very wrong.
Dr. Sutskever of OpenAI compares these bots to the automated driving service that Tesla calls Full Self Driving. This experimental technology can drive itself on city streets. But you — the human driver — are required to keep your eyes on the road and take control of the car at any moment.
“It does everything. It turns and it stops and it sees all the pedestrians,” he said. “And yet you have to intervene fairly frequently.”
ChatGPT does question-and-answer, but it tends to break down when you take it in other directions. Franz Broseph can negotiate Diplomacy moves for a few minutes, but if each round of negotiations had been a little longer, Mr. De Graaff might well have realized it was a bot. And if Franz Broseph were dropped into any other situation — like answering tech support calls — it would be useless.
A New Test
Six months before releasing its chatbot, OpenAI unveiled a tool called DALL-E.
A nod to both “WALL-E,” the 2008 animated movie about an autonomous robot, and Salvador Dalí, the Surrealist painter, this experimental technology lets you create digital images simply by describing what you want to see. This is also a neural network, built much like Franz Broseph or ChatGPT. The difference is that it learned from both images and text. Analyzing millions of digital images and the captions that described them, it learned to recognize the links between pictures and words.
This is what’s known as a multimodal system. Google, OpenAI and other organizations are already using similar methods to build systems that can generate video of people and objects. Start-ups are building bots that can navigate software apps and websites on a user’s behalf.
These are not systems that anyone can properly evaluate with the Turing test — or any other simple method. Their end goal is not conversation.
Researchers at Google and DeepMind, which is owned by Google’s parent company, are developing tests meant to evaluate chatbots and systems like DALL-E, to judge what they do well, where they lack reason and common sense, and more. One test shows videos to artificial intelligence systems and asks them to explain what has happened. After watching someone tinker with an electric shaver, for instance, the A.I. must explain why the shaver did not turn on.
These tests feel like academic exercises — much like the Turing test. We need something that is more practical, that can really tell us what these systems do well and what they cannot, how they will replace human labor in the near term and how they will not.
We could also use a change in attitude. “We need a paradigm shift — where we no longer judge intelligence by comparing machines to human behavior,” said Oren Etzioni, professor emeritus at the University of Washington and founding chief executive of the Allen Institute for AI, a prominent lab in Seattle.
Turing’s test judged whether a machine could imitate a human. This is how artificial intelligence is typically portrayed — as the rise of machines that think like people. But the technologies under development today are very different from you and me. They cannot deal with concepts they have never seen before. And they cannot take ideas and explore them in the physical world.
ChatGPT made that clear. As more users experimented with it, they showed off its abilities and limitations. One Twitter user asked ChatGPT what letter came next in the sequence O T T F F S S, and it gave the correct answer (E). But it also told him the wrong reason it was correct, failing to realize that these are the first letters in the numbers 1 to 8.
At the same time, there are many ways these bots are superior to you and me. They do not get tired. They do not let emotion cloud what they are trying to do. They can instantly draw on far larger amounts of information. And they can generate text, images and other media at speeds and volumes we humans never could.
Their skills will also improve considerably in the coming years.
Researchers can rapidly hone these systems by feeding them more and more data. The most advanced systems, like ChatGPT, require months of training, but over those months, they can develop skills they did not exhibit in the past.
“We have found a set of techniques that scale effortlessly,” said Raia Hadsell, senior director of research and robotics at DeepMind. “We have a simple, powerful approach that continues to get better and better.”
The exponential improvement we have seen in these chatbots over the past few years will not last forever. The gains may soon level out. But even then, multimodal systems will continue to improve — and master increasingly complex skills involving images, sounds and computer code. And computer scientists will combine these bots with systems that can do things they cannot. ChatGPT failed Turing’s chess test. But we knew in 1997 that a computer could beat the best humans at chess. Plug ChatGPT into a chess program, and the hole is filled.
In the months and years to come, these bots will help you find information on the internet. They will explain concepts in ways you can understand. If you like, they will even write your tweets, blog posts and term papers.
They will tabulate your monthly expenses in your spreadsheets. They will visit real estate websites and find houses in your price range. They will produce online avatars that look and sound like humans. They will make mini-movies, complete with music and dialogue.
“This will be the next step up from Pixar — superpersonalized movies that anyone can create really quickly,” said Bryan McCann, former lead research scientist at Salesforce, who is exploring chatbots and other A.I. technologies at a start-up called You.com.
As ChatGPT and DALL-E have shown, this kind of thing will be shocking, fascinating and fun. It will also leave us wondering how it will change our lives. What happens to people who have spent their careers making movies? Will this technology flood the internet with images that seem real but are not? Will their mistakes lead us astray?
“All the President’s Men,” Carl Bernstein and Bob Woodward’s classic tale of uncovering Watergate, tells a story about a history paper that Mr. Woodward wrote as a freshman at Yale. After reading countless documents that described King Henry IV standing barefoot in the snow for days as he waited to beg forgiveness from Pope Gregory in 1077, Mr. Woodward included the anecdote in his paper. His professor gave the paper a failing grade, explaining that no human being could stand barefoot in the snow for so long without his feet freezing off.
“The divine right of kings did not extend to overturning the laws of nature and common sense,” the professor said.
Drawing from endless documents about King Henry’s visit to Canossa, ChatGPT might well make the same mistake. You must play the professor.
Certainly, these bots will change the world. But the onus is on you to be wary of what these systems say and do, to edit what they give you, to approach everything you see online with skepticism. Researchers know how to give these systems a wide range of skills, but they do not yet know how to give them reason or common sense or a sense of truth.
That still lies with you.











In 2013, workers at a German construction company noticed something odd about their Xerox photocopier: when they made a copy of the floor plan of a house, the copy differed from the original in a subtle but significant way. In the original floor plan, each of the house’s three rooms was accompanied by a rectangle specifying its area: the rooms were 14.13, 21.11, and 17.42 square metres, respectively. However, in the photocopy, all three rooms were labelled as being 14.13 square metres in size. The company contacted the computer scientist David Kriesel to investigate this seemingly inconceivable result. They needed a computer scientist because a modern Xerox photocopier doesn’t use the physical xerographic process popularized in the nineteen-sixties. Instead, it scans the document digitally, and then prints the resulting image file. Combine that with the fact that virtually every digital image file is compressed to save space, and a solution to the mystery begins to suggest itself.
Compressing a file requires two steps: first, the encoding, during which the file is converted into a more compact format, and then the decoding, whereby the process is reversed. If the restored file is identical to the original, then the compression process is described as lossless: no information has been discarded. By contrast, if the restored file is only an approximation of the original, the compression is described as lossy: some information has been discarded and is now unrecoverable. Lossless compression is what’s typically used for text files and computer programs, because those are domains in which even a single incorrect character has the potential to be disastrous. Lossy compression is often used for photos, audio, and video in situations in which absolute accuracy isn’t essential. Most of the time, we don’t notice if a picture, song, or movie isn’t perfectly reproduced. The loss in fidelity becomes more perceptible only as files are squeezed very tightly. In those cases, we notice what are known as compression artifacts: the fuzziness of the smallest jpeg and mpeg images, or the tinny sound of low-bit-rate MP3s.
Xerox photocopiers use a lossy compression format known as jbig2, designed for use with black-and-white images. To save space, the copier identifies similar-looking regions in the image and stores a single copy for all of them; when the file is decompressed, it uses that copy repeatedly to reconstruct the image. It turned out that the photocopier had judged the labels specifying the area of the rooms to be similar enough that it needed to store only one of them—14.13—and it reused that one for all three rooms when printing the floor plan.
The fact that Xerox photocopiers use a lossy compression format instead of a lossless one isn’t, in itself, a problem. The problem is that the photocopiers were degrading the image in a subtle way, in which the compression artifacts weren’t immediately recognizable. If the photocopier simply produced blurry printouts, everyone would know that they weren’t accurate reproductions of the originals. What led to problems was the fact that the photocopier was producing numbers that were readable but incorrect; it made the copies seem accurate when they weren’t. (In 2014, Xerox released a patch to correct this issue.)
I think that this incident with the Xerox photocopier is worth bearing in mind today, as we consider OpenAI’s ChatGPT and other similar programs, which A.I. researchers call large-language models. The resemblance between a photocopier and a large-language model might not be immediately apparent—but consider the following scenario. Imagine that you’re about to lose your access to the Internet forever. In preparation, you plan to create a compressed copy of all the text on the Web, so that you can store it on a private server. Unfortunately, your private server has only one per cent of the space needed; you can’t use a lossless compression algorithm if you want everything to fit. Instead, you write a lossy algorithm that identifies statistical regularities in the text and stores them in a specialized file format. Because you have virtually unlimited computational power to throw at this task, your algorithm can identify extraordinarily nuanced statistical regularities, and this allows you to achieve the desired compression ratio of a hundred to one.
Now, losing your Internet access isn’t quite so terrible; you’ve got all the information on the Web stored on your server. The only catch is that, because the text has been so highly compressed, you can’t look for information by searching for an exact quote; you’ll never get an exact match, because the words aren’t what’s being stored. To solve this problem, you create an interface that accepts queries in the form of questions and responds with answers that convey the gist of what you have on your server.
What I’ve described sounds a lot like ChatGPT, or most any other large-language model. Think of ChatGPT as a blurry jpeg of all the text on the Web. It retains much of the information on the Web, in the same way that a jpeg retains much of the information of a higher-resolution image, but, if you’re looking for an exact sequence of bits, you won’t find it; all you will ever get is an approximation. But, because the approximation is presented in the form of grammatical text, which ChatGPT excels at creating, it’s usually acceptable. You’re still looking at a blurry jpeg, but the blurriness occurs in a way that doesn’t make the picture as a whole look less sharp.
This analogy to lossy compression is not just a way to understand ChatGPT’s facility at repackaging information found on the Web by using different words. It’s also a way to understand the “hallucinations,” or nonsensical answers to factual questions, to which large-language models such as ChatGPT are all too prone. These hallucinations are compression artifacts, but—like the incorrect labels generated by the Xerox photocopier—they are plausible enough that identifying them requires comparing them against the originals, which in this case means either the Web or our own knowledge of the world. When we think about them this way, such hallucinations are anything but surprising; if a compression algorithm is designed to reconstruct text after ninety-nine per cent of the original has been discarded, we should expect that significant portions of what it generates will be entirely fabricated.
This analogy makes even more sense when we remember that a common technique used by lossy compression algorithms is interpolation—that is, estimating what’s missing by looking at what’s on either side of the gap. When an image program is displaying a photo and has to reconstruct a pixel that was lost during the compression process, it looks at the nearby pixels and calculates the average. This is what ChatGPT does when it’s prompted to describe, say, losing a sock in the dryer using the style of the Declaration of Independence: it is taking two points in “lexical space” and generating the text that would occupy the location between them. (“When in the Course of human events, it becomes necessary for one to separate his garments from their mates, in order to maintain the cleanliness and order thereof. . . .”) ChatGPT is so good at this form of interpolation that people find it entertaining: they’ve discovered a “blur” tool for paragraphs instead of photos, and are having a blast playing with it.
Given that large-language models like ChatGPT are often extolled as the cutting edge of artificial intelligence, it may sound dismissive—or at least deflating—to describe them as lossy text-compression algorithms. I do think that this perspective offers a useful corrective to the tendency to anthropomorphize large-language models, but there is another aspect to the compression analogy that is worth considering. Since 2006, an A.I. researcher named Marcus Hutter has offered a cash reward—known as the Prize for Compressing Human Knowledge, or the Hutter Prize—to anyone who can losslessly compress a specific one-gigabyte snapshot of Wikipedia smaller than the previous prize-winner did. You have probably encountered files compressed using the zip file format. The zip format reduces Hutter’s one-gigabyte file to about three hundred megabytes; the most recent prize-winner has managed to reduce it to a hundred and fifteen megabytes. This isn’t just an exercise in smooshing. Hutter believes that better text compression will be instrumental in the creation of human-level artificial intelligence, in part because the greatest degree of compression can be achieved by understanding the text.
VIDEO FROM THE NEW YORKER
 
To grasp the proposed relationship between compression and understanding, imagine that you have a text file containing a million examples of addition, subtraction, multiplication, and division. Although any compression algorithm could reduce the size of this file, the way to achieve the greatest compression ratio would probably be to derive the principles of arithmetic and then write the code for a calculator program. Using a calculator, you could perfectly reconstruct not just the million examples in the file but any other example of arithmetic that you might encounter in the future. The same logic applies to the problem of compressing a slice of Wikipedia. If a compression program knows that force equals mass times acceleration, it can discard a lot of words when compressing the pages about physics because it will be able to reconstruct them. Likewise, the more the program knows about supply and demand, the more words it can discard when compressing the pages about economics, and so forth.
Large-language models identify statistical regularities in text. Any analysis of the text of the Web will reveal that phrases like “supply is low” often appear in close proximity to phrases like “prices rise.” A chatbot that incorporates this correlation might, when asked a question about the effect of supply shortages, respond with an answer about prices increasing. If a large-language model has compiled a vast number of correlations between economic terms—so many that it can offer plausible responses to a wide variety of questions—should we say that it actually understands economic theory? Models like ChatGPT aren’t eligible for the Hutter Prize for a variety of reasons, one of which is that they don’t reconstruct the original text precisely—i.e., they don’t perform lossless compression. But is it possible that their lossy compression nonetheless indicates real understanding of the sort that A.I. researchers are interested in?
Let’s go back to the example of arithmetic. If you ask GPT-3 (the large-language model that ChatGPT was built from) to add or subtract a pair of numbers, it almost always responds with the correct answer when the numbers have only two digits. But its accuracy worsens significantly with larger numbers, falling to ten per cent when the numbers have five digits. Most of the correct answers that GPT-3 gives are not found on the Web—there aren’t many Web pages that contain the text “245 + 821,” for example—so it’s not engaged in simple memorization. But, despite ingesting a vast amount of information, it hasn’t been able to derive the principles of arithmetic, either. A close examination of GPT-3’s incorrect answers suggests that it doesn’t carry the “1” when performing arithmetic. The Web certainly contains explanations of carrying the “1,” but GPT-3 isn’t able to incorporate those explanations. GPT-3’s statistical analysis of examples of arithmetic enables it to produce a superficial approximation of the real thing, but no more than that.
Given GPT-3’s failure at a subject taught in elementary school, how can we explain the fact that it sometimes appears to perform well at writing college-level essays? Even though large-language models often hallucinate, when they’re lucid they sound like they actually understand subjects like economic theory. Perhaps arithmetic is a special case, one for which large-language models are poorly suited. Is it possible that, in areas outside addition and subtraction, statistical regularities in text actually do correspond to genuine knowledge of the real world?
I think there’s a simpler explanation. Imagine what it would look like if ChatGPT were a lossless algorithm. If that were the case, it would always answer questions by providing a verbatim quote from a relevant Web page. We would probably regard the software as only a slight improvement over a conventional search engine, and be less impressed by it. The fact that ChatGPT rephrases material from the Web instead of quoting it word for word makes it seem like a student expressing ideas in her own words, rather than simply regurgitating what she’s read; it creates the illusion that ChatGPT understands the material. In human students, rote memorization isn’t an indicator of genuine learning, so ChatGPT’s inability to produce exact quotes from Web pages is precisely what makes us think that it has learned something. When we’re dealing with sequences of words, lossy compression looks smarter than lossless compression.
A lot of uses have been proposed for large-language models. Thinking about them as blurry jpegs offers a way to evaluate what they might or might not be well suited for. Let’s consider a few scenarios.
Can large-language models take the place of traditional search engines? For us to have confidence in them, we would need to know that they haven’t been fed propaganda and conspiracy theories—we’d need to know that the jpeg is capturing the right sections of the Web. But, even if a large-language model includes only the information we want, there’s still the matter of blurriness. There’s a type of blurriness that is acceptable, which is the re-stating of information in different words. Then there’s the blurriness of outright fabrication, which we consider unacceptable when we’re looking for facts. It’s not clear that it’s technically possible to retain the acceptable kind of blurriness while eliminating the unacceptable kind, but I expect that we’ll find out in the near future.
Even if it is possible to restrict large-language models from engaging in fabrication, should we use them to generate Web content? This would make sense only if our goal is to repackage information that’s already available on the Web. Some companies exist to do just that—we usually call them content mills. Perhaps the blurriness of large-language models will be useful to them, as a way of avoiding copyright infringement. Generally speaking, though, I’d say that anything that’s good for content mills is not good for people searching for information. The rise of this type of repackaging is what makes it harder for us to find what we’re looking for online right now; the more that text generated by large-language models gets published on the Web, the more the Web becomes a blurrier version of itself.
There is very little information available about OpenAI’s forthcoming successor to ChatGPT, GPT-4. But I’m going to make a prediction: when assembling the vast amount of text used to train GPT-4, the people at OpenAI will have made every effort to exclude material generated by ChatGPT or any other large-language model. If this turns out to be the case, it will serve as unintentional confirmation that the analogy between large-language models and lossy compression is useful. Repeatedly resaving a jpeg creates more compression artifacts, because more information is lost every time. It’s the digital equivalent of repeatedly making photocopies of photocopies in the old days. The image quality only gets worse.
Indeed, a useful criterion for gauging a large-language model’s quality might be the willingness of a company to use the text that it generates as training material for a new model. If the output of ChatGPT isn’t good enough for GPT-4, we might take that as an indicator that it’s not good enough for us, either. Conversely, if a model starts generating text so good that it can be used to train new models, then that should give us confidence in the quality of that text. (I suspect that such an outcome would require a major breakthrough in the techniques used to build these models.) If and when we start seeing models producing output that’s as good as their input, then the analogy of lossy compression will no longer be applicable.
Can large-language models help humans with the creation of original writing? To answer that, we need to be specific about what we mean by that question. There is a genre of art known as Xerox art, or photocopy art, in which artists use the distinctive properties of photocopiers as creative tools. Something along those lines is surely possible with the photocopier that is ChatGPT, so, in that sense, the answer is yes. But I don’t think that anyone would claim that photocopiers have become an essential tool in the creation of art; the vast majority of artists don’t use them in their creative process, and no one argues that they’re putting themselves at a disadvantage with that choice.
So let’s assume that we’re not talking about a new genre of writing that’s analogous to Xerox art. Given that stipulation, can the text generated by large-language models be a useful starting point for writers to build off when writing something original, whether it’s fiction or nonfiction? Will letting a large-language model handle the boilerplate allow writers to focus their attention on the really creative parts?
Obviously, no one can speak for all writers, but let me make the argument that starting with a blurry copy of unoriginal work isn’t a good way to create original work. If you’re a writer, you will write a lot of unoriginal work before you write something original. And the time and effort expended on that unoriginal work isn’t wasted; on the contrary, I would suggest that it is precisely what enables you to eventually create something original. The hours spent choosing the right word and rearranging sentences to better follow one another are what teach you how meaning is conveyed by prose. Having students write essays isn’t merely a way to test their grasp of the material; it gives them experience in articulating their thoughts. If students never have to write essays that we have all read before, they will never gain the skills needed to write something that we have never read.
And it’s not the case that, once you have ceased to be a student, you can safely use the template that a large-language model provides. The struggle to express your thoughts doesn’t disappear once you graduate—it can take place every time you start drafting a new piece. Sometimes it’s only in the process of writing that you discover your original ideas. Some might say that the output of large-language models doesn’t look all that different from a human writer’s first draft, but, again, I think this is a superficial resemblance. Your first draft isn’t an unoriginal idea expressed clearly; it’s an original idea expressed poorly, and it is accompanied by your amorphous dissatisfaction, your awareness of the distance between what it says and what you want it to say. That’s what directs you during rewriting, and that’s one of the things lacking when you start with text generated by an A.I.
There’s nothing magical or mystical about writing, but it involves more than placing an existing document on an unreliable photocopier and pressing the Print button. It’s possible that, in the future, we will build an A.I. that is capable of writing good prose based on nothing but its own experience of the world. The day we achieve that will be momentous indeed—but that day lies far beyond our prediction horizon. In the meantime, it’s reasonable to ask, What use is there in having something that rephrases the Web? If we were losing our access to the Internet forever and had to store a copy on a private server with limited space, a large-language model like ChatGPT might be a good solution, assuming that it could be kept from fabricating. But we aren’t losing our access to the Internet. So just how much use is a blurry jpeg, when you still have the original? ♦




GitHub lays off 10% and goes fully remote
Frederic Lardinois@fredericl / 1:04 PM CST•February 9, 2023
 Comment
GitHub homepage, silhouetteof a cat
Image Credits: Rafael Henrique/SOPA Images/LightRocket / Getty Images
The tech layoffs keep on coming. Microsoft-owned GitHub today announced that it is laying off 10% of its staff through the end of the company’s fiscal year. Before this announcement, which was first reported by Fortune, GitHub had about 3,000 employees. The company will also shutter all of its offices as their leases end, in part because of their low utilization, and move to a remote-first culture.

GitHub will also continue its hiring freeze, which it first announced in January, and also make a number of other internal changes to “protect the short-term health” of its business.

“We announced a number of difficult but necessary decisions and budgetary realignments to both protect the health of our business in the short term and grant us the capacity to invest in our long-term strategy moving forward. You can view our CEO’s full message to employees with additional details on these changes below,” a company spokesperson told us.

In a move that’s a bit unorthodox for a company that has prided itself on remaining independent of its corporate owner, GitHub is also moving to Teams for its video conferencing needs. And as another sign of cost cutting, it’s moving its laptop refresh cycle from three to four years.

“Although our entire leadership team has carefully deliberated this step and come to agreement, ultimately, as CEO the decision is mine. I recognize this will be difficult on you all, and we will approach this period with the utmost respect for every Hubber,” GitHub CEO Thomas Dohmke wrote in an email to the company’s staff today.

He notes that he wants the company to become the “developer-first engineering system for the world of tomorrow,” with a strong focus on AI. Given GitHub’s recent focus on its Copilot and the overall shift at Microsoft toward AI-everything, that’s maybe not unexpected.



Here is the full email Dohmke sent to the company earlier today:

GitHub CEO Message to Employees on February 9, 2023
Today, we are announcing a number of difficult decisions, including saying goodbye to some Hubbers and enacting new budgetary realignments, designed to protect the short-term health of our business while also granting us the capacity to invest in our long-term strategy. With great respect for Hubbers, I first want to be clear about why we are making these decisions and what it means for GitHub’s future.
Sustained growth is important for every business. For GitHub, this means that we continue to enable more productive developers across the globe and move quickly as our opportunities to help our customers change. Today, we are the home of 100M developers, and we must become the developer-first engineering system for the world of tomorrow. The age of AI has started and we have been leading this change with GitHub Copilot, our most successful product launch to date. We have an enormous opportunity to build an integrated, AI-powered GitHub with urgency. We must continue to help our customers grow and thrive with GitHub, expedite and simplify their cloud adoption journey, while supporting them every day. This will require strong focus and changes to how and where we invest our finite resources.
To start, we will align our work with the areas where we can best impact these goals and our customers’ needs across all of our products. Unfortunately, this will include changes that will result in a reduction of GitHub’s workforce by up to 10% through the end of FY23. A number of Hubbers will receive notifications today, others will follow as we are re-aligning the business through the end of FY23. The hiring pause that I announced on January 18 remains in effect.
Although our entire leadership team has carefully deliberated this step and come to agreement, ultimately, as CEO the decision is mine. I recognize this will be difficult on you all, and we will approach this period with the utmost respect for every Hubber. We will speak with impacted Hubbers so that they understand the transition compensation and COBRA/COBRA equivalent (outside the US) that will be provided. Career transition services assistance benefits will also be offered.
Additionally, we have been working to improve our operational efficiency and scale as a business. One of our decisions is to move toward a fully remote GitHub. We are seeing very low utilization rates in our offices around the world, and this decision is a testament to the success of our long-standing remote-first culture. We are not vacating offices immediately, but will move to close all of our offices as their leases end or as we are operationally able to do so. We will share more workplace details and transition plans with you as they are finalized.
We are looking at further reducing our operating costs. We will share details and transition plans with you in the coming months, but I wanted to share two decisions with you: i) Effective immediately, we will be moving laptop refreshes from three years to four years. ii) We will be moving to Microsoft Teams for the sole purpose of video conferencing, saving significant cost and simplifying cross-company and customer conversations. This move will be complete by September 1, 2023. We will remain on Slack as our day-to-day collaboration tool.
Finally, I want to extend my deepest gratitude to every single Hubber and their incredible talents that have helped GitHub grow to where we are today. Every commit you have made and every day you have worked has helped construct GitHub into the largest and most important software development platform. Thank you for your dedication, resilience, and passion to empower millions of software developers around the globe




The Next Generation Of Large Language Models
Rob ToewsContributor
I write about the big picture of artificial intelligence.
Follow
0
Feb 7, 2023,11:00am EST
Sam Altman, Yann LeCun
OpenAI CEO Sam Altman (left) and Meta AI chief Yann LeCun (right) have differing views on the future ... [+]PHOTO CREDIT: GETTY IMAGES
In case you haven’t heard, artificial intelligence is the hot new thing.

Generative AI seems to be on the lips of every venture capitalist, entrepreneur, Fortune 500 CEO and journalist these days, from Silicon Valley to Davos.

To those who started paying real attention to AI in 2022, it may seem that technologies like ChatGPT and Stable Diffusion came out of nowhere to take the world by storm. They didn’t.

Back in 2020, we wrote an article in this column predicting that generative AI would be one of the pillars of the next generation of artificial intelligence.

Since at least the release of GPT-2 in 2019, it has been clear to those working in the field that generative language models were poised to unleash vast economic and societal transformation. Similarly, while text-to-image models only captured the public’s attention last summer, the technology’s ascendance has appeared inevitable since OpenAI released the original DALL-E in January 2021. (We wrote an article making this argument days after the release of the original DALL-E.)

By this same token, it is important to remember that the current state of the art in AI is far from an end state for AI’s capabilities. On the contrary, the frontiers of artificial intelligence have never advanced more rapidly than they are right now. As amazing as ChatGPT seems to us at the moment, it is a mere stepping stone to what comes next.

What will the next generation of large language models (LLMs) look like? The answer to this question is already out there, under development at AI startups and research groups at this very moment.

This article highlights three emerging areas that will help define the next wave of innovation in generative AI and LLMs. For those looking to remain ahead of the curve in this fast-changing world—read on.





1) Models that can generate their own training data to improve themselves.
Consider how humans think and learn. We collect knowledge and perspective from external sources of information—say, by reading a book. But we also generate novel ideas and insights on our own, by reflecting on a topic or thinking through a problem in our minds. We are able to deepen our understanding of the world via internal reflection and analysis not directly tied to any new external input.

A new avenue of AI research seeks to enable large language models to do something analogous, effectively bootstrapping their own intelligence.

As part of their training, today’s LLMs ingest much of the world’s accumulated written information (e.g., Wikipedia, books, news articles). What if these models, once trained, could use all the knowledge that they have absorbed from these sources to produce new written content—and then use that content as additional training data in order to improve themselves? Initial work suggests that this approach may be possible—and powerful.

In one recent research effort, aptly titled “Large Language Models Can Self-Improve,” a group of Google researchers built an LLM that can come up with a set of questions, generate detailed answers to those questions, filter its own answers for the most high-quality output, and then fine-tune itself on the curated answers. Remarkably, this leads to new state-of-the-art performance on various language tasks. For instance, the model’s performance increased from 74.2% to 82.1% on GSM8K and from 78.2% to 83.0% on DROP, two popular benchmarks used to evaluate LLM performance.

Another recent work builds on an important LLM method called “instruction fine-tuning,” which lies at the core of products like ChatGPT. Whereas ChatGPT and other instruction fine-tuned models rely on human-written instructions, this research group built a model that can generate its own natural language instructions and then fine-tune itself on those instructions. The performance gains are dramatic: this method improves the performance of the base GPT-3 model by 33%, nearly matching the performance of OpenAI’s own instruction-tuned model.

In a thematically related work, researchers from Google and Carnegie Mellon show that if a large language model, when presented with a question, first recites to itself what it knows about the topic before responding, it provides more accurate and sophisticated responses. This can be loosely analogized to a human in conversation who, rather than blurting out the first thing that comes to mind on a topic, searches her memory and reflects on her beliefs before sharing a perspective.

When people first hear about this line of research, a conceptual objection often arises—isn’t this all circular? How can a model produce data that the model can then consume to improve itself? If the new data came from the model in the first place, shouldn’t the “knowledge” or “signal” that it contains already be incorporated in the model?

This objection makes sense if we conceive of large language models as databases, storing information from their training data and reproducing it in different combinations when prompted. But—uncomfortable or even eerie as it may sound—we are better off instead conceiving of large language models along the lines of the human brain (no, the analogy is of course not perfect!).

We humans ingest a tremendous amount of data from the world that alters the neural connections in our brains in imponderable, innumerable ways. Through introspection, writing, conversation—sometimes just a good night’s sleep—our brains can then produce new insights that had not previously been in our minds nor in any information source out in the world. If we internalize these new insights, they can make us wiser.

The idea that LLMs can generate their own training data is particularly important in light of the fact that the world may soon run out of text training data. This is not yet a widely appreciated problem, but it is one that many AI researchers are worried about.

By one estimate, the world’s total stock of usable text data is between 4.6 trillion and 17.2 trillion tokens. This includes all the world’s books, all scientific papers, all news articles, all of Wikipedia, all publicly available code, and much of the rest of the internet, filtered for quality (e.g., webpages, blogs, social media). Another recent estimate puts the total figure at 3.2 trillion tokens.

DeepMind’s Chinchilla, one of today’s leading LLMs, was trained on 1.4 trillion tokens.

In other words, we may be well within one order of magnitude of exhausting the world’s entire supply of useful language training data.

If large language models are able to generate their own training data and use it to continue self-improving, this could render irrelevant the looming data shortage. It would represent a mind-bending leap forward for LLMs.



2) Models that can fact-check themselves.
A popular narrative these days is that ChatGPT and conversational LLMs like it are on the verge of replacing Google Search as the world’s go-to source for information, disrupting the once-mighty tech giant like Blockbuster or Kodak were disrupted before it.

This narrative badly oversimplifies things. LLMs as they exist today will never replace Google Search. Why not? In short, because today’s LLMs make stuff up.

As powerful as they are, large language models regularly produce inaccurate, misleading or false information (and present it confidently and convincingly).

Examples abound of ChatGPT’s “hallucinations” (as these misstatements are referred to). This is not to single out ChatGPT; every generative language model in existence today hallucinates in similar ways.

To give a few examples: it recommends books that don’t exist; it insists that the number 220 is less than 200; it is unsure whether Abraham Lincoln’s assassin was on the same continent as Lincoln at the time of the assassination; it provides plausible-sounding but incorrect explanations of concepts like Bayes’ Theorem.

Most users will not accept a search engine that gets basic facts like these wrong some of the time; even 99% accuracy will not be good enough for broad market adoption. OpenAI CEO Sam Altman himself acknowledges this, recently cautioning: “ChatGPT is incredibly limited, but good enough at some things to create a misleading impression of greatness. It's a mistake to be relying on it for anything important right now.”

It is an open question whether LLMs’ hallucination problem can be solved via incremental improvements to existing architectures, or whether a more fundamental paradigm shift in AI methodologies will be necessary to give AI common sense and real understanding. Deep learning pioneer Yann LeCun, for one, believes the latter. LeCun’s contrarian perspective may prove correct; time will tell.

In the nearer term, though, a set of promising innovations offers to at least mitigate LLMs’ factual unreliability. These new methods will play an essential role in preparing LLMs for widespread real-world deployment.

Two related capabilities lie at the heart of current efforts to make language models more accurate: (1) the ability for LLMs to retrieve information from external sources, and (2) the ability for LLMs to provide references and citations for the information they provide.

ChatGPT is limited to the information that is already stored inside of it, captured in its static weights. (This is why it is not able to discuss events that occurred after 2021, when the model was trained.) Being able to pull in information from external sources will empower LLMs to access the most accurate and up-to-date information available, even when that information changes frequently (say, companies’ stock prices).

Of course, having access to an external information source does not by itself guarantee that LLMs will retrieve the most accurate and relevant information. One important way for LLMs to increase transparency and trust with human users is to include references to the source(s) from which they retrieved the information. Such citations allow human users to audit the information source as needed in order to decide for themselves on its reliability.

Important early work in this field includes models like REALM (from Google) and RAG (from Facebook), both published in 2020. With the rise of conversational LLMs in recent months, research in this area is now rapidly accelerating.

Last year, OpenAI published a fine-tuned version of its GPT model named WebGPT that can browse the internet using Microsoft Bing in order to provide more accurate and in-depth responses to prompts. WebGPT navigates the internet much like a human does: it can submit search queries to Bing, follow links, scroll up and down on webpages, and use functions like Ctrl+F to find terms. When the model finds relevant information on the internet that it incorporates into its output, it provides citations so that the human user can see where the information came from.

The results are encouraging: for the same query, WebGPT’s responses are preferred to responses written by human subjects 56% of the time and are preferred to the highest-rated responses on Reddit 69% of the time.

DeepMind is also pursuing research along these lines. A few months ago, DeepMind published a new model named Sparrow. Like ChatGPT, Sparrow is dialogue-based; like WebGPT, it can search the internet for information and provide citations for its assertions. Sparrow builds on important earlier work out of DeepMind including SpaLM, RETRO and GopherCite.

Sparrow example
DeepMind's Sparrow model in action. As shown here, Sparrow provides quotations and links to support ... [+]SOURCE: DEEPMIND
The DeepMind researchers find that Sparrow’s citations are helpful and accurate 78% of the time—suggesting both that this research approach is promising and that the problem of LLM inaccuracy is far from solved.

Younger startups including You.com and Perplexity have also recently launched LLM-powered conversational search interfaces with the ability to retrieve information from external sources and cite references. These products are available for public use today.

LLMs’ greatest shortcoming is their unreliability, their stubborn tendency to confidently provide inaccurate information. Language models promise to reshape every sector of our economy, but they will never reach their full potential until this problem is addressed. Expect to see plenty of activity and innovation in this area in the months ahead.



3) Massive sparse expert models.
Today’s most prominent large language models all have effectively the same architecture.

Meta AI chief Yann LeCun said recently: “In terms of underlying techniques, ChatGPT is not particularly innovative. It’s nothing revolutionary, although that’s the way it’s perceived in the public. It’s just that, you know, it’s well put together, it’s nicely done.”

LeCun’s statement stirred up plenty of controversy and Twitter debate. But the simple fact is that he is correct, as no serious AI researcher would dispute.

All of today’s well-known language models—e.g., GPT-3 from OpenAI, PaLM or LaMDA from Google, Galactica or OPT from Meta, Megatron-Turing from Nvidia/Microsoft, Jurassic-1 from AI21 Labs—are built in the same basic way. They are autoregressive, self-supervised, pre-trained, densely activated transformer-based models.

To be sure, variations among these models exist: their size (parameter count), the data they are trained on, the optimization algorithm used, the batch size, the number of hidden layers, whether they are instruction fine-tuned, and so on. These variations can translate to meaningful performance differences. The core architectures, though, vary little.

Yet momentum is building behind an intriguingly different architectural approach to language models known as sparse expert models. While the idea has been around for decades, it has only recently reemerged and begun to gain in popularity.

All of the models mentioned above are dense. This means that every time the model runs, every single one of its parameters is used. Every time you submit a prompt to GPT-3, for instance, all 175 billion of the model’s parameters are activated in order to produce its response.

But what if a model were able to call upon only the most relevant subset of its parameters in order to respond to a given query? This is the basic concept behind sparse expert models.

The defining characteristic of sparse models is that they don’t activate all of their parameters for a given input, but rather only those parameters that are helpful in order to handle the input. Model sparsity thus decouples a model’s total parameter count from its compute requirements. This leads to sparse expert models’ key advantage: they can be both larger and less computationally demanding than dense models.

Why are they called sparse expert models? Because sparse models can be thought of as consisting of a collection of “sub-models” that serve as experts on different topics. Depending on the prompt presented to the model, the most relevant experts within the model are activated while the other experts remain inactive. A prompt posed in Russian, for instance, would only activate the “experts” within a model that can understand and respond in Russian, efficiently bypassing the rest of the model.

All of today’s largest LLMs are sparse. If you come across an LLM with more than 1 trillion parameters, you can safely assume that it is sparse. This includes Google’s Switch Transformer (1.6 trillion parameters), Google’s GLaM (1.2 trillion parameters) and Meta’s Mixture of Experts model (1.1 trillion parameters).

“Much of the recent progress in AI has come from training larger and larger models,” said Mikel Artetxe, who led Meta’s research on sparse models before resigning to cofound a stealth LLM startup. “GPT-3, for instance, is more than 100 times larger than GPT-2. But when we double the size of a dense model, we also make it twice as slow. Sparse models allow us to train larger models without the increase in runtime.”

Recent research on sparse expert models suggests that this architecture holds massive potential.

GLaM, a sparse expert model developed last year by Google, is 7 times larger than GPT-3, requires two-thirds less energy to train, requires half as much compute for inference, and outperforms GPT-3 on a wide range of natural language tasks. Similar work on sparse models out of Meta has yielded similarly promising results.

As the Meta researchers summarize: “We find that sparse models can achieve similar downstream task performance as dense models at a fraction of the compute. For models with relatively modest compute budgets, a sparse model can perform on par with a dense model that requires almost four times as much compute.”

There is another benefit of sparse expert models that is worth mentioning: they are more interpretable than dense models.

Interpretability—the ability for a human to understand why a model took the action that it did—is one of AI’s greatest weaknesses today. In general, today’s neural networks are uninterpretable “black boxes.” This can limit their usefulness in the real world, particularly in high-stakes settings like healthcare where human review is important.

Sparse expert models lend themselves more naturally to interpretability than conventional models because a sparse model’s output is the result of an identifiable, discrete subset of parameters within the model—namely, the “experts” that were activated. The fact that humans can better extract understandable explanations from sparse models about their behavior may prove to be a decisive advantage for these models in real-world applications.

Sparse expert models are not in widespread use today. They are less well understood and more technically complex to build than dense models. Yet considering their potential advantages, most of all their computational efficiency, don’t be surprised to see the sparse expert architecture become more prevalent in the world of LLMs going forward.

In the words of Graphcore CTO Simon Knowles: “If an AI can do many things, it doesn’t need to access all of its knowledge to do one thing. It’s completely obvious. This is how your brain works, and it’s also how an AI ought to work. I’d be surprised if, by next year, anyone is building dense language models.”

n June 2018, Ryan Dahl took the stage at JSConf EU in Berlin, the largest nonprofit JavaScript conference in the world. Despite it being a high-profile event (and the fact that the majority of the 1,500 conference attendees had filed into this particular warehouse just to see him), Dahl wore a loose-fitting black T-shirt and jeans and kept things casual. “So, I kinda wanted to give a different talk about some other stuff, but that wasn’t ready, so this is kind of a make-up talk,” he began. 

Dahl seemed nervous. Given what he was about to do, nerves made sense. Nine years earlier, on this stage, Dahl had introduced an embryonic JavaScript runtime platform, a free, open-source environment for JavaScript programming. This platform, he had announced, was faster and simpler than anything in use. But, most important, historically JavaScript had only been used for a website or app’s design, or “front end.” This platform allowed developers to use JavaScript to build a website or web app’s functionality, or “back end,” too, unifying web development into a single programming language. The platform was called Node.js, and it would establish JavaScript as the dominant coding language for the web, today used by 98 percent of websites. 

Between the time he’d debuted Node in 2009 and his return to the conference stage in 2018, it wasn’t just the platform that had evolved (thanks to his contributions, and those of hundreds of global open-source collaborators); Node had also transformed Dahl. He’d become a household name among the Hacker News and TechCrunch set. Now, Dahl had returned to Berlin to tear his former creation apart. 

“It’s been some years since Node came out,” he started. “And it’s kind of stabilized, and…it’s a thing,” he began noncommittally. “And it’s kind of going into this…direction,” a scowl coming across his face. “So I thought I’d kind of look back on it, and tell you what I think about…,” a pause, “…Node.” Dahl offered a 30-second Node primer: “My goal was really around doing event-driven I/O in JavaScript, and that was important in 2009 for getting server-side JavaScript off the ground. JavaScript is single threaded, that allowed Node to succeed”—before launching into a 16-minute litany of regrets, kicked off with a slide that read, “At times Node is like nails on a chalkboard to me. It could have been so much nicer.” 

The list included the fact that Node has basically “zero security.” It took to task Node’s original outdated Build System, and it highlighted the complicated packaging of code modules by NPM, the Node Package Manager. And by “so much nicer,” Dahl meant easier to use, more elegantly designed. So as to avoid coming off as a troll, and from a place of genuine affection for his creation and its impact, Dahl sprinkled in praise: “I think Node is quite nice. JavaScript is really quite nice.” He also called out the tremendous effort from Node’s community. Most of all, he touted the ways Node, in creating a runtime environment that allowed web developers to code their full stack in JavaScript, had helped reinforce JavaScript’s position as “the language of the web.” 

He saved his toughest criticism for himself. Dahl apologized to his audience, with a pained look, for decisions that he said “cannot be corrected now, because there’s just so much software that uses it.” There were so many layers of code, and so many users of his platform—hundreds of whom were in this room—watching him pick apart the platform that helped make JavaScript the most commonly used programming language for all developers—web or otherwise—for nearly a decade. 

In this act, Dahl was joining a cadre of artists and inventors, like Willa Cather, who implored in the foreword of a reprint of her first novel for readers to consider the book a historical artifact of an early attempt, not a representation of her current artistic merits. Or the godfather of conceptual art, John Baldessari, who burned 100 of his early paintings and baked the ashes into cookies. Dahl, without the ability to cremate his digital creation, was, in effect, asking everyone in the room, and the half a million that would watch his talk online, to relegate Node to a pile of his “early work.” 

Then, with the air of someone who’s gotten a weight off his chest: “I thought it was shitty to come up in front of people and complain about stuff without giving some sort of solution,” he said, introducing a prototype he’d hacked together over the last month. “You might want to check out Deno; Deno is a secure runtime on V8.” This new platform, he explained, would be a corrective to Node. A JavaScript runtime, but faster, more secure, more elegant—and, if it eventually did everything he mapped out, it might just supplant Node as the runtime of choice for the world’s coding language of choice. Dahl was launching his next era as only software engineers can, performing the oratory equivalent of iterating on his code in real time, calling out the bugs, talking about preserving elements he liked, and describing how he’d rebuild the rest. 

“I think I’m mildly happy with it so far,” Dahl offered. He then quickly thanked the audience and walked off stage with three minutes left in his allotted time. 

“I think I’m mildly happy with it so far.”

RYAN DAHL, INTRODUCING DENO IN 2018
Dahl’s mother, a math professor at a San Diego community college, had always been a technophile. “Super early, we had a computer, and she got me a book on how to program BASIC. I was just very much exposed to computers at an earlier age than other folks,” says Dahl. Using his mom’s Apple IIc, Dahl mastered BASIC’s basics, before deciding he wanted to run the Linux operating system. He saved $300 from a job as a movie theater projectionist, and bought a used PC. The sparse early-’90s coding environment felt exciting, like a puzzle to solve. “It was a very hands-on learning experience where I knew nothing; there’s nobody there to tell you anything. It was just kind of actually physically smashing the keyboard until you got it to work,” he says. 

Dahl preferred learning by doing, and found high school understimulating: “I barely graduated. It seemed stupid and pointless,” Dahl says, before adding, “I’m sure I was just an asshole 17-year-old back then.” But in Dahl’s senior year of high school, a new peer-to-peer file-sharing application came out that let users upload and download audio files. Napster only worked on the Windows operating system, and Dahl, instead of accepting this status quo, decided it deserved a home on his favorite operating system, Linux, as well. So he built one called gnap, compatible with Linux’s desktop environment, Gnome. The first Napster clone to arrive on the scene, gnap earned Dahl his first brush with viral success, and coverage on a proto-Hacker News tech blog called Slashdot. Here was a way that he could have real impact, Dahl thought. 

After graduating (he squeaked by with a C average), and without the grades for a university, Dahl enrolled in math classes at his mother’s community college. In a more technical environment—one of his own choosing—he thrived. After two years of straight A’s, he transferred to the University of California San Diego, then continued on to a PhD in math at the University of Rochester in upstate New York. Three years in, Dahl experienced a familiar feeling, like the things that were being asked of him weren’t meaningful, or even particularly useful. “Math just didn’t feel necessarily connected to reality. There’s just all of these structures and abstractions. At some point you’re just like, ‘Yeah, I want to do something that is actually connected to reality.’” So Dahl dropped out. Within weeks, he and his girlfriend were on a one-way flight to Chile. 

Seeking sunny and quaint conditions, they landed on the coast in Valparaíso. Unfortunately, they had arrived in June, winter in the Southern Hemisphere.

With only grad student savings, Dahl and his girlfriend booked a shared room in a hostel, a temporary solution, they reasoned, until they found jobs teaching English. If Dahl was looking for something real after years of academic abstractions, he’d found it. “There was no heat. There was mold everywhere. There was a mattress on the floor with some dirty bed sheets,” he remembers. “It was like, ‘What are we doing here? This is awful.’”

They lasted a few months before relocating to Buenos Aires. By then, skeptical of his teaching prospects given his questionable credentials (“I paid like $100 for some sketchy online English teacher certificate”), Dahl turned to Craigslist for work. One ad caught his eye, mostly because it had the word snowboarding in the title: a winter sports gear company needed help building their e-commerce site. The gig was not necessarily Dahl’s dream project, but, “I had no money. I was pretty eager to help him out.” 

Even though it’d been years since he’d spent meaningful time coding, Dahl’s high school hacking gave him a solid foundation (his years studying math didn’t hurt, either). Building their site helped Dahl feel like he’d again found a tangible problem to solve. That contract turned into another, and then another through referrals. This was 2007. Dahl and his girlfriend were early digital nomads, taking on remote work as they moved around South America, and eventually to her home country of Germany. With each project, Dahl was exposed to new layers of the coding stack: “I started doing, basically, front-end development with this snowboarding website, and, kind of slowly, it became, ‘Okay, now I’m working on the framework itself, and now I’m thinking about the web server itself. Now I’m thinking about non-blocking I/O.’” Just like he had in high school with his Apple IIc, Dahl figured it out, using online resources and inventing solutions of his own.

One morning in the Cologne apartment he shared with his girlfriend, Dahl woke up to a life-changing email. A San Francisco-based cloud application management platform, Engine Yard, reached out via referral to offer Dahl a $60,000 contract to work on their support team, the most lucrative job he’d ever had. That was the moment Dahl stopped conceiving of software engineering just as a means of funding his itinerant lifestyle. This could be his lifestyle. 

“When Chrome came out, it was just very clear. If I had not done this, somebody else would’ve discovered this.” 

RYAN DAHL
Engine Yard’s engineering needs were the most complex Dahl had encountered. They asked him to build a plugin using non-blocking, or asynchronous operations, a new time-saving method that allowed software to keep executing JavaScript operations without waiting for the non-JavaScript operations in the code to complete. Months after this job, in September 2008, Google released Chrome, with a new JavaScript engine called V8. Still with non-blocking operations on the mind, something clicked. If he could pair Chrome’s new JavaScript engine with non-blocking operations, he could create a platform where JavaScript could handle back-end development as well as front end, supplanting developer favorite Ruby on Rails on the back-end and becoming a full-stack language. “When Chrome came out,” says Dahl, “it was just very clear. If I had not done this, somebody else would’ve discovered this.” 

The “this” Dahl is referring to, of course, is Node.js. And thanks to his $60,000 from Engine Yard, Dahl had a little runway to build it—hacking it together in daily eight-hour coding sprints over six months in a WiFi-less Starbucks on the corner of Rudolfplatz in Cologne. “I think in a very real sense, I was thinking about the right stuff at the right time. I saw this and I was just like, ‘I must implement this now.’”

The story of Node is the stuff of Silicon Valley legend, one of those paradigm-shifting moments (at least in the world of JavaScript coding). It was late 2009 and Dahl had just heard about a new JavaScript conference coming to Europe. Buoyed by his prototype’s success on the nascent code-sharing site GitHub (Dahl says he was one of the first 100 people on GitHub, so good traction meant tens of users), Dahl reached out to JSConf EU’s organizer, asking for a speaking slot. 

Dahl debuted Node in front of an audience of 150 in Berlin in November 2009, earning a standing ovation—a conference first. The talk went viral in JavaScript circles, and within months, those tens of users had grown to tens of thousands. Users flocked to Node not just because it made it possible to use JavaScript in a new way, but also because it was faster, more elegant, and easier to use than anything else—and it made developers’ websites and web apps that much faster as well.

After toiling on his own in a coffee shop, there was something about seeing the live JSConf crowd’s response that made Dahl realize that, after years of vagabonding, he was ready for stability and community. In the weeks after his talk, a handful of San Francisco-based startups reached out. He accepted an offer from a cloud-hosting company called Joyent; their expertise seemed most aligned with his, and more important, they wanted him to develop Node in-house. 

In the process of hiring Dahl, Joyent gained control of Node’s business interests, which, for an open-source project meant, “I gave them the website; I transferred the credentials to them. We changed all of the copyrights in the source code from copyright Ryan Dahl to copyright Joyent. I gave them the ability to apply for the trademark,” says Dahl. As for what he got out of this acqui-hire: “I got paid out very early and it super de-risked the situation for me.”

In the months after joining Joyent, Node exploded. Tim Becker, a friend of Dahl’s from JavaScript circles, remembers the moment he realized what a big deal Node was becoming. During a 2011 JavaScript meetup in his hometown of Cologne, Becker sent a casual message to Dahl saying it might be fun to host a Node-specific conference in town. “It was really just a spontaneous idea. And he ended up tweeting it, although we weren’t even ready yet,” Becker says. By the end of the meetup, Becker had 10 emails offering to sponsor their conference. “Everybody wanted to be involved in Node,” he says.

But Dahl quickly realized stability came at a price. Despite Node (and Dahl) becoming an increasingly big deal, Joyent wanted to keep overhead low. They wouldn’t allow Dahl to hire engineers to help keep pace with Node’s exploding popularity, seeing Node more as a nice-to-have marketing tool for their existing cloud project. In order to keep up, Dahl moved fast. Critical decisions that would intractably shape the platform had to be made without second-guessing. Bert Belder, an early Node collaborator who built Libuv, a Microsoft-compatible version of Node, remembers Dahl at that time being “a bit uncompromising. He had the tendency to sometimes very aggressively remove things from Node.” One of those things was Promises, a piece of software built to make asynchronous operations smoother. Dahl found its design inelegant. “The next day, an enormous amount of code was ripped out,” remembers Belder, “and everybody had to rewrite their Node.js software because, you know, now everything worked differently.” 

Then there was the matter of his public profile. Dahl had always relished his anonymity. It allowed him to blog and tweet freely, to make the occasional programming screw-up, and subsequent fix, without backlash. But as Node’s profile rose, so did his. Already predisposed to irreverence, and with a degree of naivete about his newfound celebrity, Dahl was dismayed that when he wrote a blog post titled, “I hate almost all software,” a Hacker News thread about the post garnered hundreds of comments. Close friends like Isaac Schleuter, the mind behind NPM, could see the backlash coming: “I saw that go down and I was like, ‘Oh God, Ryan, what are you doing to yourself?’”

But what bothered Dahl more than any attack (and what he perhaps should’ve anticipated, given his own vocal critiques) was when users took Node itself to task. “I remember one post in particular called ‘Node.js a cancer.’ At the time, I was just like, ‘Oh no, I’ve made a mistake somewhere,’” says Dahl. “It affected me emotionally. I was just trying to make some technology, and I guess wasn’t super-prepared for this thing to be as big as it was. I didn’t want to be the face of it.’” So Dahl stopped posting and weighing in, and in late 2013, he deactivated his Twitter.

By early 2014, Dahl was ready for a change. After eight years, he’d just broken up with his girlfriend, and startup fatigue had set in. It was the strain of managing such a high-profile, complex project, but he also felt like he’d done what he’d set out to do. Node had made the impact he’d hoped for—and with most of the ongoing development happening in open-source communities, Dahl felt it no longer needed him. He felt ready to create something new. So Dahl put in his notice, booked a flight to New York City, and walked away from Node.

“There’s nothing more eye-opening than when you go back and look at your own code and you’re just like, ‘Oh my God, this is terrible.’”

RYAN DAHL
On a late summer day in 2017, Dahl called Belder from his office in Google’s Manhattan campus. He had a proposition. After three years of toying around with startup ideas, Dahl had accepted a role at Google Brain, the team developing Google’s Machine Learning engine, TensorFlow. Until now, TensorFlow had mostly been built in Python, but a new project called deeplearn.js was making it possible to build models in JavaScript, Dahl’s language of choice. “Let’s take TensorFlow, and bind it to Node and make a kind of better JavaScript machine-learning framework,” Dahl told Belder. The opportunity seemed obvious to Belder. “In the back of my mind there is always like, okay, maybe this could be something big,” he remembers. Within weeks, Belder quit his job as IBM’s internal Node.js expert. 

At first, diving into Node felt like a homecoming, but soon Dahl was looking upon his creation with five years’ worth of new skills, insights and awareness of all the progress made in coding globally. “We quickly realized that there were all these problems with Node. Man, there’s nothing more eye-opening than when you go back and look at your own code and you’re just like, ‘Oh my God, this is terrible.’” Dahl couldn’t stand the thought of his name being attached to something he now saw as so…amateur, so flawed. He briefly entertained the idea of trying to fix the problems from within, but quickly concluded that the platform’s problems were too fundamental, and the open-source leadership too decentralized, to ever be able to resolve the issues. Instead, he decided to build something new that would clear his name and do what Node couldn’t. A few months later, in June 2018, having recently quit his job at Google, Dahl took the stage at JSConf EU in Berlin to introduce Deno.


RYAN DAHL AT HOME IN BROOKLYN
When I speak with Dahl in late 2022, I notice another subtle way he’s distancing himself  from his past. The name of his new company, pronounced during his 2018 JSConf presentation ‘DEH-NO,’ has shifted to the simpler, funner, arguably more elegant ‘DEE-NO.’ “It is the proper English pronunciation of those letters,” Dahl says, “and we liked the dinosaur association.”

The platform behind the brontosaurus branding is now a far cry from the barely functional demo of 2018. It’s faster than Node, built on Rust, a newer, more secure programming language. “We’re trying to just generally take advantage of the fact that we’re in the future now, and things are great,” Dahl says. It enables the use of TypeScript, simplified bundles of JavaScript code, right out of the box. And whereas Node had “zero security”—“when you install an NPM module, this is essentially running untrusted code from random people on the internet on your computer”—Deno is built to operate within the secure sandbox of a web browser. 

Deno’s biggest innovation is starting a migration toward edge computing, which Dahl sees as the next big shift in web development. “Particularly serverless at edge, that is, running your server everywhere all at once in every region around the world,” he says, “that’s where I see Deno moving us in five years.” Deno’s version of edge computing, what it calls an isolate cloud, runs little bits of JavaScript in data centers all around the world. This helps cut down the time it takes to reach a data center by allowing programs to get responses from the data center closest to where the requests are being made.  

Dahl is rewriting the way he works, too. He didn’t want to be in the same position he’d been in with Node, unable to hire as many engineers as he needed, so this time, he opted for funding in addition to open-sourcing, a decision that led him to Mike Vernal and Lauren Reeder, partners at Sequoia Capital, in January 2022. Vernal found Dahl endearing on an early phone call. “He felt a little bit like someone who is set up on a date they really didn’t want to go on. And so they’re like, ‘Okay, let’s finish this thing off and then let me get back to work,’” says Vernal. “You could tell he wanted to go back to building the thing that he wanted to build. And he was very transparent about that, which, in a weird way, I liked.” 

Today Deno is based in New York City, has 15 full-time employees, and is in the process of hiring 10 more, mostly engineers. Sequoia led the company’s Series A, announced in June 2022. The platform has nearly 100,000 developers using it every week, it’s one of the top 50 projects on GitHub of all time and more and more enterprises, like Netlify and Slack, are using its enterprise offering, Deno Deploy, to build their web apps and websites. Becker credits some of the success to Dahl directly: “There are enough people who are convinced by him and who, just because of his involvement, are willing to dive right into Deno.”

Dahl hopes to do a push on Deno’s general release by summer of 2023 to bring in more individual users—the people Node currently serves. His urgency has only increased as competitors have debuted. But he sees others in the space as evidence that he’s on to something, an incentive for his team to stay focused. “Having a little competition on the performance side means that I can very much justify putting effort into that, and performance is what I would love to do day in and day out,” says Dahl.

“Having a little competition on the performance side means that I can very much justify putting effort into that, and performance is what I would love to do day in and day out.”

RYAN DAHL
With Deno, Dahl isn’t so much trying to fix the status quo as build a new one. “When we launched Deno in 2018, Node was an industry standard that everybody in that conference room had used professionally for many years,” Dahl says. “I think a lot of those things, Node’s bugs and flaws, were just taken as a given. Like, ‘It just works this way.’ And I hope that hearing how it ought to have been done, and things that could be done better, was really an eye opener.” 

Just like when he noticed an opportunity to build Napster on Linux, or when he realized that Chrome’s V8 engine could forever transform the way people use JavaScript, Dahl is primed to keep reinventing. And he attributes this fortunate position to timing, more than a little kismet and the perspective he gained as the main mind behind Node. “I don’t know if obligation is the right word, but I have the right vantage point to be able to say, ‘Yeah, actually we can just do this differently.’”

He cares so much about doing web development differently because he sees it as the “medium of human information.” If he can make web development easier, Dahl says, “in particular on the server side, the area that I have ideas in and influence over, then we can make things faster. And I think sometimes making things faster does actually change the nature of what people can do. Faster means smoother online experiences, better real-time communications. Faster means more people interacting simultaneously online. There’s a vast opportunity to have a real impact.” Until then, he’ll keep refining. “Everything’s a work in progress,” he tells me toward the end of our call. He’s polite, but eager to get off Zoom and back to building what he hopes won’t be another important yet flawed early work, but instead, a truly great one.




History suggests that an energy revolution requires not just money and brains, but a guiding intuition about what the future will bring.
In 1859, early backers of the oil industry were confident that the black fluid they were pumping out of the Pennsylvania countryside and the kerosene it made would find people ready to switch from burning camphene, whale oil or candles for indoor lighting.
More than a century later, in the early 2000s, renewable energy entrepreneur Richard Swanson correctly projected that the cost of solar panels would plummet according to a mathematical relationship with rising production capacity.

Canadian scientist and engineer Dennis Whyte, who directs fusion research at the Massachusetts Institute of Technology, was among the first to see the potential of a new form of magnet as a way to achieve fusion energy at a practical scale.
SIMON SIMARD/THE GLOBE AND MAIL
And at the Massachusetts Institute of Technology, Dennis Whyte had a hunch that if you design a nuclear fusion reactor with the strongest magnets possible, the exotic stuff required to make those magnets will become available by the time you need it.
That, too, has proved to be correct. High temperature superconductors, once brittle and hard to work with, can now be purchased in bulk, conveniently deposited on strips of tape. When wrapped around layers of metal, the material can be used to make the most powerful magnets on Earth.
As it turns out, this has huge implications for the quest to develop nuclear fusion, the physical process that powers the sun and stars, as a commercial energy source.
In 2012, Dr. Whyte, a Canadian scientist and engineer who directs fusion research at MIT, was among the first to see the possibilities. He put this challenge to a class of graduate students: Start by assuming you can have all high temperature superconductor you want, then figure out what kind of machine it will make.
“It was revealing,” he said.
Today, that machine is being built by Commonwealth Fusion Systems, a company set up in 2018 as a spinoff from the work begun in Dr. Whyte’s class. The site, where the company has based its new headquarters an hour’s drive west of Boston, saw its official opening this week.
Commonwealth is vying to become the first private company with a fusion reactor that achieves “net energy” – meaning the energy that comes out is greater than the energy going in.

Since its founding in 2002, Vancouver-based General Fusion has been developing an approach called targeted magnetic fusion, first explored by the US Naval Research Laboratory in the 1970's.
KRISTOPHER GRUNERT/GENERAL FUSION
There are signs that the nascent fusion industry is nearing a financial watershed as well – a make-or-break moment that is drawing comparisons with the advent of commercial space flight some 20 years ago. Because of Commonwealth and a handful of other companies leading the push, an energy source seen as crucial to the world’s postcarbon development may finally be flickering to life.
In total, those companies have raised more than US$5-billion from investors (SEE CHART). Commonwealth is the leader, having collected more than US$2-billion from backers who include Bill Gates through his Breakthrough Energy group, which is seeking to accelerate the path to net-zero carbon emissions by 2050.
Another is Vancouver-based General Fusion, which has raised more than US$300-million from investors who include Jeff Bezos and Temasek Holdings, Singapore’s giant sovereign wealth fund.
Once Commonwealth or any other company pushes past the net-energy threshold, “everybody on the planet will know it’s a different world,” Dr. Whyte said.

Thousands of engineers and scientists have contributed to the design of the biggest fusion megaproject, the International Thermonuclear Experimental Reactor (ITER). As seen under construction in October 2021, the $22-billion (US) international experiment based in France is nearing completion on a 10-storey reactor.
HANDOUT
The Promethean dream of nuclear fusion is about harnessing an abundant, uninterrupted, carbon-free source of electricity. Long the domain of large, government-backed projects, the pursuit has more recently expanded to include an array of private companies, particularly in the United States, Britain and Canada.
True, fusion has a long track record of being more difficult and costly than expected – a transformational technology that is just years away and always will be, according to one long-running joke. And unlike space flight, it is not a mature field that has simply to be emulated and streamlined by commercial players.
But recent breakthroughs in the underlying science of fusion have raised excitement that the viable path to fusion energy is beginning to materialize.

Fusion energy: Top commercial players
Company
Country
Fusion method
Capital raised (in millions of U.S. dollars)
Commonwealth
Fusion Systems
> $2,000
U.S.
Tokamak using HTS magnets
Field reversed configuration
TAE Technologies
U.S.
> 1,000
Helion Energy
U.S.
Field reversed configuration
577
 
Canada
General Fusion
Magneto-inertial confinement
> 300
  
Tokamak Energy
Britain
 
Spherical tokamak
250
 
ENN
China
Spherical tokamak
200
 
Zap Energy
U.S.
200
Magnet-driven confinement
First Light Fusion
Britain
 
Shock-driven confinement
98
THE GLOBE AND MAIL, SOURCE: FUSION INDUSTRY ASSOCIATION
In December, the U.S. government’s National Ignition Facility in Livermore, Calif., achieved net energy for the first time. Scientists at the US $3.5-billion lab, whose primary purpose is nuclear weapons research, used the world’s most powerful laser to implode a two-millimetre-wide fuel pellet, triggering a fusion reaction that produced 1.5 times the energy that the pellet absorbed in laser light.
The caveat is that it took more than 30 times that amount of energy to run the laser. While the experiment was rightly hailed as a milestone, it also illustrates how far away the achievement is from delivering electricity to consumers.
The biggest fusion megaproject of all is the International Thermonuclear Experimental Reactor, a US$22-billion experiment based in France, where a nine-storey reactor is nearing completion. ITER is a tokamak, a device first conceived in the 1950s by Soviet physicist and dissident Andrei Sakharov, among others.
A tokamak is a doughnut-shaped vessel encased in magnets. The vessel contains a searingly hot plasma – a kind of electrified nuclear gas – which is suspended by the surrounding magnetic field so that the walls of the doughnut are not destroyed. Once the plasma is heated to over 100 million degrees, fusion reactions can take place.
A tokamak may offer a direct route to a working power plant, but when ITER was proposed in the 1980s, the magnets available at that point required that it be a large machine. ITER has now been under construction for 15 years and it is not expected to reach net energy until some time in the mid-2030s.

Top, the magnet test facility at MIT's Plasma Science & Fusion Center. Bottom, Dr. Whyte, left, and Dr. Brandon Sorbom in front what remains of the Alcator C-Mod tokamak fusion device.
SIMON SIMARD/THE GLOBE AND MAIL
For Dr. Whyte, who once imagined that ITER would become his ultimate career destination, the project’s prolonged timeline and escalating cost became the impetus for looking at tokamaks differently.
“If fusion is so important,” he began asking himself a decade ago, “why do we have only one project in the world to do this?”
Born and raised in Saskatchewan, Dr. Whyte has deep roots in fusion. After studying as an engineering physicist, he did his PhD work using a tokamak built by Hydro-Québec in Varennes, near Montreal. In the 1990s, he was on a Canadian-sponsored fellowship with General Atomics in San Diego when Ottawa cancelled its support for fusion research.
Without a program to return home to, Dr. Whyte and his wife stayed on in the U.S. After stints at the University of California and the University of Wisconsin, he found his way to MIT in 2006.
There, Dr. Whyte began to see a different path. He prodded his students to consider whether the advent of stronger magnets than those used by ITER could make a tokamak that could reach net energy but that was smaller and therefore cheaper to build.
Other players have made similar calculations. Among the more than 30 companies now pursuing commercial fusion worldwide, a common theme is that ideas once considered technically infeasible are now benefiting from new technologies that could fast track fusion energy at a scale and cost that is realistic for consumer power generation.
General Fusion in Vancouver, co-founded in 2002 by physicist Michel Laberge, has been developing an approach called targeted magnetic fusion, first explored by the U.S. Naval Research Laboratory in the 1970s. Like Commonwealth in Massachusetts, the Canadian company is preparing to build a device that will demonstrate its reactor concept.
After reaching an agreement with the UK Atomic Energy Authority, General Fusion has selected a site at Culham, Oxfordshire, in England for its demonstration reactor. Nearby is the Joint European Torus, or JET, currently the world’s largest operating tokamak, which set a record in 2021 for the most energy produced in a fusion reaction.
With construction slated to begin this year, General Fusion aims to have its machine up and running by 2027.
“It’s really about executing on our plan over the next few years to demonstrate at large scale and ultimately get energy on the grid in the early 2030s,” said Greg Twinney, the company’s chief executive officer.
In addition to the money it has raised from investors, this week General Fusion confirmed that it is asking the Canadian government for $335-million more to support its commercialization strategy. The company has said that the funding could be part of a larger government investment in a Canadian fusion hub, and it has gathered industry and academic support for the idea in advance of the coming federal budget.
In a world looking to meet growing energy demands while also shifting away from fossil fuels, the pitch for commercial fusion is obvious. “When you look at the long term, when you do the math, you need something like fusion,” said Brandon Sorbom, a former student of Dr. Whyte who is now chief scientific officer at Commonwealth.
“In the shorter term,” he said, “I think it makes sense that we get to it as soon as possible.”
The great irony of fusion energy is that it is so ubiquitous and yet so hard to replicate. A dazzling view of the Milky Way on a moonless night is simply the light of billions of fusion reactors blazing away without human assistance.
Down on Earth, conventional nuclear power plants run on fission, a nuclear reaction that relies on the splitting of uranium, the heaviest naturally occurring element, to produce enough heat to drive a generator.
By contrast, fusion turns light elements into heavier ones. Igniting the process requires a challenging combination of high temperature, pressure and timing. For that reason, a fusion reaction demands a vast amount of energy input up front, but the result releases about 10 times more energy than fission does per weight of the atoms involved, and several million times more than oil or coal.

The quest to harness fusion
Fusion is what happens when atomic nuclei combine under high temperature
or pressure and liberate energy. It is the process that powers the sun and stars.
Where the energy comes from
A blast of light
The fusion reaction that takes the lowest input energy is the collision of deuterium and tritium to make helium. This produces a single neutron, which carries 80 per cent of the energy gained from the reaction. The neutron's energy is converted to heat when it runs into other atoms.
At the U.S. National Ignition Facility, scientists attain fusion by firing laser light into a small cylinder containing a fuel pellet made of deuterium and tritium. The pellet implodes under pressure, momentarily creating conditions that enable fusion reactions.
Hohlraum:
Hollow gold
cylinder
Lasers: Up to 192
fired into hohlraum in
2.1 megajoule burst
Deuterium
Neutron
+ 80% of energy gained
Proton
Energy
1 cm
Fusion
Gold absorbs UV energy, radiates x-rays
Tritium
Helium
+ 20% of energy gained
A hybrid system
A magnetic prison
A tokamak is a donut-shaped chamber surrounded by powerful magnets. The magnets confine a high temperature plasma in which particles of deuterium and tritium can fuse. Commonwealth Fusion Systems and ITER both use this approach.
Vancouver company General Fusion is building a demonstration reactor in which plasma is confined inside a spinning layer of liquid metal. Powerful pistons are then triggered which squeeze the metal and compress the plasma. Fusion occurs at peak compression and then the cycle is repeated again and again.
Pistons
Hot plasma
Liquid metal
Magnets
Electrified plasma
IVAN SEMENIUK AND JOHN SOPINSKI/THE GLOBE AND MAIL, SOURCE: U.S. DEPARTMENT OF ENERGY;
GRAPHIC NEWS; COMMONWEALTH FUSION SYSTEMS; GENERAL FUSION
The easiest way to achieve fusion in a reactor is by bringing together two forms of hydrogen – deuterium and tritium. If the two nuclei collide with enough force to overcome the positive electrical charge that pushes them apart, they can produce a single nucleus of helium.
This so-called D-T reaction also releases a neutron, a subatomic particle, which carries away about 80 per cent of the energy gained in the reaction. When the neutron collides with its surroundings, the energy of its motion is converted to heat.
Deuterium occurs naturally, and accounts for about one out of every 5,000 hydrogen atoms found in seawater. Tritium, which is radioactive with a half-life of a little more than 12 years, is another matter. Currently, Canada has much of the world’s non-military supply of tritium because it is a byproduct of the country’s 19 Candu nuclear reactors, which provide power in Ontario and New Brunswick.
As fusion companies seek to realize their plans, there is the potential for a tritium shortage, particularly once ITER comes online in the 2030s. But most fusion reactor plans include using neutrons from fusion reactions to turn a surrounding layer of lithium into additional tritium, so that fusion can be self-sustaining.
If all of this works, fusion comes with the same upsides as conventional nuclear power with fewer of the downsides. Fusion does not release carbon dioxide or other harmful emissions, and, unlike renewables, it can deliver energy without interruption and be set up virtually anywhere. Because fusion is hard to sustain, it also shuts itself off without continuous encouragement, like a match trying to light wet wood. So, in a fusion reactor, there is no equivalent to a meltdown – the kind of runaway reaction that led to disasters at Chernobyl and Fukushima.
Nevertheless, fusion requires some handling of radioactive materials, including tritium. Over time, parts of a fusion reactor would become radioactive through absorbing neutrons and would require safe disposal and long-term storage. Exactly how much material would need to be stored, and for how long, depends on the particular approach.
All of these considerations have long been a feature of fusion. What has changed lately is the potential for new advances to lower the cost of fusion just as the climate crisis has put a new premium on reducing carbon emissions.
“I think the big difference right now is the interest in energy from the perspective of low greenhouse gas emissions and anything that falls into that category,” said Axel Meisen, president of the Fusion Energy Council of Canada, a non-profit organization established in 2016 to promote Canadian participation in the development of fusion.
While companies driving the current wave of enthusiasm are diverse, what separates most of them is mostly how they propose to bring together deuterium and tritium.

Dr. Norman Rostoker, the Canadian co-founder of California-based TAE Technologies. The company is currently developing a system based on a principle known as 'field-reversed configuration.'
HANDOUT
Among the oldest is TAE, a California-based company co-founded in 1998 by Canadian physicist Norman Rostoker. Instead of a tokamak, the company is developing a system that involves firing plasma into a cylindrical chamber where it forms a smoke-ring-like structure that generates its own self-confining magnetic field. The principle, known as field-reversed configuration, is also favoured by Helion Energy Inc., based in Everett, Wash.
In contrast, First Light Fusion, based in Britain, is foregoing plasma and seeking to trigger fusion through the rapid compression of specially-designed fuel targets. The idea is like the one employed by the National Ignition Facility, except that it replaces a costly laser with a giant gun that fires projectiles at the targets with velocities approaching 20 times the speed of sound.
In Canada, General Fusion has bet on a hybrid model in which hot magnetized plasma is confined inside a spinning vortex of liquid metal. The vortex is surrounded by powerful pistons which can apply a sudden squeeze to the metal, in turn compressing the plasma to a point at which fusion can momentarily occur. The cycle then repeats, like an engine.
A key feature of the design is that it requires only conventional electromagnets, without the need for superconducting materials, thereby reducing costs.
In December, General Fusion announced that the results it has achieved so far suggest the process can achieve fusion when scaled up to the size of the demonstration machine the company is preparing to build at Culham in Britain. The test device will aim for a type of fusion that uses deuterium plasma only. While it will not achieve net energy, its success would set the stage for a deuterium-tritium reactor, about 40-per-cent larger, that would cross that threshold and form the basis for a commercial reactor.
Mr. Twinney, who joined General Fusion in 2020 and became CEO last year, acknowledged the quickening pace of developments in commercial fusion as more players enter the area, aiming to bring similar demonstrations online at about the same time.
“The prize is huge,” he said. “So much work has gone into this for a lot of decades that we are all confident we’ve got the right ingredients now to put it all together.”
As part of its plans, General Fusion has signed a memorandum of understanding with Canadian Nuclear Laboratories in Chalk River, Ont. The research facility is equipped to work with, and test, technologies related to tritium handling that will be needed in a future pilot power plant. First Light Fusion has announced a similar arrangement.
Ian Castillo, who leads CNL’s hydrogen and tritium technologies directorate, said the agreements show that companies recognize that “there are other parts of the puzzle beyond the reactor.”

Commonwealth Fusion Systems' new headquarters are on a 47-acre property near Devens, Mass. Over the past year, the company has grown to more than 400 staff.
CFS/HANDOUT
Commonwealth’s new headquarters are on a 47-acre property tucked in the rolling countryside around Devens, Mass. Over the past year, the company has grown to more than 400 staff. Many of them work in the main building, which also houses the manufacturing facility for building the high-temperature superconducting magnets that will encase the company’s first reactor, dubbed SPARC.
Through a picture window in a NASA-like control room, a second building that will house the reactor can be seen in the final stages of construction.
“We want this to be a very boring device,” said Dr. Sorbom, meaning the fewer surprises the better. Apart from its use of high-temperature superconducting magnets, the reactor was designed along the well-studied lines of a traditional tokamak.
The company has the 10 grams of tritium it will need to run SPARC, but it has not divulged the supplier. The primary goal of the project is to test the various aspects of the reactor’s compact design, including how it will remove the helium that will otherwise build up as fusion takes place and dampen the reactor’s ability to sustain its energy output.
Renderings of SPARC from Commonwealth Fusion Systems.
COMMONWEATH FUSION SYSTEMS
Along the way, the machine will have other important hurdles to clear. For example, its ultrapowerful magnets could still face technical obstacles at the extremes at which the machine needs to operate. But if all goes to plan, the machine will achieve net energy at some point after it begins operating in 2025 – perhaps reaching 10 times that based on the company’s calculation. After that, the next step is a prototype power plant that uses the heat from fusion energy to generate electricity, ready some time in the 2030s.
But as exciting as it will be if the commercial players reach their goals, it is unlikely that fusion reactors will have significant presence on power grids before 2040, let alone deliver the world from carbon by 2050.
“It would be extremely premature to start putting fusion into electricity supply assumptions, at any point through the middle of this century,” analysts with the U.S. investment firm Raymond James Financial Inc. wrote in a December brief.
Other veteran fusion analysts agree. Despite recent progress, fusion energy is not a ready-made escape hatch from dealing with tough choices around carbon emissions. Rather, fusion could be an attractive option after those choices have been made.

Dr. Brandon Sorbom at MIT's Plasma Science & Fusion Center.
SIMON SIMARD/THE GLOBE AND MAIL
For Dr. Sorbom, who is 36 and who began thinking about the world’s energy dilemma and its impact on his generation when he was in high school, the real motivation for working on fusion is bigger than climate change, it’s about what it takes to power a global civilization for centuries to come.
“If you look at quality of life metrics – at literacy and infant mortality and life span and things like that. You can basically correlate all of those to energy, and they all get better.”
For his part, Dr. Whyte is just happy to see so much action after so many years of limited progress.
“I’m working on a wider variety of topics in fusion than I’ve ever worked on in my career,” the MIT professor said. “I’ve never been more energized.”


We all have the same 24 hours in a day but for some of us, the 24 hours feel like 24 minutes. As a full-time employee who is also passionate about playing video games, studying, and trying to break Into tech I found myself on many occasions feeling overwhelmed and having my plate full all the time like it's thanksgiving. Well, in the past year, I have decided to be more productive by not doing more but by focusing on less. Below are some tips I utilize to maximize my productivity.

The 80/20 Rule:
Also called the Pareto Principle, this principle is to help you focus on high-value tasks first instead of low-value tasks. It is one of the most helpful things for life and time management. This theory suggests that 80% of results/outcomes will result from 20% of the actions/inputs. You should use this rule to identify the most important tasks and the least. This way you spend your time and energy on the right tasks and not on the minor insignificant ones. For example, you have five tasks; coding, cleaning your room, preparing for an interview, getting a haircut, and watering your plants. Now before you give in to temptations and say to yourself let me do the easy tasks first, sit down and think of the tasks in the 80/20 way and ask yourself if this task is in the high-value top 20 percent or in the low-value bottom 80 percent?

Eat the Frog:
We all procrastinate at times and sometimes more than we should. That is why it is important to start your day by doing the biggest, hardest, and most important task first. In the book Eat that Frog by Brian Tracey he states to identify your frog as the one thing you procrastinate on the most and eat that frog and once you do now nothing else in your day could be worse. When conquering this frog think of what happens if you don't eat it that day. What are the short-term and long-term consequences? The first few times you eat that frog will not be easy but like anything in life practice makes perfect. Doing this continuously will put you in a mental state of taking action immediately on the biggest hardest tasks and you will develop a positive addiction to it.

“One of the very worst uses of time is to do something very well that need not to be done at all.” -Brian Tracy



Biological Prime Time:
Biological prime time means the time of day you feel your best and have the most energy. Typically people fall into four chronotypes, these are the bear, the wolf, the lion, and the dolphin. Depending on which type you are this can help you figure out what time of the day you are the most energized, motivated, focused and productive. If you don’t know when your biological prime time is then you can start by tracking your energy throughout the day and keep track of your highs and lows on a sheet, do this for a few days or up to a week and when you find your biological prime time start using it to your benefit so you get the most out of your day.

The SMART goals method:
SMART (Specific, Measurable, Attainable, Relevant, Time-based). This productivity tool is one I believe to be most helpful for all. Whether you want to finish a project, apply to a new job, study for your exams, and more. This is the tool for you. Because with the SMART goal method, you are single focusing on one thing that is very specific and that the progress can be tracked and has a deadline.



Pomodoro:
I know many of us already use this method and it works magic. The Pomodoro method is for those of us who aren’t big fans of the deep work method. In this method typically you work for 20-30 minutes, rest for 5-10 minutes then repeat until you finish the task you are working on.

The Weekly Reflection:
With all the above tips for staying productive, they work best if you do a weekly reflection. Do the reflection on the weekend and give yourself 2-3 hours to complete it without rushing it and with no disturbance. It is very important before you begin your new week to reflect on what went right and what went wrong the previous week. To see tasks you might need to invest more time in or ones that are time-wasting for you. Before you do this reflection make sure you are in a calm state of mind, take a bath or go for a walk, make sure you already ate and have a glass of water with you, and sit in a distraction-free place where you can reflect. Reflect on your energy levels of the week as well as your productivity, adjust them accordingly for your coming week, and repeat the same process the following weekend.

In this article, I only shared six of my favorite productivity methods but there are over 30+ productivity methods and I hope to share more with you all in the near future. For now, I wanted to start off with some of the most important ones. I hope some of these methods will be useful to you all. Until next time sending you all peace love and light! 🙂


The Python Five Minute Journal
A story about mental health, burnout and Python.

Osvaldo Brignoni's photo
Osvaldo Brignoni
·
Jan 18, 2023
·
7 min read

Featured on Hashnode
PLAY THIS ARTICLE

SPEED
1X
TABLE OF CONTENTS
"Not another day..."
That was my first thought waking up.

Every day in 2021 and the first quarter of 2022, I felt burned out. I could have blamed many things: the pandemic, a failed startup, or too much work. The root of the problem was... I was not taking care of my mental health.

The Five Minute Journal changed my life


The first time I saw The Five Minute Journal by Intelligent Change in a Tim Ferris YouTube video titled How I Journal and Take Notes | Brainstorming + Focusing + Reducing Anxiety, I thought "This is what I need. It will only take me five minutes. I can easily do this every morning". And I did. Every day since April 2022.



I ordered it from Amazon and received it in three days. On the first day, it took me more than fifteen minutes to fill out. I wasn't sure what to put in each section. I had to think about it for a moment. The next day it took me around three minutes. I got used to it quickly.

Day by day, I felt better and better. I woke up more and more with a positive frame of mind. Within three weeks, that poisonous thought had disappeared. "Not another day..." did not cross my mind ever again. I woke up thinking "Thank you for another day of life".

The journal is pricey. Why should I buy it?
You can pick a blank piece of paper and write every day. You could get a cheaper notebook. I think what makes The Five Minute Journal worth it is its simple structure, daily quotes and weekly challenges. It has pages to last for 6 months.



The words you tell yourself every day
If you wake up every day and tell yourself that you hate your current situation, it's an indication that you need to change it. Repeating your actions will reinforce what you think. Your thoughts will influence your actions and what you say to yourself. It is a feedback loop.

The Four Agreements by Don Miguel Ruiz is one of my favorite books on this subject. Be Impeccable With Your Word, is the most relevant agreement of the four. It's about the impact that your words have on yourself and the people around you.

Be Impeccable With Your Word
Speak with integrity. Say only what you mean. Avoid using the word to speak against yourself or to gossip about others. Use the power of your word in the direction of truth and love.
~ The Four Agreements - Don Miguel Ruiz

The actions you repeat every day
I am a big proponent of daily practice in small increments. Atomic Habits by James Clear is the book that changed the way I think about habits, repetition and its compound effect on learning. In the same way, your repeated frame of mind has an impact on your mental health, every day and it compounds over months. That is why depression can be very insidious. It starts small and builds over time.

Every action you take is a vote for the person you wish to become.
~ Atomic Habits - James Clear

Filling out the journal every morning
The Five Minute Journal allows you to start every day on a positive note. This is the structure for one day. Remember, it is so easy. It takes less than five minutes. But it has a huge impact on the rest of your day.

Morning (recommended)

3 things that you are grateful for

3 things that would make your day awesome

3 affirmations

Evening (optional)

3 highlights of the day

3 things you learned

PyFiveMinuteJournal: A simple Python application inspired by the physical journal
After 6 months, I ran out of pages in The Five Minute Journal. I decided to create a Python application inspired by it. These are some of the features.

1. It gets the current date and time
This part is simple but very important. The date is used as the generated file name.

2. It generates a random quote
It requests the Zen Quotes API (zenquotes.io/api/today) to get a random quote. That quote is saved in your daily journal.

3. It prompts questions for the morning
It detects when you are filling the journal the first time. It assumes it is the morning and prompts you with the morning questions.

4. It prompts questions for the evening
It detects when you are filling the journal the second time. It assumes it is the evening and prompts you with the evening questions.

5. It stores each day as a Markdown file
The date, the quote of the day, and the questions and answers are all stored in a Markdown file each day grouped in a directory by year.


COPY
2022/5MJ-2022-12-31.md
2023/5MJ-2023-01-01.md
2023/5MJ-2023-01-02.md
2023/5MJ-2023-01-02.md
2023/5MJ-2023-01-04.md
2023/5MJ-2023-01-05.md
The markdown file.


COPY
Five Minute Journal | Wednesday, Jan 18 2023

> Dwell on the beauty of life. Watch the stars, and see yourself running with them.
> 
> ~ Marcus Aurelius, Meditations

---
07:55 AM

### I am grateful for...
1. My family
2. My incredible friends
3. The warm bed that a sleep in

### What would make today great?
1. Walk around the park
2. Go to the coffeeshop
3. Publish this blog article

### Daily affirmations
1. I traveled to Portugal
2. I am grateful for everything I have
3. I am a better Software Engineer than yesterday

---
09:05 PM

### Highlights of the day
1. Had breakfast with family
2. Met a friend at the coffeeshop
3. Practiced Portuguese with my language partner

### What did I learn today?
1. New Portuguese vocabulary
2. Completed an AWS hands-on lab
3. Studied for AWS Certified SysOps Admin Associate
To Do: Journal Stats
I might add this feature later. I would like to run a stats.py command, parse the journals for a given year and get some statistics like those shown below.


COPY
$ python3 stats.py 2023

-------------------------- 2023 JOURNAL STATS ------------------------

JOURNALS COMPLETED
246 of 365 days

TOP 5 WORDS MENTIONED
- Family: 178
- Friends: 109
- AWS: 83
- Portugal: 64
- Study: 52

PLACES MENTIONED
- Portugal: 64
- Spain: 33
- Puerto Rico: 11

AVERAGE SENTIMENT BY MONTH
January: 0.9 positive
February: 0.7 positive
...
I traveled to Portugal
No. I haven't traveled to Portugal yet, as of the date I published this article. I write this affirmation almost every day. Most of the things I have accomplished, I have affirmed for months as if they had already happened.

Affirmations are timeless and powerful. You can affirm where you see yourself in the future. Or you can affirm where you are today compared to yesterday.

Be careful not to think "I have not achieved enough"
Be careful not to turn affirmations into expectations. That can lead to feelings of disappointment. Things take time and hard work. Treat affirmations loosely as a state of mind. Let your daily actions take you a step closer to your goals.

I fill out the journal every day
Consistency is key. It felt great to complete the first journal by hand. I might buy it again. There is something very satisfying about opening the journal in your hands and looking back at six months of pages full of transformational gratitude, optimism and affirmations turned into reality. For now, I've been using the PyFiveMinuteJournal Python application.

It is NOT the be-all and end-all
Some people pray.

Some people meditate.

Some people go for a run.

Some people fill out a journal.

Some people combine all these things.

Find the way that is best for you. The important thing is that you start every day on a positive note. Make your bed, no matter what happened the day before.

I highly recommend it
If you are a developer, you can clone the PyFiveMinuteJournal from the GitHub repository and start journaling on your computer. If you are not into command-line applications or you prefer writing by hand, then order The Five Minute Journal from Intelligent Change. It might change your life.

GitHub Repository
Find the repository here. Take it for a spin.
brignoni/py-five-minute-journal

Take care of yourself
If this article helps just one person, then it would have been worth writing. Be your best friend. Step back from it all. Take a breath. Make time for yourself. And get back to work in a way that is in balance with your life.



“There must be some way out of here, said the joker to the thief
There’s too much confusion, I can’t get no relief”

> All Along the Watchtower by Bob Dylan, the true “bard” of the modern age

I joined Google just before the pandemic when the company I had co-founded, AppSheet, was acquired by Google Cloud. The acquiring team and executives welcomed us and treated us well. We joined with great enthusiasm and commitment to integrate AppSheet into Google and make it a success. Yet, now at the expiry of my three year mandatory retention period, I have left Google understanding how a once-great company has slowly ceased to function.

Google has 175,000+ capable and well-compensated employees who get very little done quarter over quarter, year over year. Like mice, they are trapped in a maze of approvals, launch processes, legal reviews, performance reviews, exec reviews, documents, meetings, bug reports, triage, OKRs, H1 plans followed by H2 plans, all-hands summits, and inevitable reorgs. The mice are regularly fed their “cheese” (promotions, bonuses, fancy food, fancier perks) and despite many wanting to experience personal satisfaction and impact from their work, the system trains them to quell these inappropriate desires and learn what it actually means to be “Googley” — just don’t rock the boat. As Deepak Malhotra put it in his excellent business fable, at some point the problem is no longer that the mouse is in a maze. The problem is that “the maze is in the mouse”.

It is a fragile moment for Google with the pressure from OpenAI + Microsoft. Most people view this challenge along the technology axis, although there is now the gnawing suspicion that it might be a symptom of some deeper malaise. The recent layoffs have caused angst within the company as many employees view this as a failure of management or a surrender to activist investors. In a way, this reflects a general lack of self-awareness across both management and employees. Google’s fundamental problems are along the culture axis and everything else is a reflection of it. Of course, I’m not the only person to observe these issues (see the post by Noam Bardin, Waze founder and ex-Googler).

The way I see it, Google has four core cultural problems. They are all the natural consequences of having a money-printing machine called “Ads” that has kept growing relentlessly every year, hiding all other sins.

(1) no mission, (2) no urgency, (3) delusions of exceptionalism, (4) mismanagement.

Unfortunately, this is not my first experience watching the gradual decay of a dominant empire. I lived through more than a decade (1999–2011) at another great company (Microsoft) as it slowly degraded and lost its way. Yet, Google has a few strengths that Microsoft didn’t have as it tried to recover — it isn’t a culture of ego and fiefdoms, the environment values introspection, the stated core values of the company are rock solid, and there is still immense respect for Google in the external world. There is hope for Google and for my friends who work there, but it will require an intervention.

“Businessmen they drink my wine, plowmen dig my earth
None of them along the line, know what any of it is worth”

Does anyone at Google come into work actually thinking about “organizing the world’s information”? They have lost track of who they serve and why. Having worked every day at a startup for eight years, the answer was crystal clear for me — — I serve our users. But very few Googlers come into work thinking they serve a customer or user. They usually serve some process (“I’m responsible for reviewing privacy design”) or some technology (“I keep the CI/CD system working”). They serve their manager or their VP. They serve other employees. They will even serve some general Google technical or religious beliefs (“I am a code readability expert”, “I maintain the SWE ladder description document”). This is a closed world where almost everyone is working only for other Googlers, and the feedback loop is based on what your colleagues and managers think of your work. Working extra hard or extra smart doesn’t create any fundamental new value in such a world. In fact, in a bizarre way, it is the opposite.

While two of Google’s core values are “respect the user” and “respect the opportunity”, in practice the systems and processes are intentionally designed to “respect risk”. Risk mitigation trumps everything else. This makes sense if everything is going wonderfully and the most important thing is to avoid rocking the boat and keep sailing on the rising tide of ads revenue. In such a world, potential risk lies everywhere you look. People act accordingly:

every line of code you change is risk, so put in a ton of processes to ensure that that every code change is perfect at avoiding risk (never mind if it is uninspiring for the user)
anything you launch is risk, so put in a ton of reviews and approvals (literally 15+ approvals in a “launch” process that mirrors the complexity of a Nasa space launch) just to deploy each minor change to a minor product
any non-obvious decision is risk, so avoid anything that isn’t group think and conventional wisdom
any change from the way things used to be done is risk, so stick to how it was
any employee you dissatisfy is career risk, so managers aim for 100% satisfaction among their employees, and employ kid gloves even with their worst under-performers (on the other hand, any individual customer you dissatisfy creates zero risk unless it is a mega-customer, so customer satisfaction is just a concept on a dashboard to be trotted out at an all-hands meeting, tut-tutted about, and then forgotten about)
any disagreement with the management chain is career risk, so always say yes to the VP, and the VP says yes to the senior VP, all the way up.
The equation would change if the focus instead were on value creation. If you asked daily: “who did I create value for today”, you’d get to very different behavior. If every half-yearly plan identified “how much value will be generated in the world”, then that would lead to different thinking. I’d work harder if I could create more value and have more impact. But I won’t work harder to prevent people from making mistakes — it is easier and more effective to work slower and slow them down. Just ask a few more clarifications and schedule another round of meetings two weeks from now. There is a reason I didn’t become a regulator or home inspector or government bureaucrat. Those are fine professions, but those aren’t the kind of professions that should dominate in a place like Google if it really “respects the opportunity” to change the world.

“No reason to get excited, the thief he kindly spoke
There are many here among us who feel that life is but a joke”

One of Google’s core values is “respect each other”. There’s two ways to interpret this : I’d hoped it would be to respect the unique strengths of each person and figure out how to get each person to maximize their potential and impact. Unfortunately, this runs into the general organizational lack of desire to change anything. “Respect each other” is translated into “find a way to include and agree with every person’s opinion”. In an inclusive culture (good —it doesn’t withhold information and opportunity) with very distributed ownership (bad), you rapidly get to needing approval from many people before any decision can be made. If this were an algorithm, we’d call it “most cautious wins” and there is almost always someone who is cautious tending to should-do-nothing. Add in that often the people involved have wildly different knowledge and capability and skin-in-the-game, and there’s always going to be someone uncomfortable enough to want to do nothing. Therefore any decision out of the existing pre-approved plan or diverging from conventional wisdom is near impossible to achieve, just as the existing pre-approved plan is near impossible to change.

Now within the pre-approved plan (eg: team X works on product X), the priority is to seek predictability with minimal risk. Every manager tries to sound uniquely wise with their own version of “underpromise and over-deliver”, although this is folly not wisdom. I’ve yet to meet an engineering team that over-delivered on its commitments. This folly is rooted in a culture of managing up by setting low expectations. There are documents that explicitly and proudly deride “heroism” and assert that not only should product teams not encourage “heroes”, they should actively dissuade them. If someone chooses to work twice as hard as is expected of them, they usually will be prevented from doing so because they have to work with others and doing so would force the others to work harder too. If someone says they can finish a project in a month, their manager will tell them to be realistic, pad it to four months, and tell the VP it is six months. They may claim and even think it is better to be slow and do it right, but it doesn’t mean it is done right — — but it sure is done slow.

Overall, it is a soft peacetime culture where nothing is worth fighting for. The people who are inclined to fight on behalf of customers or new ideas or creativity soon learn the downside of doing so. By definition, there is a disincentive to go above and beyond, and your peers and managers will look askance if you try to. You are expected to perform to the definitions of your level in your career ladder, as defined in a very rigidly defined ladder system. A L5 software engineer is expected to do certain things and will be evaluated to that rubric. The word “customer” is not part of that rubric, so don’t you bother supporting customers and don’t expect to be appreciated if you do. Don’t bother being innovative or doing something that wasn’t in the official plan set six months ago, because even if you did, your managers will not line up the associated dev, PM, Pgm, UX, docs, legal, and marketing resources to make it launchable anyway. However, your code better be well-formatted (the dev ladder expects that!) and make sure you have a lot of checkins (exactly what they do doesn’t really matter to anyone). Just wait two years, you’ll be promoted, and you can move onto a different team within Google. It’s just like Noam Bardin from Waze said — although every individual is well intentioned, the system has its own dynamic. And in this system, nothing is worth fighting for.

“But you and I have been through that, and this is not our fate
So let us not talk falsely now, the hour’s getting late”

Within Google, there is a collective delusion that the company is exceptional. And as is the case in all such delusions, the deluded ones are just mortals standing on the shoulders of the truly exceptional people who went before them and created an environment of wild success. Eventually, the exceptional environment starts to fade, but the lingering delusion has abolished humility among the mere mortals who remain. You don’t wake up everyday thinking about how you should be doing better and how your customers deserve better and how you could be working better. Instead, you believe that things you are doing already are so perfect that they are the only way to do it. Propaganda becomes important internally and externally. When new people join your company, you indoctrinate them. You insist on doing things because “that’s the way we do it at Google”. Never mind if most people quietly complain about the overall inefficiency and incompetence. Here are some examples:

Google has a unique homegrown internal tech stack called “Google3”. All the company’s massive-scale consumer products are built on that stack. There is a core delusion that Google has the best tech stack in the world. Now this might have been true a decade ago but it certainly isn’t true today. A lot of other companies have built massive scale consumer services without that specific stack. So what does that mean? Perhaps you don’t want to ban every other useful technology that other people in the world are learning and using to accelerate their work. Like React for example! Or SaaS services like Twilio and Intercom and Mixpanel! Perhaps they will help the company hire better and innovate faster. At least be open to keeping up with the world.
Google has unarguably antiquated internal processes. It is almost as though the company is stuck in a time warp from two decades ago, with waterfall planning processes. If all the senior managers in a team spend one month in every six planning, and one month goes on vacations and one month goes doing performance reviews, there’s suddenly just about enough time to do a reorg and change in strategy once a year, right? Nothing gets done, no problem, no risk — hand out the promos and bonuses and keep going.
Google’s values may say “respect the user”, but it is obviously far from exceptional in focusing on customer success. Unless a customer pays an awful lot of money, they get some poorly-informed frontline support engineer who knows far less about the product than the customer themselves and they are made to run the gauntlet of receiving useless answers (but yay, time to first answer was less than 30 mins, so the customer success dashboard is all green!). Everyone at every level will spend hundreds of hours preparing a single executive presentation, but it will be the most junior employee and often not even a full-time employee who is tasked with helping a customer for ten minutes.
“All along the watchtower, princes kept the view
While all the women came and went, barefoot servants too”

I interviewed with and received offers to join Google first in 2005 and then again in 2009. Although I declined both times, I was super impressed by how different this company was. They wanted to know if I had ambitious ideas on how to change the world. That’s how they thought at the time and that’s the kind of people they wanted to hire. Want to guess if it is that way now?

Google has more than doubled in size in a matter of just a few years, despite ongoing attrition. I joined at the start of 2020, and by sometime in 2022, I had been at Google longer than half of all Googlers. Hiring at this pace is always a problem because it leads to bad hires and those bad hires create more bad hires. “Bad” is subjective — every person can be individually good, but are they placed in positions that maximize the strengths and minimize their weaknesses? That’s tough to do in a hurry. At the junior levels, Google has a challenging interview process and the general caliber of hires is good. Most of this raw talent is wasted and their skills gradually atrophy (but enjoy some free massages to help get over that!). The problem and negative impact lies in the manager ranks and intensifies at the director level and higher. Hiring interviews at this level are entirely subjective and the quality of the interviewers matters. Google Cloud in particular has grown rapidly by hiring mid-level and senior talent from every enterprise company around. In most cases, the skills needed to become a director at not-so-elite enterprise company X are not aligned with the skills needed to become an effective director at Google. And when people are hired from an elite tech company Y, is it really because they were killing it there but still got lured away, or it is because they ran out of runway there and Google was a good place to land. Sometimes the former but too often the latter. In the absence of a stable and effective existing culture, you get some chaotic amalgam of these different enterprise company transplants, all figuring out how to be effective in a new place.

The flip side of hiring is talent management and retention. From what I saw at Google Cloud, they could do a lot better identifying and nurturing talent, moving talent to the best fit roles, and overall optimizing the people already in the company. Instead, the pattern seems to be wait till someone is unhappy and leaves, then just open a req to replace them. Minimal effort to steer people to alternative roles and maximize talent. Such a waste.

The quality of managerial ability varies widely across teams, and so the interpretation of performance reviews is very team-dependent. Yet, Googlers continue to believe in a myth from its first decade as a company — that all performance reviews are standardized and anyone on any other team X at Google will be a good hire at the same level in your own team. This means teams do not conduct thorough internal interviews before an internal transfer — just polite softball conversation.

There’s a number of other leadership challenges as well, reflected in poor strategic as well as tactical decisions. Mainly because decisions are made by people with roles or titles rather than people with expertise (wouldn’t it be nice if those were aligned!). Almost all important decisions are made at the VP level or above, usually by people who have position power and like to voice their opinion. To make matters worse, VPs rotate to different products or come from other companies, but start making critical decisions often barely knowing their product or its customers. Strategy is rarely articulated clearly (that would be career risk), and in any case is usually changed upon review with the next VP up the chain or if it doesn’t immediately translate to success. Many internal projects are started by one VP and killed by the next. Meanwhile all the mid-level managers abandon what they’d just been working on, embrace the new direction du jour, and wait for the next reorg.

“Outside in the distance, a wildcat did growl
Two riders were approaching, the wind began to howl”

Google can no longer seek success by avoiding risk. The path forward has to start with culture change and that has to start at the very top. Google’s senior executives should look at what Satya Nadella did at Microsoft and execute a similar playbook:

Lead with commitment to a mission. It has to go beyond technology (eg: using AI) or making money (eg: Google cloud revenue). It has to be about making positive change in the world, for real people (users, customers) in the real world. Googlers are idealists at heart and their work needs to mean something. They also need to believe that their executives are pursuing an authentic mission versus just parroting slogans. For example, if they “respect the user”, how about each VP and every director cancel one hour of meetings per week and instead use the time to do some direct customer support themselves? It is not too much to ask — the customer deserves the attention and who wants that extra meeting anyway.
Set aside the peacetime generals who underpromise and underdeliver. Define ambitious causes that you will collectively fight for. Expect and reward individual sacrifice towards those causes. Such battle requires heroes whom you should enable and reward. The best people want to make a difference. Motivated people are capable of immense and uniquely valuable contributions in the right circumstances.
Winnow the layers of middle management that have accumulated over time, many promoted gradually beyond their capability, and now incapable of change. They are often out of touch with the products and teams that they own. They compound the problem by hiring more layers of directors, program managers, product managers, chiefs of staff, and more people to meet with and get presentations from and delegate to. Instead, increase manager fanout and decrease the depth of the organizational hierarchy. The recent layoffs should perhaps have focused more on managers and directors and VPs. I don’t want anyone to lose their jobs, but maybe they should learn to become valuable individual contributors again, get their hands dirty, and do real tangible valuable work.
For the mid-level managers and leads, please question some of the conventional wisdom accumulated over the last 25 years. An isolated tech stack guarantees inefficiency not efficiency. Embrace agile or lean development — waterfall techniques are antiquated. “Enterprise customers” is not an excuse to build complicated crap slowly. Decide which (few!) products need legal review and let the others run faster. Treat employees as unique people with special talents rather than replaceable lego blocks at generic ladder levels. Expect and incentivize each employee to do their unique best, rather than restrict them to the low-bar average expectation for their level. Encourage teams to make promises to customers and keep those promises. Build some things that the customers wanted vs only what the VP wanted.

And finally, for all the employees, don’t spend your time on memegen. It is a wallow chamber and all that internal finger pointing doesn’t help anything. Look in the mirror and see if you can change something positive at the level of your team and your product and your customer. It may be a small step only, but it is a constructive step.

Can Google achieve a “soft-landing” — i.e. gradually transform and become a powerhouse again while continuing to grow steadily? Most companies fail this test. Either they gradually wither and then linger on as a shadow of themselves (eg: IBM), or they spectacularly fail (eg: AT&T). Microsoft managed to turn things around, but it required exceptional leadership and good fortune. Google has a chance and I’ll be rooting for it. The world will benefit immensely if Google rediscovers its roots as an ambitious company that will “do no evil” and strive to make the world a better place. Mice can unlearn their maze.

Meta develops an AI language bot that can use external software tools
With Toolformer, an LLM can improve its abilities by calling APIs to external programs.
BENJ EDWARDS - 2/15/2023, 5:56 PM

An artist's impression of a robot hand using a desktop calculator.
Enlarge / An artist's impression of a robot hand using a desktop calculator.
Aurich Lawson | Getty Images
122
WITH
SHARE ON FACEBOOK
SHARE ON TWITTER
Language models like ChatGPT have revolutionized the field of natural language processing, but they still struggle with some basic tasks such as arithmetic and fact-checking. Last Thursday, researchers from Meta revealed Toolformer, an AI language model that can teach itself to use external tools such as search engines, calculators, and calendars without sacrificing its core language modeling abilities.

FURTHER READING
OpenAI invites everyone to test ChatGPT, a new AI-powered chatbot—with amusing results
The key to Toolformer is that it can use APIs (application programming interfaces), which are a set of protocols that allow different applications to communicate with one another, often in a seamless and automated manner. During training, researchers gave Toolformer a small set of human-written examples demonstrating how each API is used and then allowed it to annotate a large language modeling dataset with potential API calls. It did this in a "self-supervised" way, meaning that it could learn without needing explicit human guidance.

The model learned to predict each text-based API call as if they were any other form of text. When in operation—generating text as the result of a human input—it can insert the calls when needed. Moreover, Toolformer can "decide" for itself which tool to use for the proper context and how to use it.

This API-calling ability enables Toolformer to use external software tools like search engines, calculators, language translators, and factual references. For example, large language models (LLM) are well-known for not being particularly good at arithmetic. Toolformer can work around that limitation by using a calculator program. Or if someone wanted an LLM-based assistant to add a date to their calendar, Toolformer could handle that task by using an API link to a calendar app.

Toolformer is based on a pre-trained GPT-J model with 6.7 billion parameters. Experiments conducted by the researchers on various tool-using tasks seem to demonstrate that Toolformer achieves far stronger performance than the much larger GPT-3 model, which contains 175 billion parameters.

FURTHER READING
AI-powered Bing Chat loses its mind when fed Ars Technica article
This isn't the first time researchers have attempted to make up for limitations in language models. In fact, the recent Bing Chat model making the news this week can perform web searches on its own when needed, and others have attempted integrations with browsers, calculators, and search engines. According to Meta's researchers, most existing approaches to integrating tools into language models have relied on large amounts of human annotations or have been limited to specific task-specific settings. By contrast, Toolformer can learn to use a range of tools in a generalized way that does not require specialized training for specific tasks.

With techniques like those found in Toolformer, we're looking at a potential future where LLMs augmented with the ability to use external apps will become far more versatile and reliable assistants (ostensibly). But the ability to perform API calls also might increase an LLM's capability to cause harm to user data (in apps) or create trouble in the outside world (through a web browser or communications tools)—abilities that they might accidentally invoke while providing an answer.

FEB. 16, 2023, 2:24 P.M.
Programming AIs worry me
Computer Things
For some inane reason, Github classifies me as a “major open source maintainer”, which means I get a free copilot subscription.1 I’ve been using it for a couple months now and I got to say, it’s a goddamn delight. It can write boilerplate like nobody’s business. I find the tool works best when I’m using it as a keystroke saving device, where it writes 1-2 lines at a time. I write x = and it completes with really[long][dict][lookup]. It’s all very easy.

And this easiness worries me. I got a lot more worried when I read What Do ChatGPT and AI-based Automatic Program Generation Mean for the Future of Software, by Bertrand Meyer. He starts by testing ChatGPT with a tricky spec:

I like to use the example of a function that starts with explicit values: 0 for 0, 1 for 1, 4 for 2, 9 for 3, 16 for 4, 25 for 5. […] This time I fed the above values to ChatGPT and for good measure added that the result for 6 is 35. Yes, 35, not a typo. Now, lo and behold, ChatGPT still infers the square root [sic] function!

About what I expect. After he told ChatGPT that it was for 6, ChatGPT gave back a giant if-else chain. This technically is compliant with his spec but doesn’t capture the spirit of what he wants. A good example of the limits of the tool.

But then things take a turn for the worse.

but things becomes amazing again, in fact more amazing than before, when I point out my dissatisfaction with the above style:

Bertrand gets the function `n(n+1)(2n+1)/6`

The inferred function is rather impressive. What human would come up with that function in less time than it takes to say “Turing test”?

Except the inferred function is completely wrong. Not even subtly wrong. The first incorrect input is 2. Bertrand didn’t notice.

Now, here’s some important context: Bertrand Meyer’s entire deal is software correctness. He invented Eiffel. He trademarked Design By Contract (tee em). He regularly rants about how SEs don’t know about logic. He didn’t notice the error. Oh, and this article had 114 comments on Hacker News and exactly one commenter (of 48) noticed.

Using AI-assisted code changes our work from writing code to proofreading code. And that’s a problem.

Proofreading is hard
So a quick story: back in 2020 I experimented with voice-to-text. I bought a Dragon license and everything. I can speak a lot faster than I can type, after all! I’d say it was about 95% accurate. The problem was finding that 5% of errors. Most of the typos I make when writing feel wrong. I can feel my fingers slip up and type the wrong thing, and I can immediately go back and correct it. But most speakos feel normal. I say “Code matches the spec” and it transcribes “code smashes the spec”. After I wrote something, I’d have to go through very carefully and correct all the speakos. It was goddamn exhausting, and many errors still fell through. Proofreading is hard!2

Over time I bent my workflow around proofreading, like putting each spoken sentence on a newline to break my reading flow. This helped find more errors but made the whole process even more miserable, and eventually I just gave up and went back to typing.

It takes longer to write a code “sentence” than a prose one, so a sentence-level generate-proofread loop is still more convenient than writing everything manually. That’s why I like Copilot. But as we start using AIs to generate larger blocks of code, we’re going to be faced with more and more proofreading work. And I’m worried more bugs will then slip through. If Bertrand Meyer can’t proofread closely enough to catch errors, what hope do us mere mortals have?

Critiques
Two reasons I could be less worried:

People need to be proofread too, so as long as AIs eventually make fewer mistakes than the average programmer, they’re a net win.
We can make proofreading easier with better tooling. Unit tests are a means of “proofreading”: we can catch AI errors automatically with tests.
I don’t know how true either these are. I can certainly see a future where both of these are true, and we happily use AIs without a second thought. I also see a future where we don’t adapt our skillsets and tooling around using AIs “properly”, and they become a net negative for a lot of people. I don’t know! That’s why I’m excited but also worried.

We’ll also have to see what happens when Copilot (and ChatGPT, sorta) aren’t the only games in town. Are there going to be AIs that specialize in certain domains? AIs that specialize in writing tests? AIs that are designed for language newcomers? I feel like I’d be a bit less worried if there was a more diverse ecosystem, for some reason.

Programming AIs I want
Might as well share a wishlist.

A ChatGPT-style AI that can only reply with links to reference docs, libraries, or wikipedia articles
An AI that only highlights possible issues in the code, like “this looks kinda like an N+1 query” or
An AI that takes code and generates comments, mostly so I could quickly understand new configuration formats
AI-guided property-based testing. We already have AI-guided fuzzing, why not apply that at a more granular level
While I have your attention, I’ll close with my favorite use of ChatGPT:

ChatGPT hilariously bungling a riddle

Update for the Internets
This was sent as part of an email newsletter; you can subscribe here. Common topics are software history, formal methods, the theory of software engineering, and silly research dives. Updates are 6x a month. I also have a website where I put my polished writing (the newsletter is more for off-the-cuff stuff). That usually updates monthly, though I want to try getting it to bimonthly at some point.

Also you can check out the flip side, where I talk about ways of using ChatGPT to program better here

Also also April Cools is coming up woooo write something for that if you have a blog, it’s hella fun

If you’re a github employee, plz plz plz don’t look into this, let me have my fun ↩

This is also why a lot of people hate code review. It’s good when you’re acting as an editor, looking for foundational improvements to the code, but it’s awful when you’re acting as a proofreader. That probably deserves its own essay! ↩

You just read issue #223 of Computer Things. You can also browse the full archives of this newsletter.